{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a correct answer for the question in the sample.txt file\n",
    "correct_answer = {\n",
    "  \"Suçun_Temel_Unsurları\": {\n",
    "    \"Fail\": \"C\",\n",
    "    \"Mağdur\": \"Market (Mülkiyet hakkı ihlal edilen taraf)\",\n",
    "    \"Konu\": \"Marketten alınan ürünler\",\n",
    "    \"Fiil\": \"Ürünlerin kasada ödeme yapılmaksızın dışarı çıkarılması\",\n",
    "    \"Netice\": {\n",
    "      \"Marketin_malvarlığında_azalma\": True,\n",
    "      \"Nedensellik_Bağı\": \"C’nin fiili ile netice arasında nedensellik bağı mevcuttur.\",\n",
    "      \"Objektif_İsnadiyet\": \"Fiil, neticeyi doğuracak şekilde gerçekleştirilmiştir.\"\n",
    "    }\n",
    "  },\n",
    "  \"Manevi_Unsur\": \"C, bu eylemi kasten mi gerçekleştirmiştir? C’nin kasıtlı olup olmadığı, marketten çıkarken ödeme yapmadığını fark etmesiyle bağlantılıdır. C, ödeme yapmayı unuttuğunu iddia etmektedir, bu nedenle kast unsuru tartışmalı olabilir.\",\n",
    "  \"Hukuka_Aykırılık_Unsuru\": \"Hukuka aykırılık unsuru açısından, C’nin eylemi hırsızlık suçu açısından değerlendirildiğinde, hukuka aykırı bir durum mevcuttur. Ancak, manevi unsurdaki belirsizlik bu hususu etkileyebilir.\",\n",
    "  \"Suçun_Nitelikli_Unsurları\": \"Bu olayda nitelikli bir unsur yoktur.\",\n",
    "  \"Kusurluluk\": \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"Yaptırımın_Uygulanması_İçin_Gereken_Veya_Yaptırım_Uygulanmasını_Engelleyen_Özel_Koşullar\": \"C’nin ödeme yapmayı unuttuğunu iddia etmesi, suçun manevi unsurunu etkileyebilir ve bu durum yaptırım uygulanmasını engelleyebilir.\",\n",
    "  \"Suçun_Özel_Görünüş_Biçimleri\": {\n",
    "    \"Teşebbüs\": \"Eylem tamamlanmıştır, teşebbüs söz konusu değildir.\",\n",
    "    \"İştirak\": \"Olayda iştirak eden başka bir kişi yoktur.\",\n",
    "    \"İçtima\": \"Tek bir suç tipi söz konusudur.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "test_sentences = [\"Havayı kirletmek, çevreyi kirletmek anlamına gelir.\", \"Hava kirliliği, çevre kirliliğinin bir parçasıdır.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "Cosine similarity (%) : 0.0\n",
      "0.04399732626783181\n",
      "67.0\n"
     ]
    }
   ],
   "source": [
    "# THRASH CODE\n",
    "\n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = correct_answer[\"Kusurluluk\"]\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime          Benzerlik Skoru\n",
      "------------  -----------------\n",
      "ateşkes                0.823591\n",
      "uzlaşma                0.794623\n",
      "ittifak                0.774118\n",
      "antlaşma               0.773479\n",
      "müzakereler            0.762028\n",
      "barışı                 0.759955\n",
      "antlaşmayı             0.749817\n",
      "müzakereleri           0.747546\n",
      "mütareke               0.727691\n",
      "müttefiklik            0.717039\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"barış\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_Word2Vec:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.generate_model(model_name)\n",
    "\n",
    "    def generate_model(self, model_name):\n",
    "        return Word2Vec.load(model_name)\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    def extract_key_features(self, words_1, words_2):\n",
    "        \n",
    "        # Nested iteration to compare each word in the sentences\n",
    "        #Dict: {word_1:{word_comp1:score, word_comp2:score, ...}, word_2:{word_comp1:score, word_comp2:score, ...}}\n",
    "        searched_pairs = [];similarity_dict = {}\n",
    "\n",
    "        for word_1 in words_1:\n",
    "            if word_1 not in similarity_dict:\n",
    "                similarity_dict.update({word_1:{}})\n",
    "            for word_2 in words_2:\n",
    "                if (word_1, word_2) in searched_pairs or (word_2, word_1) in searched_pairs:\n",
    "                    continue\n",
    "                try:\n",
    "                    searched_pairs.append((word_1, word_2))\n",
    "                    # Calculate the similarity between the words\n",
    "                    similarity_dict[word_1].update({word_2:self.model.wv.similarity(word_1, word_2)})\n",
    "                except:\n",
    "                    pass\n",
    "        return similarity_dict\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Clean the sentences and split them into words\n",
    "        sentence_1 = self.clean_sentence(sentence_1); words_1 = sentence_1.split()\n",
    "        sentence_2 = self.clean_sentence(sentence_2); words_2 = sentence_2.split()\n",
    "\n",
    "        # Extract key features\n",
    "        similarity_dict = self.extract_key_features(words_1, words_2)\n",
    "            \n",
    "        # Extract informations from the similarity_dict\n",
    "        key_features = []\n",
    "        for key, value in similarity_dict.items():\n",
    "            if len(value) > 0:\n",
    "                # Sort and get the best match\n",
    "                sorted_dict = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "                max_score = sorted_dict[0][1]\n",
    "                best_key = sorted_dict[0][0]\n",
    "\n",
    "                key_features.append({\"key\":key, \"score\":max_score,\"best_match\":best_key})\n",
    "\n",
    "        # Calculate the average score\n",
    "        avg_score = sum([x[\"score\"] for x in key_features]) / len(key_features)\n",
    "\n",
    "        return avg_score#,key_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_comparator = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "#avg_score,sim_dict = word2vec_comparator.compare_sentences(test_sentences[0], test_sentences[1])\n",
    "#\n",
    "#print(tabulate([x.values() for x in sim_dict], headers=[\"Kelime\", \"En Benzer Kelime, Benzerlik Skoru\"]))\n",
    "#print(\"\\nAverage Score: \", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding with turkish trained word2vec\n",
    "#def calculate_vector_of_senetence(sentence):\n",
    "#    sentence_vector = np.zeros(100)\n",
    "#    for word in sentence.split():\n",
    "#        try:\n",
    "#            sentence_vector += model.wv[word]\n",
    "#        except:\n",
    "#            pass\n",
    "#    return sentence_vector\n",
    "#\n",
    "#def cosine_similarity(v1, v2):\n",
    "#    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "#\n",
    "#def measure_similarity(sentence_1, sentence_2):\n",
    "#    v1 = calculate_vector_of_senetence(sentence_1)\n",
    "#    v2 = calculate_vector_of_senetence(sentence_2)\n",
    "#    cos_sim = cosine_similarity(v1, v2)\n",
    "#\n",
    "#    print(\"\\nCosine similarity: \", cos_sim)\n",
    "#\n",
    "#\n",
    "#measure_similarity(\n",
    "#    test_sentences[0],\n",
    "#    test_sentences[1]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelfile System Prompt: \n",
    "\n",
    "Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {\"cümleler\":<cümleler>,\"analiz/neden\":<kısaca>,\"değerlendirme\": <>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class SentenceComparator_Ollama:\n",
    "    def __init__(self,modelfile_system, llama_version=\"llama3.1\", temperature=0.4):\n",
    "        self.system_prompt = modelfile_system\n",
    "        self.llama_version = llama_version\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.role_messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: C kişisi kasada ödeme yapmadan marketten çıkmıştır, İkinci cümle: C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 0\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: Havalar güzelken denize gitmek çok iyi olur., İkinci cümle: Bir insan ev almadan önce araba parası biriktirmeli.'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 1\"\n",
    "                }\n",
    "            ]\n",
    "        self.generate_model()\n",
    "    \n",
    "    def generate_model(self):\n",
    "        modelfile = f'''\n",
    "        FROM {self.llama_version}\n",
    "        SYSTEM {self.system_prompt} \n",
    "        PARAMETER temperature {self.temperature}\n",
    "        '''\n",
    "        ollama.create(model=f'MAHDAN_{self.llama_version}', modelfile=modelfile)\n",
    "\n",
    "    def calculate_similarity(self,sentence_1,sentence_2):\n",
    "        sentence_in = 'İlk Cümle: ' + sentence_1 + ' ,\\n İkinci Cümle: ' + sentence_2        \n",
    "\n",
    "        # Concat messages and sentence\n",
    "        messages_temp = self.role_messages.copy()\n",
    "\n",
    "        messages_temp.append({\n",
    "            'role': 'user',\n",
    "            'content': sentence_in\n",
    "        })\n",
    "\n",
    "        response = ollama.chat(model=f'MAHDAN_{self.llama_version}', messages= messages_temp)\n",
    "\n",
    "        return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )\n",
    "#\n",
    "#\n",
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "# Class of Semantic Similarity\n",
    "class SentenceComparator_semantic:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#model_name = 'bert-base-multilingual-cased'\n",
    "# Calculate cosine similarity between two sentences with BERT\n",
    "class SentenceComparator_bert_cosine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return BertModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentence):\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings1 = self.get_embeddings(sentence_1)\n",
    "        embeddings2 = self.get_embeddings(sentence_2)\n",
    "        \n",
    "        return cosine_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "#model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Class of SBERT Similarity\n",
    "class SentenceComparator_SBERT:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "        \n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çalışmak için *İnternet* gerektiriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NLI pipeline oluşturma (Türkçe destekleyen model kullanılabilir)\n",
    "#nli_model = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "class SentenceComparator_NLI:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return pipeline(\"text-classification\", model=self.model_name)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        input_sentence = sentence_1 + ' [SEP] ' + sentence_2\n",
    "        result = self.model(input_sentence)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    sentences = [ \n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C kişisi bir banka müdürü.\"\n",
    "    ] \n",
    "\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for index2, sentence2 in enumerate(sentences):\n",
    "            if index != index2:\n",
    "\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                #print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\n",
    "                #print(\"Return: \\n\", result)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                #print(\"\\n\")\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} model execution time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n",
    "\n",
    "# Test the NLI model\n",
    "#test_model(nli_model, \"nli_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "test_model(nli_model, \"nli_model\")\n",
    "\n",
    "semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "test_model( semantic_similarity, \"semantic_similarity\")\n",
    "\n",
    "bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "\n",
    "sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "test_model(sbert_similarity, \"sbert_similarity\")\n",
    "\n",
    "word2vec_sim = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "test_model(word2vec_sim, \"word2vec_sim\")\n",
    "\n",
    "sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "\n",
    "ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "    llama_version=\"llama3.1\",\n",
    "    modelfile_system= sys_prompt,\n",
    "    temperature=0.4\n",
    ")\n",
    "test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n",
    "\n",
    "#Çöp\n",
    "#ollama_model_mistral = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_mistral, \"ollama_model_mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim</th>\n",
       "      <th>sbert_cos_sim</th>\n",
       "      <th>nli_model</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>word2vec_sim</th>\n",
       "      <th>ollama_model_llama3.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi ödeme yapmayı unutarak marketten çıkm...</td>\n",
       "      <td>0.626447</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>0.717026</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.330787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi hırsızdır ve hırsızlık suçu işlediği ...</td>\n",
       "      <td>0.692395</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi bir banka müdürü.</td>\n",
       "      <td>0.597883</td>\n",
       "      <td>0.2924</td>\n",
       "      <td>0.573564</td>\n",
       "      <td>0.4174</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentence1  \\\n",
       "0  C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1  C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2  C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3  C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4  C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "\n",
       "                                           Sentence2  bert_cos_sim  \\\n",
       "0  C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.964577   \n",
       "1  C kişisi marketten alışveriş yapmıştır ve kasa...      0.636648   \n",
       "2  C kişisi ödeme yapmayı unutarak marketten çıkm...      0.626447   \n",
       "3  C kişisi hırsızdır ve hırsızlık suçu işlediği ...      0.692395   \n",
       "4                         C kişisi bir banka müdürü.      0.597883   \n",
       "\n",
       "   sbert_cos_sim  nli_model  semantic_similarity  word2vec_sim  \\\n",
       "0         0.9383   0.769148               0.9611      0.946022   \n",
       "1         0.1996   0.589680               0.7214      0.398693   \n",
       "2         0.3011   0.717026               0.7751      0.330787   \n",
       "3         0.3268   0.619045               0.5339      0.513608   \n",
       "4         0.2924   0.573564               0.4174      0.316620   \n",
       "\n",
       "   ollama_model_llama3.1  \n",
       "0                      0  \n",
       "1                      1  \n",
       "2                      1  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "# For sbert_similarity_log.xlsx\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensordeğerlendirme:]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "                        axis=1, \n",
    "                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "             \n",
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
