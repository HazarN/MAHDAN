{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a correct answer for the question in the sample.txt file\n",
    "correct_answer = {\n",
    "  \"Suçun_Temel_Unsurları\": {\n",
    "    \"Fail\": \"C\",\n",
    "    \"Mağdur\": \"Market (Mülkiyet hakkı ihlal edilen taraf)\",\n",
    "    \"Konu\": \"Marketten alınan ürünler\",\n",
    "    \"Fiil\": \"Ürünlerin kasada ödeme yapılmaksızın dışarı çıkarılması\",\n",
    "    \"Netice\": {\n",
    "      \"Marketin_malvarlığında_azalma\": True,\n",
    "      \"Nedensellik_Bağı\": \"C’nin fiili ile netice arasında nedensellik bağı mevcuttur.\",\n",
    "      \"Objektif_İsnadiyet\": \"Fiil, neticeyi doğuracak şekilde gerçekleştirilmiştir.\"\n",
    "    }\n",
    "  },\n",
    "  \"Manevi_Unsur\": \"C, bu eylemi kasten mi gerçekleştirmiştir? C’nin kasıtlı olup olmadığı, marketten çıkarken ödeme yapmadığını fark etmesiyle bağlantılıdır. C, ödeme yapmayı unuttuğunu iddia etmektedir, bu nedenle kast unsuru tartışmalı olabilir.\",\n",
    "  \"Hukuka_Aykırılık_Unsuru\": \"Hukuka aykırılık unsuru açısından, C’nin eylemi hırsızlık suçu açısından değerlendirildiğinde, hukuka aykırı bir durum mevcuttur. Ancak, manevi unsurdaki belirsizlik bu hususu etkileyebilir.\",\n",
    "  \"Suçun_Nitelikli_Unsurları\": \"Bu olayda nitelikli bir unsur yoktur.\",\n",
    "  \"Kusurluluk\": \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"Yaptırımın_Uygulanması_İçin_Gereken_Veya_Yaptırım_Uygulanmasını_Engelleyen_Özel_Koşullar\": \"C’nin ödeme yapmayı unuttuğunu iddia etmesi, suçun manevi unsurunu etkileyebilir ve bu durum yaptırım uygulanmasını engelleyebilir.\",\n",
    "  \"Suçun_Özel_Görünüş_Biçimleri\": {\n",
    "    \"Teşebbüs\": \"Eylem tamamlanmıştır, teşebbüs söz konusu değildir.\",\n",
    "    \"İştirak\": \"Olayda iştirak eden başka bir kişi yoktur.\",\n",
    "    \"İçtima\": \"Tek bir suç tipi söz konusudur.\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "Cosine similarity (%) : 0.0\n"
     ]
    }
   ],
   "source": [
    "# THRASH CODE\n",
    "\n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = correct_answer[\"Kusurluluk\"]\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"word2vec/w2v_.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime          Benzerlik Skoru\n",
      "------------  -----------------\n",
      "ateşkes                0.823591\n",
      "uzlaşma                0.794623\n",
      "ittifak                0.774118\n",
      "antlaşma               0.773479\n",
      "müzakereler            0.762028\n",
      "barışı                 0.759955\n",
      "antlaşmayı             0.749817\n",
      "müzakereleri           0.747546\n",
      "mütareke               0.727691\n",
      "müttefiklik            0.717039\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(model.wv.most_similar(\"barış\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umutc\\AppData\\Local\\Temp\\ipykernel_21060\\1357340413.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  model.wv.word_vec(\"umut\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1495104 , -1.4914255 , -0.50925356, -0.9685314 ,  2.1551907 ,\n",
       "        0.10626572,  0.4027821 ,  1.0281931 ,  0.41044936, -1.1525857 ,\n",
       "       -0.0205108 ,  1.0924134 , -1.9218051 ,  1.3797586 , -0.63527036,\n",
       "       -0.38006008, -0.6512365 , -0.96633595,  1.1853794 ,  0.7896848 ,\n",
       "       -0.03258616,  0.8834496 , -1.6903982 ,  0.9449919 ,  0.6057014 ,\n",
       "        0.59224516, -1.0036951 ,  2.0536163 , -2.1637177 , -0.65654767,\n",
       "        1.0522053 ,  0.11371119,  1.1112392 , -0.43076926,  0.13155091,\n",
       "       -1.1467836 , -0.8198967 ,  1.1959015 , -0.5887494 , -1.0079744 ,\n",
       "       -0.25314665,  0.5018188 , -0.76072204, -0.30214065, -0.13227591,\n",
       "        0.6748753 ,  0.7053673 ,  1.5428567 , -0.08245109,  0.76109725,\n",
       "       -0.6433578 , -1.2249595 , -0.8891968 , -1.5681715 , -0.4665735 ,\n",
       "        0.23464409,  0.9810163 ,  0.7976722 ,  1.6759675 ,  0.8835468 ,\n",
       "        0.21888028,  1.7479744 ,  0.10691787,  0.25808322,  0.4888047 ,\n",
       "        1.4980937 ,  1.0098569 , -0.28364947, -0.6473856 ,  1.0754474 ,\n",
       "       -0.7372982 ,  0.2214374 , -0.68053114, -0.97661483,  1.8531004 ,\n",
       "        0.90171987,  1.5329516 ,  1.2696122 ,  0.5022536 , -0.7828172 ,\n",
       "       -0.51381963,  0.64165884,  0.40742102, -1.4720764 , -0.76259804,\n",
       "        1.090137  , -0.79868716,  0.5095711 , -0.22643751,  1.2640641 ,\n",
       "       -1.4126805 , -0.7636473 , -3.278555  ,  0.06060509, -0.7998284 ,\n",
       "       -0.8611428 , -1.5374006 ,  1.47675   ,  0.18929918,  1.5398192 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec(\"umut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                    score  best_match\n",
      "------------------  --------  ------------\n",
      "c                   0.725464  b\n",
      "bu                  1         bu\n",
      "eylemi              0.629131  hareketi\n",
      "kasten              0.777449  bilerek\n",
      "mi                  1         mi\n",
      "gerçekleştirmiştir  0.642038  yaptı\n",
      "Average Score: 0.7956802348295847\n"
     ]
    }
   ],
   "source": [
    "class SentenceComparator_Word2Vec:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.generate_model(model_name)\n",
    "\n",
    "    def generate_model(self, model_name):\n",
    "        return Word2Vec.load(model_name)\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    def extract_key_features(self, words_1, words_2):\n",
    "        \n",
    "        # Nested iteration to compare each word in the sentences\n",
    "        #Dict: {word_1:{word_comp1:score, word_comp2:score, ...}, word_2:{word_comp1:score, word_comp2:score, ...}}\n",
    "        searched_pairs = [];similarity_dict = {}\n",
    "\n",
    "        for word_1 in words_1:\n",
    "            if word_1 not in similarity_dict:\n",
    "                similarity_dict.update({word_1:{}})\n",
    "            for word_2 in words_2:\n",
    "                if (word_1, word_2) in searched_pairs or (word_2, word_1) in searched_pairs:\n",
    "                    continue\n",
    "                searched_pairs.append((word_1, word_2))\n",
    "\n",
    "                # Calculate the similarity between the words\n",
    "                similarity_dict[word_1].update({word_2:self.model.wv.similarity(word_1, word_2)})\n",
    "        \n",
    "        return similarity_dict\n",
    "    \n",
    "    def compare_sentences(self, sentence_1, sentence_2):\n",
    "        # Clean the sentences and split them into words\n",
    "        sentence_1 = self.clean_sentence(sentence_1); words_1 = sentence_1.split()\n",
    "        sentence_2 = self.clean_sentence(sentence_2); words_2 = sentence_2.split()\n",
    "\n",
    "        # Extract key features\n",
    "        similarity_dict = self.extract_key_features(words_1, words_2)\n",
    "            \n",
    "        # Extract informations from the similarity_dict\n",
    "        key_features = []\n",
    "        for key, value in similarity_dict.items():\n",
    "            if len(value) > 0:\n",
    "                # Sort and get the best match\n",
    "                sorted_dict = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "                max_score = sorted_dict[0][1]\n",
    "                best_key = sorted_dict[0][0]\n",
    "\n",
    "                key_features.append({\"key\":key, \"score\":max_score,\"best_match\":best_key})\n",
    "\n",
    "        # Calculate the average score\n",
    "        avg_score = sum([x[\"score\"] for x in key_features]) / len(key_features)\n",
    "\n",
    "        return key_features, avg_score\n",
    "\n",
    "sentence_1 = \"C, bu eylemi kasten mi gerçekleştirmiştir?\"\n",
    "sentence_2 = \"B, bu hareketi bilerek mi yaptı?\"\n",
    "\n",
    "sim_dict, avg_score = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\").compare_sentences(sentence_1, sentence_2)\n",
    "\n",
    "print(tabulate(sim_dict, headers=\"keys\"))\n",
    "print(\"Average Score:\", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C’nin', 'kusur', 'durumu,', 'ödeme', 'yapmayı', 'unuttuğunu', 'iddia', 'etmesi', 'sebebiyle,', 'kusurluluğu', 'tartışmaya', 'açabilir.']\n",
      "[\"C'nin\", 'dikkat', 've', 'özen', 'yükümlülüğüne', 'aykırı', 'davranmış', 'olması', 'nedeniyle', 'kusurlu', 'olduğu', 'değerlendirilebilir.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m best_matches, score_word_avg \u001b[38;5;241m=\u001b[39m sentence_comparator_word_by_word(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate(best_matches\u001b[38;5;241m.\u001b[39mitems(), headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKelime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEn Benzer Kelime, Benzerlik Skoru\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAverage Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, score_word_avg)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "best_matches, score_word_avg = sentence_comparator_word_by_word(\n",
    "    \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "    \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\", model)\n",
    "\n",
    "print(tabulate(best_matches.items(), headers=[\"Kelime\", \"En Benzer Kelime, Benzerlik Skoru\"]))\n",
    "print(\"\\nAverage Score: \", score_word_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\n",
      "Sentence 2:  C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\n",
      "\n",
      "Cosine similarity:  0.6457037298717322\n",
      "Difference of vectors:  62.19235489334093 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_vector_of_senetence(sentence):\n",
    "    sentence_vector = np.zeros(100)\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            sentence_vector += model.wv[word]\n",
    "        except:\n",
    "            pass\n",
    "    return sentence_vector\n",
    "\n",
    "def difference_of_vectors(v1, v2):\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def measure_similarity(sentence_1, sentence_2):\n",
    "    v1 = calculate_vector_of_senetence(sentence_1)\n",
    "    v2 = calculate_vector_of_senetence(sentence_2)\n",
    "    cos_sim = cosine_similarity(v1, v2)\n",
    "    diff = difference_of_vectors(v1, v2)\n",
    "\n",
    "    print(\"Sentence 1: \", sentence_1)\n",
    "    print(\"Sentence 2: \", sentence_2)\n",
    "    print(\"\\nCosine similarity: \", cos_sim)\n",
    "    print(\"Difference of vectors: \", diff, \"\\n\")\n",
    "\n",
    "\n",
    "measure_similarity(\n",
    "    \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "    \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "modelfile = '''\n",
    "FROM llama3.1\n",
    "SYSTEM Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {\"cümleler\":<cümleler>,\"analiz/neden\":<2 cümle>,\"değerlendirme\": <[\"aynı anlamlı\",\"alakasız\",\"benzer anlamlı\",\"zıt anlamlı\",\"yetersiz bilgi\"]>}.\n",
    "'''\n",
    "\n",
    "ollama.create(model='MAHDAN', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " İlk Cümle: C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir. ,\n",
      " İkinci Cümle: C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\n",
      "Return: \n",
      " [{'cümleler': 'C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir., C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.', 'analiz/neden': 'İki cümle de C'nin kusur durumuna değinmekte. İlk cümlede, ödeme yapmayı unutma durumu üzerinde durulurken, ikinci cümlede dikkat ve özen yükümlülüğü açısından suçlama ileri sürülüyor.', 'değerlendirme': ['benzer anlamlı']}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_output= [\n",
    "{\n",
    "  \"cümleler\": \"C kişisi kasada ödeme yapmadan marketten çıkmıştır., C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "  \"analiz/neden\": \"İki cümle de C kişisinin kasada ödeme yapmadan marketten çıktığını belirtiyor. Ancak ikinci cümle daha fazla detay sunuyor.\",\n",
    "  \"değerlendirme\": [\"benzer anlamlı\"]\n",
    "},\n",
    "{\n",
    "  \"cümleler\": \"C kişisi kasada ödeme yapmadan marketten çıkmıştır., C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "  \"analiz/neden\": \"İlk cümlede sadece ödeme yapmadan çıkma durumu belirtiliyor, ikinci cümlede ise ödeme yapmayı unutma durumu var. İki cümle de aynı olayı ifade ediyor fakat farklı kelimeler kullanıyor.\",\n",
    "  \"değerlendirme\": [\"benzer anlamlı\"]\n",
    "},\n",
    "{\n",
    "  \"cümleler\": \"C kişisi kasada ödeme yapmadan marketten çıkmıştır., C kişisi hırsızdır.\",\n",
    "  \"analiz/neden\": \"İlk cümlede C kişisinin ödeme yapmadan marketten çıktığı belirtiliyor, ikinci cümlede ise C'nin hırsız olduğu öne sürülüyor. İki cümle arasında anlam farkı var.\",\n",
    "  \"değerlendirme\": [\"zıt anlamlı\"]\n",
    "},\n",
    "{\n",
    "  \"cümleler\": \"C kişisi kasada ödeme yapmadan marketten çıkmıştır., C kişisinin parası yoktur.\",\n",
    "  \"analiz/neden\": \"İlk cümlede ödeme yapmadan marketten çıkma durumu belirtiliyor, ikinci cümlede ise C'nin parası olmadığı ifade ediliyor. Cümleler arasında doğrudan bir anlam bağlantısı bulunmuyor.\",\n",
    "  \"değerlendirme\": [\"alakasız\"]\n",
    "},\n",
    "{\n",
    "  \"cümleler\": \"C kişisi kasada ödeme yapmadan marketten çıkmıştır., C kişisi bir banka müdürü.\",\n",
    "  \"analiz/neden\": \"İlk cümlede ödeme yapmadan marketten çıkma durumu belirtiliyor, ikinci cümlede ise C'nin banka müdürü olduğu belirtiliyor. İki cümle arasında anlam benzerliği yok.\",\n",
    "  \"değerlendirme\": [\"alakasız\"]\n",
    "}\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': ' İkili değerlendir: C kişisi kasada ödeme yapmadan marketten çıkmıştır, C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır, C kişisi ödeme yapmayı unutarak marketten çıkmıştır, C kişisi hırsızdır, C kişisinin parası yoktur, C kişisi bir banka müdürü'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': f\"{example_output}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def similarity_sentences(sentence):\n",
    "    # Concat messages and sentence\n",
    "    messages_temp = messages.copy()\n",
    "\n",
    "    messages_temp.append({\n",
    "        'role': 'user',\n",
    "        'content': sentence\n",
    "    })\n",
    "\n",
    "    response = ollama.chat(model='MAHDAN', messages= messages_temp)\n",
    "\n",
    "    return response['message']['content']\n",
    "#Sentence examples\n",
    "sentences = [ \n",
    "  \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "]\n",
    "\n",
    "#            \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "#            \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "#            \"C kişisi hırsızdır.\",\n",
    "#            \"C kişisinin parası yoktur.\",\n",
    "#            \"C kişisi bir banka müdürü.\"\n",
    "#] \n",
    "\n",
    "checked_pairs = []\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    for index2, sentence2 in enumerate(sentences):\n",
    "        if index != index2 and (sentence,sentence2) not in checked_pairs and (sentence2,sentence) not in checked_pairs:\n",
    "            sentence_in = 'İlk Cümle: ' + sentence + ' ,\\n İkinci Cümle: ' + sentence2\n",
    "            \n",
    "            print(\"Input: \\n\", sentence_in)\n",
    "            print(\"Return: \\n\", similarity_sentences(sentence_in))\n",
    "            print(\"\\n\")\n",
    "            checked_pairs.append((sentence,sentence2))\n",
    "            #measure_similarity(sentence,sentence2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull(model='mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "# Class of Semantic Similarity\n",
    "class SentenceComparator_semantic:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "# Calculate cosine similarity between two sentences with BERT\n",
    "class SentenceComparator_bert_cosine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return BertModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentence):\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings1 = self.get_embeddings(sentence_1)\n",
    "        embeddings2 = self.get_embeddings(sentence_2)\n",
    "        \n",
    "        return cosine_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Class of SBERT Similarity\n",
    "class SentenceComparator_SBERT:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "        \n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NLI pipeline oluşturma (Türkçe destekleyen model kullanılabilir)\n",
    "nli_model = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "class SentenceComparator_NLI:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return pipeline(\"text-classification\", model=self.model_name)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        input_sentence = sentence_1 + ' [SEP] ' + sentence_2\n",
    "        result = self.model(input_sentence)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    sentences = [ \n",
    "        \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C kişisinin parası yoktur.\",\n",
    "        \"C kişisi bir banka müdürü.\"\n",
    "    ] \n",
    "\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for index2, sentence2 in enumerate(sentences):\n",
    "            if index != index2:\n",
    "\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                #print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\n",
    "                #print(\"Return: \\n\", result)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                #print(\"\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} model execution time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n",
    "\n",
    "# Test the NLI model\n",
    "#test_model(nli_model, \"nli_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\umutc\\AppData\\Local\\Temp\\ipykernel_4432\\2594203702.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 37.31837701797485 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 8.016632318496704 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m test_model( semantic_similarity, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m bert_cosine_similarity \u001b[38;5;241m=\u001b[39m SentenceComparator_bert_cosine(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m test_model(bert_cosine_similarity, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_cosine_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m sbert_similarity \u001b[38;5;241m=\u001b[39m SentenceComparator_SBERT(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-multilingual-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m test_model(sbert_similarity, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msbert_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[64], line 31\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, model_name)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index2, sentence2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m index2:\n\u001b[1;32m---> 31\u001b[0m         result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcalculate_similarity(sentence, sentence2)\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m#print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m#print(\"Return: \\n\", result)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         append_to_log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentence1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentence2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 25\u001b[0m, in \u001b[0;36mSentenceComparator_bert_cosine.calculate_similarity\u001b[1;34m(self, sentence_1, sentence_2)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_similarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_1, sentence_2):\n\u001b[1;32m---> 25\u001b[0m     embeddings1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embeddings(sentence_1)\n\u001b[0;32m     26\u001b[0m     embeddings2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embeddings(sentence_2)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity(embeddings1, embeddings2)\n",
      "Cell \u001b[1;32mIn[60], line 16\u001b[0m, in \u001b[0;36mSentenceComparator_bert_cosine.get_embeddings\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m---> 16\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[0;32m     17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2269\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2272\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2273\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2274\u001b[0m     init_configuration,\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2276\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2277\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2278\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2279\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2280\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2281\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2282\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2283\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2503\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2505\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2510\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:115\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocab_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m     )\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_file)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_tokens \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict([(ids, tok) \u001b[38;5;28;01mfor\u001b[39;00m tok, ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize \u001b[38;5;241m=\u001b[39m do_basic_tokenize\n",
      "File \u001b[1;32mc:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:35\u001b[0m, in \u001b[0;36mload_vocab\u001b[1;34m(vocab_file)\u001b[0m\n\u001b[0;32m     33\u001b[0m vocab \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m---> 35\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m     37\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_trace_dispatch_regular.py:326\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# ENDIF\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m''' This is the callback used when we enter some context in the debugger.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m        We also decorate the thread we are in with info about the debugging.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m            This is the global debugger (this method should actually be added as a method to it).\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;66;03m# IFDEF CYTHON\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# cdef str filename;\u001b[39;00m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# cdef str base;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;66;03m# DEBUG = 'code_to_debug' in frame.f_code.co_filename\u001b[39;00m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# if DEBUG: print('ENTER: trace_dispatch: %s %s %s %s' % (frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "test_model(nli_model, \"nli_model\")\n",
    "\n",
    "semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "test_model( semantic_similarity, \"semantic_similarity\")\n",
    "\n",
    "bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "\n",
    "sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "test_model(sbert_similarity, \"sbert_similarity\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
