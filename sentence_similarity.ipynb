{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a correct answer for the question in the sample.txt file\n",
    "correct_answer = {\n",
    "  \"Suçun_Temel_Unsurları\": {\n",
    "    \"Fail\": \"C\",\n",
    "    \"Mağdur\": \"Market (Mülkiyet hakkı ihlal edilen taraf)\",\n",
    "    \"Konu\": \"Marketten alınan ürünler\",\n",
    "    \"Fiil\": \"Ürünlerin kasada ödeme yapılmaksızın dışarı çıkarılması\",\n",
    "    \"Netice\": {\n",
    "      \"Marketin_malvarlığında_azalma\": True,\n",
    "      \"Nedensellik_Bağı\": \"C’nin fiili ile netice arasında nedensellik bağı mevcuttur.\",\n",
    "      \"Objektif_İsnadiyet\": \"Fiil, neticeyi doğuracak şekilde gerçekleştirilmiştir.\"\n",
    "    }\n",
    "  },\n",
    "  \"Manevi_Unsur\": \"C, bu eylemi kasten mi gerçekleştirmiştir? C’nin kasıtlı olup olmadığı, marketten çıkarken ödeme yapmadığını fark etmesiyle bağlantılıdır. C, ödeme yapmayı unuttuğunu iddia etmektedir, bu nedenle kast unsuru tartışmalı olabilir.\",\n",
    "  \"Hukuka_Aykırılık_Unsuru\": \"Hukuka aykırılık unsuru açısından, C’nin eylemi hırsızlık suçu açısından değerlendirildiğinde, hukuka aykırı bir durum mevcuttur. Ancak, manevi unsurdaki belirsizlik bu hususu etkileyebilir.\",\n",
    "  \"Suçun_Nitelikli_Unsurları\": \"Bu olayda nitelikli bir unsur yoktur.\",\n",
    "  \"Kusurluluk\": \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"Yaptırımın_Uygulanması_İçin_Gereken_Veya_Yaptırım_Uygulanmasını_Engelleyen_Özel_Koşullar\": \"C’nin ödeme yapmayı unuttuğunu iddia etmesi, suçun manevi unsurunu etkileyebilir ve bu durum yaptırım uygulanmasını engelleyebilir.\",\n",
    "  \"Suçun_Özel_Görünüş_Biçimleri\": {\n",
    "    \"Teşebbüs\": \"Eylem tamamlanmıştır, teşebbüs söz konusu değildir.\",\n",
    "    \"İştirak\": \"Olayda iştirak eden başka bir kişi yoktur.\",\n",
    "    \"İçtima\": \"Tek bir suç tipi söz konusudur.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "test_sentences = [\"Havayı kirletmek, çevreyi kirletmek anlamına gelir.\", \"Hava kirliliği, çevre kirliliğinin bir parçasıdır.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "Cosine similarity (%) : 0.0\n",
      "0.04399732626783181\n",
      "67.0\n"
     ]
    }
   ],
   "source": [
    "# THRASH CODE\n",
    "\n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = correct_answer[\"Kusurluluk\"]\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime          Benzerlik Skoru\n",
      "------------  -----------------\n",
      "ateşkes                0.823591\n",
      "uzlaşma                0.794623\n",
      "ittifak                0.774118\n",
      "antlaşma               0.773479\n",
      "müzakereler            0.762028\n",
      "barışı                 0.759955\n",
      "antlaşmayı             0.749817\n",
      "müzakereleri           0.747546\n",
      "mütareke               0.727691\n",
      "müttefiklik            0.717039\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"barış\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_Word2Vec:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.generate_model(model_name)\n",
    "\n",
    "    def generate_model(self, model_name):\n",
    "        return Word2Vec.load(model_name)\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    def extract_key_features(self, words_1, words_2):\n",
    "        \n",
    "        # Nested iteration to compare each word in the sentences\n",
    "        #Dict: {word_1:{word_comp1:score, word_comp2:score, ...}, word_2:{word_comp1:score, word_comp2:score, ...}}\n",
    "        searched_pairs = [];similarity_dict = {}\n",
    "\n",
    "        for word_1 in words_1:\n",
    "            if word_1 not in similarity_dict:\n",
    "                similarity_dict.update({word_1:{}})\n",
    "            for word_2 in words_2:\n",
    "                if (word_1, word_2) in searched_pairs or (word_2, word_1) in searched_pairs:\n",
    "                    continue\n",
    "                try:\n",
    "                    searched_pairs.append((word_1, word_2))\n",
    "                    # Calculate the similarity between the words\n",
    "                    similarity_dict[word_1].update({word_2:self.model.wv.similarity(word_1, word_2)})\n",
    "                except:\n",
    "                    pass\n",
    "        return similarity_dict\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Clean the sentences and split them into words\n",
    "        sentence_1 = self.clean_sentence(sentence_1); words_1 = sentence_1.split()\n",
    "        sentence_2 = self.clean_sentence(sentence_2); words_2 = sentence_2.split()\n",
    "\n",
    "        # Extract key features\n",
    "        similarity_dict = self.extract_key_features(words_1, words_2)\n",
    "            \n",
    "        # Extract informations from the similarity_dict\n",
    "        key_features = []\n",
    "        for key, value in similarity_dict.items():\n",
    "            if len(value) > 0:\n",
    "                # Sort and get the best match\n",
    "                sorted_dict = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "                max_score = sorted_dict[0][1]\n",
    "                best_key = sorted_dict[0][0]\n",
    "\n",
    "                key_features.append({\"key\":key, \"score\":max_score,\"best_match\":best_key})\n",
    "\n",
    "        # Calculate the average score\n",
    "        avg_score = sum([x[\"score\"] for x in key_features]) / len(key_features)\n",
    "\n",
    "        return avg_score#,key_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_comparator = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "#avg_score,sim_dict = word2vec_comparator.compare_sentences(test_sentences[0], test_sentences[1])\n",
    "#\n",
    "#print(tabulate([x.values() for x in sim_dict], headers=[\"Kelime\", \"En Benzer Kelime, Benzerlik Skoru\"]))\n",
    "#print(\"\\nAverage Score: \", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding with turkish trained word2vec\n",
    "#def calculate_vector_of_senetence(sentence):\n",
    "#    sentence_vector = np.zeros(100)\n",
    "#    for word in sentence.split():\n",
    "#        try:\n",
    "#            sentence_vector += model.wv[word]\n",
    "#        except:\n",
    "#            pass\n",
    "#    return sentence_vector\n",
    "#\n",
    "#def cosine_similarity(v1, v2):\n",
    "#    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "#\n",
    "#def measure_similarity(sentence_1, sentence_2):\n",
    "#    v1 = calculate_vector_of_senetence(sentence_1)\n",
    "#    v2 = calculate_vector_of_senetence(sentence_2)\n",
    "#    cos_sim = cosine_similarity(v1, v2)\n",
    "#\n",
    "#    print(\"\\nCosine similarity: \", cos_sim)\n",
    "#\n",
    "#\n",
    "#measure_similarity(\n",
    "#    test_sentences[0],\n",
    "#    test_sentences[1]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelfile System Prompt: \n",
    "\n",
    "Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {\"cümleler\":<cümleler>,\"analiz/neden\":<kısaca>,\"değerlendirme\": <>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class SentenceComparator_Ollama:\n",
    "    def __init__(self,modelfile_system, llama_version=\"llama3.1\", temperature=0.4):\n",
    "        self.system_prompt = modelfile_system\n",
    "        self.llama_version = llama_version\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.role_messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: C kişisi kasada ödeme yapmadan marketten çıkmıştır, İkinci cümle: C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 0\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: Havalar güzelken denize gitmek çok iyi olur., İkinci cümle: Bir insan ev almadan önce araba parası biriktirmeli.'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 1\"\n",
    "                }\n",
    "            ]\n",
    "        self.generate_model()\n",
    "    \n",
    "    def generate_model(self):\n",
    "        modelfile = f'''\n",
    "        FROM {self.llama_version}\n",
    "        SYSTEM {self.system_prompt} \n",
    "        PARAMETER temperature {self.temperature}\n",
    "        '''\n",
    "        ollama.create(model=f'MAHDAN_{self.llama_version}', modelfile=modelfile)\n",
    "\n",
    "    def calculate_similarity(self,sentence_1,sentence_2):\n",
    "        sentence_in = 'İlk Cümle: ' + sentence_1 + ' ,\\n İkinci Cümle: ' + sentence_2        \n",
    "\n",
    "        # Concat messages and sentence\n",
    "        messages_temp = self.role_messages.copy()\n",
    "\n",
    "        messages_temp.append({\n",
    "            'role': 'user',\n",
    "            'content': sentence_in\n",
    "        })\n",
    "\n",
    "        response = ollama.chat(model=f'MAHDAN_{self.llama_version}', messages= messages_temp)\n",
    "\n",
    "        return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )\n",
    "#\n",
    "#\n",
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "# Class of Semantic Similarity\n",
    "class SentenceComparator_semantic:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#model_name = 'bert-base-multilingual-cased'\n",
    "# Calculate cosine similarity between two sentences with BERT\n",
    "class SentenceComparator_bert_cosine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return BertModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentence):\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings1 = self.get_embeddings(sentence_1)\n",
    "        embeddings2 = self.get_embeddings(sentence_2)\n",
    "        \n",
    "        return cosine_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "#model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Class of SBERT Similarity\n",
    "class SentenceComparator_SBERT:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "        \n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çalışmak için *İnternet* gerektiriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NLI pipeline oluşturma (Türkçe destekleyen model kullanılabilir)\n",
    "#nli_model = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "class SentenceComparator_NLI:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return pipeline(\"text-classification\", model=self.model_name)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        input_sentence = sentence_1 + ' [SEP] ' + sentence_2\n",
    "        result = self.model(input_sentence)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    sentences = [ \n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C kişisi bir banka müdürü.\",\n",
    "        \"C kişisinin mesleği banka müdürlüğüdür.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"Leclerc, 2021 Formula 1 sezonunda 3. sırayı aldı.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\"\n",
    "    ] \n",
    "\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for index2, sentence2 in enumerate(sentences):\n",
    "            if index != index2:\n",
    "\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                #print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\n",
    "                #print(\"Return: \\n\", result)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                #print(\"\\n\")\n",
    "                computation_count += 1\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n",
    "\n",
    "# Test the NLI model\n",
    "#test_model(nli_model, \"nli_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "#word2vec_sim = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "#test_model(word2vec_sim, \"word2vec_sim\")\n",
    "#\n",
    "#sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "#                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "#                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "#                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "#\n",
    "#ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n",
    "#\n",
    "#Çöp\n",
    "#ollama_model_mistral = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_mistral, \"ollama_model_mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim</th>\n",
       "      <th>sbert_cos_sim</th>\n",
       "      <th>nli_model</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>word2vec_sim</th>\n",
       "      <th>ollama_model_llama3.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi ödeme yapmayı unutarak marketten çıkm...</td>\n",
       "      <td>0.626447</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>0.717026</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.330787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi hırsızdır ve hırsızlık suçu işlediği ...</td>\n",
       "      <td>0.692395</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi bir banka müdürü.</td>\n",
       "      <td>0.597883</td>\n",
       "      <td>0.2924</td>\n",
       "      <td>0.573564</td>\n",
       "      <td>0.4174</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.325234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>Leclerc, 2021 Formula 1 sezonunda 3. sırayı aldı.</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3263</td>\n",
       "      <td>0.243224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C kişisi ödeme yapmadı sonra da marketten çıka...</td>\n",
       "      <td>0.935725</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>0.898081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C kişisi kötü bir insan.</td>\n",
       "      <td>0.600229</td>\n",
       "      <td>0.3436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>0.475786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>Kusurlu olan C kişisi, ödeme yapmayı unuttuğun...</td>\n",
       "      <td>0.827527</td>\n",
       "      <td>0.7437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7963</td>\n",
       "      <td>0.782649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "151  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "152  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "153  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "154  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "155  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...      0.636648   \n",
       "2    C kişisi ödeme yapmayı unutarak marketten çıkm...      0.626447   \n",
       "3    C kişisi hırsızdır ve hırsızlık suçu işlediği ...      0.692395   \n",
       "4                           C kişisi bir banka müdürü.      0.597883   \n",
       "..                                                 ...           ...   \n",
       "151  C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.710953   \n",
       "152  Leclerc, 2021 Formula 1 sezonunda 3. sırayı aldı.      0.483920   \n",
       "153  C kişisi ödeme yapmadı sonra da marketten çıka...      0.935725   \n",
       "154                           C kişisi kötü bir insan.      0.600229   \n",
       "155  Kusurlu olan C kişisi, ödeme yapmayı unuttuğun...      0.827527   \n",
       "\n",
       "     sbert_cos_sim  nli_model  semantic_similarity  word2vec_sim  \\\n",
       "0           0.9383   0.769148               0.9611      0.946022   \n",
       "1           0.1996   0.589680               0.7214      0.398693   \n",
       "2           0.3011   0.717026               0.7751      0.330787   \n",
       "3           0.3268   0.619045               0.5339      0.513608   \n",
       "4           0.2924   0.573564               0.4174      0.316620   \n",
       "..             ...        ...                  ...           ...   \n",
       "151         0.3956        NaN               0.7210      0.325234   \n",
       "152         0.0482        NaN               0.3263      0.243224   \n",
       "153         0.9581        NaN               0.8807      0.898081   \n",
       "154         0.3436        NaN               0.4244      0.475786   \n",
       "155         0.7437        NaN               0.7963      0.782649   \n",
       "\n",
       "     ollama_model_llama3.1  \n",
       "0                        0  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        0  \n",
       "4                        0  \n",
       "..                     ...  \n",
       "151                      0  \n",
       "152                      0  \n",
       "153                      1  \n",
       "154                      0  \n",
       "155                      0  \n",
       "\n",
       "[156 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "# For sbert_similarity_log.xlsx\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "                        axis=1, \n",
    "                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "             \n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim</th>\n",
       "      <th>sbert_cos_sim</th>\n",
       "      <th>nli_model</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>word2vec_sim</th>\n",
       "      <th>ollama_model_llama3.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi ödeme yapmayı unutarak marketten çıkm...</td>\n",
       "      <td>0.626447</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>0.717026</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.330787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi hırsızdır ve hırsızlık suçu işlediği ...</td>\n",
       "      <td>0.692395</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi bir banka müdürü.</td>\n",
       "      <td>0.597883</td>\n",
       "      <td>0.2924</td>\n",
       "      <td>0.573564</td>\n",
       "      <td>0.4174</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.517482</td>\n",
       "      <td>0.7210</td>\n",
       "      <td>0.325234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>Leclerc, 2021 Formula 1 sezonunda 3. sırayı aldı.</td>\n",
       "      <td>0.483920</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.3263</td>\n",
       "      <td>0.243224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C kişisi ödeme yapmadı sonra da marketten çıka...</td>\n",
       "      <td>0.935725</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>0.780258</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>0.898081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>C kişisi kötü bir insan.</td>\n",
       "      <td>0.600229</td>\n",
       "      <td>0.3436</td>\n",
       "      <td>0.576259</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>0.475786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>C kişisi marketten çıkarken ödeme yapmayı unut...</td>\n",
       "      <td>Kusurlu olan C kişisi, ödeme yapmayı unuttuğun...</td>\n",
       "      <td>0.827527</td>\n",
       "      <td>0.7437</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>0.7963</td>\n",
       "      <td>0.782649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "151  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "152  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "153  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "154  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "155  C kişisi marketten çıkarken ödeme yapmayı unut...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...      0.636648   \n",
       "2    C kişisi ödeme yapmayı unutarak marketten çıkm...      0.626447   \n",
       "3    C kişisi hırsızdır ve hırsızlık suçu işlediği ...      0.692395   \n",
       "4                           C kişisi bir banka müdürü.      0.597883   \n",
       "..                                                 ...           ...   \n",
       "151  C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.710953   \n",
       "152  Leclerc, 2021 Formula 1 sezonunda 3. sırayı aldı.      0.483920   \n",
       "153  C kişisi ödeme yapmadı sonra da marketten çıka...      0.935725   \n",
       "154                           C kişisi kötü bir insan.      0.600229   \n",
       "155  Kusurlu olan C kişisi, ödeme yapmayı unuttuğun...      0.827527   \n",
       "\n",
       "     sbert_cos_sim  nli_model  semantic_similarity  word2vec_sim  \\\n",
       "0           0.9383   0.769148               0.9611      0.946022   \n",
       "1           0.1996   0.589680               0.7214      0.398693   \n",
       "2           0.3011   0.717026               0.7751      0.330787   \n",
       "3           0.3268   0.619045               0.5339      0.513608   \n",
       "4           0.2924   0.573564               0.4174      0.316620   \n",
       "..             ...        ...                  ...           ...   \n",
       "151         0.3956   0.517482               0.7210      0.325234   \n",
       "152         0.0482   0.638243               0.3263      0.243224   \n",
       "153         0.9581   0.780258               0.8807      0.898081   \n",
       "154         0.3436   0.576259               0.4244      0.475786   \n",
       "155         0.7437   0.646259               0.7963      0.782649   \n",
       "\n",
       "     ollama_model_llama3.1  \n",
       "0                        0  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        0  \n",
       "4                        0  \n",
       "..                     ...  \n",
       "151                      0  \n",
       "152                      0  \n",
       "153                      1  \n",
       "154                      0  \n",
       "155                      0  \n",
       "\n",
       "[156 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.875\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1\"]\n",
    "X = all_results_parameters.drop(\"ollama_model_llama3.1\", axis=1)\n",
    "\n",
    "normalized_X = (X - X.mean()) / X.std()\n",
    "\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6\n",
      "Recall:  0.6\n",
      "True Positive:  3\n",
      "True Negative:  25\n",
      "False Positive:  2\n",
      "False Negative:  2\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "print(\"True Positive: \", conf_matrix[1][1])\n",
    "print(\"True Negative: \", conf_matrix[0][0])\n",
    "print(\"False Positive: \", conf_matrix[0][1])\n",
    "print(\"False Negative: \", conf_matrix[1][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7646 - loss: 0.5767\n",
      "Epoch 2/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7825 - loss: 0.4355\n",
      "Epoch 3/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8189 - loss: 0.3914\n",
      "Epoch 4/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8378 - loss: 0.3952\n",
      "Epoch 5/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7627 - loss: 0.3933\n",
      "Epoch 6/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7788 - loss: 0.4143\n",
      "Epoch 7/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8657 - loss: 0.3289\n",
      "Epoch 8/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8500 - loss: 0.3674\n",
      "Epoch 9/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8497 - loss: 0.3381\n",
      "Epoch 10/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8426 - loss: 0.3942\n",
      "Epoch 11/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8587 - loss: 0.3406\n",
      "Epoch 12/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7621 - loss: 0.4388\n",
      "Epoch 13/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8786 - loss: 0.2957\n",
      "Epoch 14/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8388 - loss: 0.2878\n",
      "Epoch 15/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9013 - loss: 0.2774\n",
      "Epoch 16/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.2540\n",
      "Epoch 17/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8536 - loss: 0.3000\n",
      "Epoch 18/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8805 - loss: 0.2619\n",
      "Epoch 19/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8604 - loss: 0.2593\n",
      "Epoch 20/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8423 - loss: 0.2635\n",
      "Epoch 21/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9153 - loss: 0.2238\n",
      "Epoch 22/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9149 - loss: 0.2205\n",
      "Epoch 23/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8786 - loss: 0.2832\n",
      "Epoch 24/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.2330\n",
      "Epoch 25/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9044 - loss: 0.2117\n",
      "Epoch 26/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8901 - loss: 0.2300\n",
      "Epoch 27/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9239 - loss: 0.2120\n",
      "Epoch 28/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8962 - loss: 0.2043\n",
      "Epoch 29/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8630 - loss: 0.2709\n",
      "Epoch 30/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8926 - loss: 0.1924\n",
      "Epoch 31/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8552 - loss: 0.2663 \n",
      "Epoch 32/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9263 - loss: 0.1654 \n",
      "Epoch 33/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9256 - loss: 0.1817\n",
      "Epoch 34/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8895 - loss: 0.2284\n",
      "Epoch 35/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8937 - loss: 0.2042\n",
      "Epoch 36/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8551 - loss: 0.2944 \n",
      "Epoch 37/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9278 - loss: 0.1609\n",
      "Epoch 38/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9096 - loss: 0.1920\n",
      "Epoch 39/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8867 - loss: 0.1978\n",
      "Epoch 40/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9196 - loss: 0.1595\n",
      "Epoch 41/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9055 - loss: 0.1730\n",
      "Epoch 42/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9478 - loss: 0.1592\n",
      "Epoch 43/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8436 - loss: 0.2431 \n",
      "Epoch 44/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8937 - loss: 0.2348\n",
      "Epoch 45/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8891 - loss: 0.1893 \n",
      "Epoch 46/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.1361\n",
      "Epoch 47/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8968 - loss: 0.1619\n",
      "Epoch 48/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8963 - loss: 0.1644\n",
      "Epoch 49/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9090 - loss: 0.1615\n",
      "Epoch 50/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9155 - loss: 0.1438\n",
      "Epoch 51/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9354 - loss: 0.1786\n",
      "Epoch 52/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8985 - loss: 0.1840\n",
      "Epoch 53/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9213 - loss: 0.1702\n",
      "Epoch 54/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9018 - loss: 0.1787\n",
      "Epoch 55/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8904 - loss: 0.2000\n",
      "Epoch 56/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9269 - loss: 0.1691\n",
      "Epoch 57/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9717 - loss: 0.1061\n",
      "Epoch 58/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8778 - loss: 0.2023\n",
      "Epoch 59/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9322 - loss: 0.1380 \n",
      "Epoch 60/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9450 - loss: 0.1438 \n",
      "Epoch 61/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9096 - loss: 0.2049\n",
      "Epoch 62/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1479\n",
      "Epoch 63/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8672 - loss: 0.1718\n",
      "Epoch 64/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9242 - loss: 0.1541\n",
      "Epoch 65/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9252 - loss: 0.1532 \n",
      "Epoch 66/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9118 - loss: 0.1600\n",
      "Epoch 67/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9000 - loss: 0.1673\n",
      "Epoch 68/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9411 - loss: 0.1286\n",
      "Epoch 69/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9110 - loss: 0.1484\n",
      "Epoch 70/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9187 - loss: 0.1526\n",
      "Epoch 71/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9313 - loss: 0.1577\n",
      "Epoch 72/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9573 - loss: 0.1077\n",
      "Epoch 73/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9360 - loss: 0.1372\n",
      "Epoch 74/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9172 - loss: 0.1410\n",
      "Epoch 75/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9071 - loss: 0.1512\n",
      "Epoch 76/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8867 - loss: 0.1353\n",
      "Epoch 77/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9351 - loss: 0.1562\n",
      "Epoch 78/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9249 - loss: 0.1339 \n",
      "Epoch 79/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9208 - loss: 0.1299\n",
      "Epoch 80/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9231 - loss: 0.1481\n",
      "Epoch 81/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9365 - loss: 0.1356\n",
      "Epoch 82/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9282 - loss: 0.1351\n",
      "Epoch 83/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9555 - loss: 0.1128\n",
      "Epoch 84/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9087 - loss: 0.1514\n",
      "Epoch 85/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9545 - loss: 0.1105 \n",
      "Epoch 86/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9673 - loss: 0.1077 \n",
      "Epoch 87/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.1449 \n",
      "Epoch 88/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9446 - loss: 0.1150\n",
      "Epoch 89/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9042 - loss: 0.1747\n",
      "Epoch 90/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9518 - loss: 0.1077 \n",
      "Epoch 91/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1237 \n",
      "Epoch 92/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9233 - loss: 0.1383\n",
      "Epoch 93/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9549 - loss: 0.0957 \n",
      "Epoch 94/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9396 - loss: 0.1190 \n",
      "Epoch 95/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9451 - loss: 0.1018\n",
      "Epoch 96/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9683 - loss: 0.0961\n",
      "Epoch 97/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9199 - loss: 0.1336 \n",
      "Epoch 98/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0728\n",
      "Epoch 99/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9648 - loss: 0.0785\n",
      "Epoch 100/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9601 - loss: 0.1001\n",
      "Epoch 101/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9094 - loss: 0.1487\n",
      "Epoch 102/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9371 - loss: 0.1229 \n",
      "Epoch 103/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9346 - loss: 0.1015\n",
      "Epoch 104/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.0931 \n",
      "Epoch 105/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9533 - loss: 0.1008\n",
      "Epoch 106/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9393 - loss: 0.1153\n",
      "Epoch 107/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9151 - loss: 0.1268 \n",
      "Epoch 108/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.1654\n",
      "Epoch 109/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.0727\n",
      "Epoch 110/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9575 - loss: 0.0985\n",
      "Epoch 111/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.0959\n",
      "Epoch 112/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9167 - loss: 0.1271 \n",
      "Epoch 113/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8943 - loss: 0.1682 \n",
      "Epoch 114/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9634 - loss: 0.0846 \n",
      "Epoch 115/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9159 - loss: 0.1181 \n",
      "Epoch 116/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9413 - loss: 0.1388\n",
      "Epoch 117/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9870 - loss: 0.0551\n",
      "Epoch 118/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9278 - loss: 0.1103\n",
      "Epoch 119/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9377 - loss: 0.1093\n",
      "Epoch 120/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9675 - loss: 0.0716\n",
      "Epoch 121/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9464 - loss: 0.1273\n",
      "Epoch 122/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9596 - loss: 0.0928 \n",
      "Epoch 123/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9497 - loss: 0.0762 \n",
      "Epoch 124/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8656 - loss: 0.1679 \n",
      "Epoch 125/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9664 - loss: 0.0856 \n",
      "Epoch 126/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9457 - loss: 0.0935\n",
      "Epoch 127/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9046 - loss: 0.1149\n",
      "Epoch 128/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9090 - loss: 0.1023\n",
      "Epoch 129/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9659 - loss: 0.0849\n",
      "Epoch 130/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9261 - loss: 0.1074\n",
      "Epoch 131/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9473 - loss: 0.0788\n",
      "Epoch 132/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9477 - loss: 0.0839\n",
      "Epoch 133/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9404 - loss: 0.1458\n",
      "Epoch 134/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9583 - loss: 0.1078 \n",
      "Epoch 135/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9399 - loss: 0.1104\n",
      "Epoch 136/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9214 - loss: 0.0938 \n",
      "Epoch 137/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9253 - loss: 0.1258 \n",
      "Epoch 138/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9256 - loss: 0.0803\n",
      "Epoch 139/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9656 - loss: 0.0788 \n",
      "Epoch 140/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9305 - loss: 0.0805\n",
      "Epoch 141/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9297 - loss: 0.1020\n",
      "Epoch 142/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9173 - loss: 0.1241\n",
      "Epoch 143/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9625 - loss: 0.0606 \n",
      "Epoch 144/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9611 - loss: 0.0915\n",
      "Epoch 145/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9613 - loss: 0.0709\n",
      "Epoch 146/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9328 - loss: 0.0783\n",
      "Epoch 147/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9448 - loss: 0.0949 \n",
      "Epoch 148/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9684 - loss: 0.0724\n",
      "Epoch 149/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9668 - loss: 0.0615\n",
      "Epoch 150/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9812 - loss: 0.0724\n",
      "Epoch 151/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8958 - loss: 0.1282 \n",
      "Epoch 152/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9637 - loss: 0.0657\n",
      "Epoch 153/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9552 - loss: 0.0779\n",
      "Epoch 154/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9547 - loss: 0.1204\n",
      "Epoch 155/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9314 - loss: 0.0994\n",
      "Epoch 156/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.0716 \n",
      "Epoch 157/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9631 - loss: 0.0715 \n",
      "Epoch 158/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9315 - loss: 0.1036\n",
      "Epoch 159/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9804 - loss: 0.0400\n",
      "Epoch 160/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9734 - loss: 0.0661\n",
      "Epoch 161/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9302 - loss: 0.1082\n",
      "Epoch 162/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9664 - loss: 0.0738\n",
      "Epoch 163/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9617 - loss: 0.0831\n",
      "Epoch 164/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9337 - loss: 0.1287 \n",
      "Epoch 165/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9066 - loss: 0.1129 \n",
      "Epoch 166/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9234 - loss: 0.1127 \n",
      "Epoch 167/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9335 - loss: 0.0900\n",
      "Epoch 168/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9445 - loss: 0.0711 \n",
      "Epoch 169/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9865 - loss: 0.0489\n",
      "Epoch 170/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9623 - loss: 0.0873\n",
      "Epoch 171/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9577 - loss: 0.0729\n",
      "Epoch 172/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.0615 \n",
      "Epoch 173/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9674 - loss: 0.0814 \n",
      "Epoch 174/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9385 - loss: 0.0893 \n",
      "Epoch 175/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9521 - loss: 0.0574 \n",
      "Epoch 176/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9653 - loss: 0.0907\n",
      "Epoch 177/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9873 - loss: 0.0488\n",
      "Epoch 178/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.0748\n",
      "Epoch 179/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0403 \n",
      "Epoch 180/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.0711 \n",
      "Epoch 181/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9653 - loss: 0.0663\n",
      "Epoch 182/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9550 - loss: 0.0758 \n",
      "Epoch 183/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9590 - loss: 0.0841\n",
      "Epoch 184/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9772 - loss: 0.0556 \n",
      "Epoch 185/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9485 - loss: 0.0688\n",
      "Epoch 186/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9534 - loss: 0.0720\n",
      "Epoch 187/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9737 - loss: 0.0564 \n",
      "Epoch 188/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9774 - loss: 0.0567 \n",
      "Epoch 189/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9269 - loss: 0.0891 \n",
      "Epoch 190/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9501 - loss: 0.0648 \n",
      "Epoch 191/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9391 - loss: 0.0741 \n",
      "Epoch 192/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9730 - loss: 0.0699 \n",
      "Epoch 193/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9369 - loss: 0.0691 \n",
      "Epoch 194/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9713 - loss: 0.0668\n",
      "Epoch 195/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9523 - loss: 0.0528\n",
      "Epoch 196/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9412 - loss: 0.0649 \n",
      "Epoch 197/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9654 - loss: 0.0828\n",
      "Epoch 198/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.0739\n",
      "Epoch 199/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9254 - loss: 0.1143 \n",
      "Epoch 200/200\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9547 - loss: 0.0582 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8438 - loss: 1.8548\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Accuracy:  0.84375\n"
     ]
    }
   ],
   "source": [
    "# Keras Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=1)\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
