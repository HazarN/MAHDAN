{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Cosine similarity (%) : 91.66666666666669\n",
      "0.8477624970048978\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# !!! This section explains why we should not compare two sentences with these methods. !!! \n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilemez.\"\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime        Benzerlik Skoru\n",
      "----------  -----------------\n",
      "Mahkeme              0.860295\n",
      "mahkemenin           0.813442\n",
      "davanın              0.806494\n",
      "tutuklama            0.799902\n",
      "soruşturma           0.791518\n",
      "temyiz               0.771838\n",
      "mahkemede            0.771153\n",
      "dava                 0.770335\n",
      "yargılama            0.769724\n",
      "savcılık             0.730116\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "model = Word2Vec.load(\"utils/word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"mahkeme\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_similarity_comperators import SentenceComparator_Word2Vec,\\\n",
    "                                            SentenceComparator_Ollama,\\\n",
    "                                            SentenceComparator_semantic,\\\n",
    "                                            SentenceComparator_bert_cosine,\\\n",
    "                                            SentenceComparator_SBERT,\\\n",
    "                                            SentenceComparator_NLI,\\\n",
    "                                            SentenceComparator_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "    \"\"\"\n",
    "    This function tests the given model with the given name.\n",
    "\n",
    "    Args:\n",
    "        model(SentenceComparator): A SentenceComparator object.\n",
    "        model_name(str): A string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    test_sentences = [\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi kasada ödeme yapmadan marketten çıkmıştır.\",\n",
    "        \"C kasaya ödeme yapması gerekirken yapmamıştır.\",\n",
    "        \"C markete girdi ve sonra ödeme yapmadan çıktı.\",\n",
    "        \"Şahıs aldığı ürünleri parasını ödemeden çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi kesin unutkan birisidir ve ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Ben C kişisinin kötü birisi olduğunu biliyorum.\",\n",
    "        \"C kişisi iyi bir insan değil.\",\n",
    "        \"Kötü bir insan olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi marketten satın aldığı ürünleri kasada ödeme yapmadan çıkarmıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kameralarıyla doğrulanmıştır.\",\n",
    "        \"C kişisinin kasada ödeme yapmadan çıkması bilinçli bir eylem olarak değerlendirilebilir.\",\n",
    "        \"C kasada ödeme yapmadığı için sorumlu tutulmalıdır.\",\n",
    "        \"C kişisinin ödeme yapmadığına dair hiçbir kanıt bulunmamaktadır.\",\n",
    "        \"Market çalışanları, C'nin ödeme yapmadığını fark etmiştir.\",\n",
    "        \"C kişisi ödeme yapmayı unuttuğunu savunmaktadır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası asılsızdır.\",\n",
    "        \"C'nin kasada ödeme yapmaması kasıtlı bir davranış olarak değerlendirilemez.\",\n",
    "        \"C, dikkat eksikliği nedeniyle ödeme yapmayı unutmuş olabilir.\",\n",
    "        \"C kişisi ödeme yapmadan çıkmayı bir hata olarak tanımlamıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kayıtlarıyla teyit edilmiştir.\",\n",
    "        \"C'nin kasadan ödeme yapmadan ayrılması bilinçli bir davranış olarak nitelendirilebilir.\",\n",
    "        \"C, kasada ödeme yapmadığı için sorumluluk almalıdır.\",\n",
    "        \"C'nin ödeme yapmadığına dair herhangi bir kanıt yoktur.\",\n",
    "        \"Market çalışanları, C’nin kasada ödeme yapmadığını fark etti.\",\n",
    "        \"C kişisi, ödeme yapmayı unuttuğunu iddia ediyor.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası gerçeği yansıtmamaktadır.\",\n",
    "        \"C'nin ödeme yapmaması kasıtlı olarak değerlendirilemez.\",\n",
    "        \"C'nin dikkatsizliği yüzünden ödemeyi unutmuş olabileceği düşünülüyor.\",\n",
    "        \"C kişisi, ödeme yapmadan ayrılmayı bir hata olarak kabul etmiştir.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create log and excel files\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "    create_log(\"model_exec_times.txt\", f\"This file contains the execution times of the models. Number of test_sentences: {len(test_sentences)}\")\n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    elif model_name == \"sentiment_analysis\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"sentiment_1\": [], \"sentiment_2\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    compared_sentence_pairs = []\n",
    "    for index, sentence in enumerate(test_sentences):\n",
    "        for index2, sentence2 in enumerate(test_sentences):\n",
    "            if index != index2 and ( ( sentence, sentence2 ) not in compared_sentence_pairs\\\n",
    "                               and   ( sentence2, sentence ) not in compared_sentence_pairs):\n",
    "\n",
    "                # Calculate the similarity is a essential function of SentenceComparator classes\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                elif model_name == \"sentiment_analysis\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"sentiment_1\": result[0], \"sentiment_2\": result[1]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                \n",
    "                computation_count += 1\n",
    "                compared_sentence_pairs.append((sentence, sentence2))\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "#word2vec_sim = SentenceComparator_Word2Vec(\"utils/word2vec/w2v_.model\")\n",
    "#test_model(word2vec_sim, \"word2vec_sim\")\n",
    "#\n",
    "#sentiment_analysis = SentenceComparator_sentiment_analysis()\n",
    "#test_model(sentiment_analysis, \"sentiment_analysis\")\n",
    "#\n",
    "#sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "#                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "#                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "#                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "#\n",
    "#ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_LABEL_1_1</th>\n",
       "      <th>sentiment_analysis_LABEL_1_2</th>\n",
       "      <th>sentiment_analysis_LABEL_2_1</th>\n",
       "      <th>sentiment_analysis_LABEL_2_2</th>\n",
       "      <th>ollama_model_llama3.1_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi kasada ödeme yapmadan marketten çıkmı...</td>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kasaya ödeme yapması gerekirken yapmamıştır.</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C markete girdi ve sonra ödeme yapmadan çıktı.</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "699  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "700  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "701  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "702  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "703  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim_Result  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...             0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...             0.636648   \n",
       "2    C kişisi kasada ödeme yapmadan marketten çıkmı...             0.625543   \n",
       "3       C kasaya ödeme yapması gerekirken yapmamıştır.             0.657169   \n",
       "4       C markete girdi ve sonra ödeme yapmadan çıktı.             0.587133   \n",
       "..                                                 ...                  ...   \n",
       "699  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.697819   \n",
       "700  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.718892   \n",
       "701  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.703241   \n",
       "702  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.764747   \n",
       "703  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.766007   \n",
       "\n",
       "     sbert_cos_sim_Result  nli_model_CONTRADICTION  nli_model_ENTAILMENT  \\\n",
       "0                  0.9383                        0                     1   \n",
       "1                  0.1996                        1                     0   \n",
       "2                  0.1646                        1                     0   \n",
       "3                  0.4852                        1                     0   \n",
       "4                  0.2732                        1                     0   \n",
       "..                    ...                      ...                   ...   \n",
       "699                0.7362                        1                     0   \n",
       "700                0.6233                        1                     0   \n",
       "701                0.6767                        1                     0   \n",
       "702                0.6354                        1                     0   \n",
       "703                0.8142                        0                     0   \n",
       "\n",
       "     nli_model_NEUTRAL  nli_model_Result  semantic_similarity_Result  \\\n",
       "0                    0          0.769148                      0.9611   \n",
       "1                    0          0.589680                      0.7214   \n",
       "2                    0          0.747958                      0.7042   \n",
       "3                    0          0.710494                      0.6675   \n",
       "4                    0          0.737624                      0.6462   \n",
       "..                 ...               ...                         ...   \n",
       "699                  0          0.844054                      0.6441   \n",
       "700                  0          0.514596                      0.6645   \n",
       "701                  0          0.601376                      0.7230   \n",
       "702                  0          0.867559                      0.6490   \n",
       "703                  1          0.532981                      0.5583   \n",
       "\n",
       "     word2vec_sim_Result  sentiment_analysis_LABEL_1_1  \\\n",
       "0               0.946022                             1   \n",
       "1               0.398693                             1   \n",
       "2               0.312992                             1   \n",
       "3               0.391442                             1   \n",
       "4               0.396546                             1   \n",
       "..                   ...                           ...   \n",
       "699             0.515694                             0   \n",
       "700             0.496692                             0   \n",
       "701             0.457836                             1   \n",
       "702             0.658151                             1   \n",
       "703             0.408180                             1   \n",
       "\n",
       "     sentiment_analysis_LABEL_1_2  sentiment_analysis_LABEL_2_1  \\\n",
       "0                               0                             0   \n",
       "1                               1                             0   \n",
       "2                               1                             0   \n",
       "3                               0                             0   \n",
       "4                               1                             0   \n",
       "..                            ...                           ...   \n",
       "699                             1                             1   \n",
       "700                             1                             1   \n",
       "701                             1                             0   \n",
       "702                             1                             0   \n",
       "703                             1                             0   \n",
       "\n",
       "     sentiment_analysis_LABEL_2_2  ollama_model_llama3.1_Result  \n",
       "0                               1                             0  \n",
       "1                               0                             1  \n",
       "2                               0                             1  \n",
       "3                               1                             1  \n",
       "4                               0                             0  \n",
       "..                            ...                           ...  \n",
       "699                             0                             1  \n",
       "700                             0                             1  \n",
       "701                             0                             0  \n",
       "702                             0                             0  \n",
       "703                             0                             1  \n",
       "\n",
       "[704 rows x 15 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For sbert_similarity\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "\n",
    "# Dummy encoding\n",
    "dummy = pd.get_dummies(nli_df[\"label\"])\n",
    "nli_df.drop(\"label\", axis=1, inplace=True)\n",
    "nli_df = pd.concat([nli_df, dummy], axis=1)\n",
    "\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "########################################\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_df = excel_to_df(\"sentiment_analysis_log.xlsx\", \"Results\")\n",
    "\n",
    "\"\"\"\n",
    "There are two options for processing sentiment analysis results.\n",
    "\n",
    "1. We can calculate sentiment score as a difference between sentiment_1 and sentiment_2. \n",
    "If they are equal, the score will be 1. Otherwise, the score will be 0. With this approach, we have a binary classification problem.\n",
    "\n",
    "2. We can use sentiment_1 and sentiment_2 as two separate features. With this approach, \n",
    "we have a multi-class classification problem. The first sentence have two labels, and the second sentence have two labels.\n",
    "Threfore, we have 4 labels in total.\n",
    "\n",
    "\"\"\"\n",
    "# Dummy encoding Option 2\n",
    "dummy_1 = pd.get_dummies(sentiment_df[\"sentiment_1\"])\n",
    "dummy_2 = pd.get_dummies(sentiment_df[\"sentiment_2\"])\n",
    "\n",
    "rename_all_columns = lambda df, suffix: df.rename(columns={col: col + suffix for col in df.columns})\n",
    "\n",
    "dummy_1 = rename_all_columns(dummy_1, \"_setnence1\")\n",
    "dummy_2 = rename_all_columns(dummy_2, \"_setnence2\")\n",
    "\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "sentiment_df = pd.concat([sentiment_df, dummy_1, dummy_2], axis=1)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:Cüaıbzkfakı .23456789]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "#all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], sentiment_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "#                        axis=1, \n",
    "#                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "\n",
    "def concat_columns_except_sentences(df_list, df_list_names):\n",
    "    initial_df = df_list[0].iloc[:,:2]\n",
    "    for index, df in enumerate(df_list):\n",
    "        df_except_sentences = df[ df.columns.difference([\"Sentence1\", \"Sentence2\"]) ] * 1\n",
    "        df_except_sentences.columns = [f\"{df_list_names[index]}_{col}\" for col in df_except_sentences.columns]\n",
    "        initial_df = pd.concat([initial_df, df_except_sentences], axis=1)\n",
    "    return initial_df\n",
    "\n",
    "all_results = concat_columns_except_sentences(\n",
    "    [bert_cos_sim, sbert_cos_df, nli_df, semantic_df, word2vec_df, sentiment_df, ollama_df], \n",
    "    [\"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "             \n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "#all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Normalize lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1_Result\"]\n",
    "X = all_results_parameters.drop(\"ollama_model_llama3.1_Result\", axis=1)\n",
    "\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg, Accuracy:  0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Reg, Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_LABEL_1</th>\n",
       "      <th>sentiment_analysis_LABEL_1</th>\n",
       "      <th>sentiment_analysis_LABEL_2</th>\n",
       "      <th>sentiment_analysis_LABEL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_cos_sim_Result  sbert_cos_sim_Result  nli_model_CONTRADICTION  \\\n",
       "0               0.964577                0.9383                        0   \n",
       "1               0.636648                0.1996                        1   \n",
       "2               0.625543                0.1646                        1   \n",
       "3               0.657169                0.4852                        1   \n",
       "4               0.587133                0.2732                        1   \n",
       "..                   ...                   ...                      ...   \n",
       "699             0.697819                0.7362                        1   \n",
       "700             0.718892                0.6233                        1   \n",
       "701             0.703241                0.6767                        1   \n",
       "702             0.764747                0.6354                        1   \n",
       "703             0.766007                0.8142                        0   \n",
       "\n",
       "     nli_model_ENTAILMENT  nli_model_NEUTRAL  nli_model_Result  \\\n",
       "0                       1                  0          0.769148   \n",
       "1                       0                  0          0.589680   \n",
       "2                       0                  0          0.747958   \n",
       "3                       0                  0          0.710494   \n",
       "4                       0                  0          0.737624   \n",
       "..                    ...                ...               ...   \n",
       "699                     0                  0          0.844054   \n",
       "700                     0                  0          0.514596   \n",
       "701                     0                  0          0.601376   \n",
       "702                     0                  0          0.867559   \n",
       "703                     0                  1          0.532981   \n",
       "\n",
       "     semantic_similarity_Result  word2vec_sim_Result  \\\n",
       "0                        0.9611             0.946022   \n",
       "1                        0.7214             0.398693   \n",
       "2                        0.7042             0.312992   \n",
       "3                        0.6675             0.391442   \n",
       "4                        0.6462             0.396546   \n",
       "..                          ...                  ...   \n",
       "699                      0.6441             0.515694   \n",
       "700                      0.6645             0.496692   \n",
       "701                      0.7230             0.457836   \n",
       "702                      0.6490             0.658151   \n",
       "703                      0.5583             0.408180   \n",
       "\n",
       "     sentiment_analysis_LABEL_1  sentiment_analysis_LABEL_1  \\\n",
       "0                             1                           0   \n",
       "1                             1                           1   \n",
       "2                             1                           1   \n",
       "3                             1                           0   \n",
       "4                             1                           1   \n",
       "..                          ...                         ...   \n",
       "699                           0                           1   \n",
       "700                           0                           1   \n",
       "701                           1                           1   \n",
       "702                           1                           1   \n",
       "703                           1                           1   \n",
       "\n",
       "     sentiment_analysis_LABEL_2  sentiment_analysis_LABEL_2  \n",
       "0                             0                           1  \n",
       "1                             0                           0  \n",
       "2                             0                           0  \n",
       "3                             0                           1  \n",
       "4                             0                           0  \n",
       "..                          ...                         ...  \n",
       "699                           1                           0  \n",
       "700                           1                           0  \n",
       "701                           0                           0  \n",
       "702                           0                           0  \n",
       "703                           0                           0  \n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44644703,  0.37668813,  0.05561013,  0.06242784, -0.09295003,\n",
       "        0.13803581,  0.3736199 , -0.55249954, -0.07450976, -0.01439088,\n",
       "        0.07450976,  0.01439088])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.5384615384615384\n",
      "Recall:  0.175\n",
      "True Positive:  7\n",
      "True Negative:  95\n",
      "False Positive:  6\n",
      "False Negative:  33\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_conf_matrix(y_test, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    print(\"True Positive: \", conf_matrix[1][1])\n",
    "    print(\"True Negative: \", conf_matrix[0][0])\n",
    "    print(\"False Positive: \", conf_matrix[0][1])\n",
    "    print(\"False Negative: \", conf_matrix[1][0])\n",
    "    \n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=100, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model2\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "def keras_model_tuner(hp):\n",
    "    hidden_layer_num = hp.Int('hidden_layer_num', min_value=1, max_value=5, step=1)\n",
    "    layer_unit = hp.Int('layer_unit', min_value=16, max_value=128, step=16)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer_unit, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(hidden_layer_num):\n",
    "        model.add(layers.Dense(layer_unit, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='ollama_model2'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001 )\n",
    "\n",
    "#tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model2\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Accuracy:  0.75177304964539\n",
      "Precision:  0.8571428571428571\n",
      "Recall:  0.15\n",
      "True Positive:  6\n",
      "True Negative:  100\n",
      "False Positive:  1\n",
      "False Negative:  34\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='ollama_model2'\n",
    ")\n",
    "\n",
    "tuner.reload()\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_columns(df, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    This function adds the Confidence, Real Value, and Prediction columns to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    if type(df) != pd.DataFrame:\n",
    "        df = pd.DataFrame(df)\n",
    "    df[\"Confidence\"] = 1.0\n",
    "    df[\"Real Value\"] = 5\n",
    "    df[\"Prediction\"] = 5\n",
    "    df[\"Accuracy\"] = 5\n",
    "    #print(df.head())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, val in enumerate(y_pred):\n",
    "        \n",
    "        real =  y_test.iloc[index]\n",
    "        pred = 0 if val < 0.5 else 1\n",
    "        confidence = (val-0.5)*2 if pred == 1 else (0.5-val)*2\n",
    "        \n",
    "        df[\"Confidence\"][index] = confidence\n",
    "        df[\"Real Value\"][index] = real\n",
    "        df[\"Prediction\"][index] = pred\n",
    "        df[\"Accuracy\"][index] = 1 if real == pred else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_accuracy(x,y,predictor):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predictor.\n",
    "\n",
    "    Args:\n",
    "        x: A pandas dataframe.\n",
    "        y: A pandas dataframe.\n",
    "        predictor: A predictor model.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: A float.\n",
    "    \"\"\"\n",
    "    y_pred = predictor.predict(x)\n",
    "    y_pred = [1 if val > 0.5 else 0 for val in y_pred]\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test Accuracy:  0.75177304964539\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Train Accuracy:  0.7122557726465364\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Entire Accuracy:  0.7201704545454546\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Test Accuracy: \", calculate_accuracy(X_test, y_test, model))\n",
    "# Train\n",
    "print(\"Train Accuracy: \", calculate_accuracy(X_train, y_train, model))\n",
    "# Entire\n",
    "print(\"Entire Accuracy: \", calculate_accuracy(normalized_X, y, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815188</td>\n",
       "      <td>1.590520</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.817017</td>\n",
       "      <td>-0.407003</td>\n",
       "      <td>-1.016843</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271673</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.436070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>-0.582501</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.225806</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>-0.524713</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.215165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678161</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.522652</td>\n",
       "      <td>-0.037228</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.401310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688566</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.574057</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>0.070940</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.518599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.286441</td>\n",
       "      <td>-0.745840</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.842474</td>\n",
       "      <td>0.195577</td>\n",
       "      <td>1.072469</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.561216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.351238</td>\n",
       "      <td>-0.550725</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.255695</td>\n",
       "      <td>0.718084</td>\n",
       "      <td>-0.655045</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.351091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.256504</td>\n",
       "      <td>0.109878</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.441526</td>\n",
       "      <td>0.229506</td>\n",
       "      <td>-0.615179</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.221825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.433858</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.138172</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>-0.462409</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.203513</td>\n",
       "      <td>0.724211</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.640465</td>\n",
       "      <td>1.445522</td>\n",
       "      <td>1.360279</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.442997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.815188  1.590520 -0.892143  3.009509 -0.918113 -0.817017 -0.407003   \n",
       "1    1.271673  0.764906  1.120897 -0.332280 -0.918113 -0.325933  0.588475   \n",
       "2    0.787798 -0.582501 -0.892143  3.009509 -0.918113 -1.225806  0.475152   \n",
       "3    0.678161 -0.317145  1.120897 -0.332280 -0.918113  0.638371  0.522652   \n",
       "4    0.688566 -0.634903  1.120897 -0.332280 -0.918113 -0.574057 -1.054368   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "136  0.286441 -0.745840  1.120897 -0.332280 -0.918113 -0.842474  0.195577   \n",
       "137  0.351238 -0.550725 -0.892143 -0.332280  1.089190 -0.255695  0.718084   \n",
       "138  0.256504  0.109878  1.120897 -0.332280 -0.918113  1.441526  0.229506   \n",
       "139 -0.433858  0.238654 -0.892143 -0.332280  1.089190  1.138172  0.382865   \n",
       "140  0.203513  0.724211 -0.892143 -0.332280  1.089190 -0.640465  1.445522   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "0   -1.016843  0.570789  0.377964 -0.570789 -0.377964    0.017966           1   \n",
       "1    1.624017  0.570789 -2.645751 -0.570789  2.645751    0.436070           1   \n",
       "2   -0.524713  0.570789  0.377964 -0.570789 -0.377964    0.215165           1   \n",
       "3   -0.037228  0.570789  0.377964 -0.570789 -0.377964    0.401310           1   \n",
       "4    0.070940  0.570789  0.377964 -0.570789 -0.377964    0.518599           1   \n",
       "..        ...       ...       ...       ...       ...         ...         ...   \n",
       "136  1.072469  0.570789  0.377964 -0.570789 -0.377964    0.561216           0   \n",
       "137 -0.655045  0.570789  0.377964 -0.570789 -0.377964    0.351091           0   \n",
       "138 -0.615179 -1.751960  0.377964  1.751960 -0.377964    0.221825           0   \n",
       "139 -0.462409  0.570789  0.377964 -0.570789 -0.377964    0.346781           1   \n",
       "140  1.360279  0.570789  0.377964 -0.570789 -0.377964    0.442997           0   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         0  \n",
       "..          ...       ...  \n",
       "136           0         1  \n",
       "137           0         1  \n",
       "138           0         1  \n",
       "139           0         0  \n",
       "140           0         1  \n",
       "\n",
       "[141 rows x 16 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Error rate: % 3.225806451612903       Number of high confidence predictions:  31\n",
      "Number of faults:  35    Faults from high confidence predictions:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815188</td>\n",
       "      <td>1.590520</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.817017</td>\n",
       "      <td>-0.407003</td>\n",
       "      <td>-1.016843</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271673</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.436070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>-0.582501</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.225806</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>-0.524713</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.215165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678161</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.522652</td>\n",
       "      <td>-0.037228</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.401310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688566</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.574057</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>0.070940</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.518599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.880661</td>\n",
       "      <td>-0.038967</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.522799</td>\n",
       "      <td>0.362507</td>\n",
       "      <td>-0.760867</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.316467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.277258</td>\n",
       "      <td>-0.140984</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.540841</td>\n",
       "      <td>0.336721</td>\n",
       "      <td>0.105899</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.372742</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.039912</td>\n",
       "      <td>0.067510</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.510728</td>\n",
       "      <td>0.276328</td>\n",
       "      <td>-0.208648</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.470710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.168701</td>\n",
       "      <td>-0.763679</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-1.038817</td>\n",
       "      <td>-0.791758</td>\n",
       "      <td>-0.845175</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.654165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.712509</td>\n",
       "      <td>-0.548495</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.409182</td>\n",
       "      <td>-0.968867</td>\n",
       "      <td>-1.415428</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.373990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.507932</td>\n",
       "      <td>0.823441</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.556588</td>\n",
       "      <td>-0.909831</td>\n",
       "      <td>-1.208781</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.275613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.039668</td>\n",
       "      <td>-0.634346</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.744138</td>\n",
       "      <td>-0.896259</td>\n",
       "      <td>-0.962822</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.552538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.446974</td>\n",
       "      <td>0.667906</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.854990</td>\n",
       "      <td>0.526045</td>\n",
       "      <td>-0.833751</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.161035</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.326526</td>\n",
       "      <td>-0.013323</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.068289</td>\n",
       "      <td>-0.559683</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.480271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.549689</td>\n",
       "      <td>-0.493305</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.384092</td>\n",
       "      <td>-0.126749</td>\n",
       "      <td>0.093489</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.587611</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.796507</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.122937</td>\n",
       "      <td>0.109397</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.464666</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.282194</td>\n",
       "      <td>-0.462645</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.150760</td>\n",
       "      <td>0.321114</td>\n",
       "      <td>0.457528</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.416547</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.400726</td>\n",
       "      <td>0.187366</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.979483</td>\n",
       "      <td>1.049231</td>\n",
       "      <td>-0.595118</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.313304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.330753</td>\n",
       "      <td>-0.717409</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>0.948774</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.597320</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.484438</td>\n",
       "      <td>0.718762</td>\n",
       "      <td>-0.150209</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.226870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.064504</td>\n",
       "      <td>1.528641</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.787594</td>\n",
       "      <td>1.670810</td>\n",
       "      <td>1.312323</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.305227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.747359</td>\n",
       "      <td>1.209210</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>0.896550</td>\n",
       "      <td>0.910501</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.482929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.932159</td>\n",
       "      <td>0.913193</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.786369</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.367950</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-1.108159</td>\n",
       "      <td>-2.200838</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.740699</td>\n",
       "      <td>0.216613</td>\n",
       "      <td>-1.578397</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.118627</td>\n",
       "      <td>-0.711834</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.958481</td>\n",
       "      <td>0.469045</td>\n",
       "      <td>-0.211750</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.402025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.278953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.144541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.839466</td>\n",
       "      <td>-1.832908</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.020924</td>\n",
       "      <td>-0.931545</td>\n",
       "      <td>-0.877639</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.552177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.375575</td>\n",
       "      <td>-0.061266</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.361450</td>\n",
       "      <td>0.561331</td>\n",
       "      <td>-0.744705</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.543801</td>\n",
       "      <td>-1.504557</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.348936</td>\n",
       "      <td>-0.358731</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.466926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.529476</td>\n",
       "      <td>1.927233</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.157388</td>\n",
       "      <td>0.635297</td>\n",
       "      <td>1.727076</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.665267</td>\n",
       "      <td>-0.115340</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.101153</td>\n",
       "      <td>0.715369</td>\n",
       "      <td>-0.430724</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.487033</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.577954</td>\n",
       "      <td>-0.520064</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.460191</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>-0.881556</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.485424</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-1.097475</td>\n",
       "      <td>-1.439891</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.535934</td>\n",
       "      <td>0.697726</td>\n",
       "      <td>-1.442881</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.393076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.433858</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.138172</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>-0.462409</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.815188  1.590520 -0.892143  3.009509 -0.918113 -0.817017 -0.407003   \n",
       "1    1.271673  0.764906  1.120897 -0.332280 -0.918113 -0.325933  0.588475   \n",
       "2    0.787798 -0.582501 -0.892143  3.009509 -0.918113 -1.225806  0.475152   \n",
       "3    0.678161 -0.317145  1.120897 -0.332280 -0.918113  0.638371  0.522652   \n",
       "4    0.688566 -0.634903  1.120897 -0.332280 -0.918113 -0.574057 -1.054368   \n",
       "14  -0.880661 -0.038967  1.120897 -0.332280 -0.918113 -0.522799  0.362507   \n",
       "16   0.277258 -0.140984 -0.892143 -0.332280  1.089190  1.540841  0.336721   \n",
       "18  -1.039912  0.067510 -0.892143 -0.332280  1.089190  0.510728  0.276328   \n",
       "25  -1.168701 -0.763679 -0.892143 -0.332280  1.089190 -1.038817 -0.791758   \n",
       "33  -0.712509 -0.548495  1.120897 -0.332280 -0.918113 -0.409182 -0.968867   \n",
       "40  -0.507932  0.823441  1.120897 -0.332280 -0.918113  0.556588 -0.909831   \n",
       "42  -0.039668 -0.634346 -0.892143 -0.332280  1.089190 -0.744138 -0.896259   \n",
       "44   0.446974  0.667906 -0.892143  3.009509 -0.918113 -1.854990  0.526045   \n",
       "48  -0.326526 -0.013323  1.120897 -0.332280 -0.918113  0.068289 -0.559683   \n",
       "52  -0.549689 -0.493305 -0.892143 -0.332280  1.089190  0.384092 -0.126749   \n",
       "53   0.796507  0.013436 -0.892143 -0.332280  1.089190  1.122937  0.109397   \n",
       "65   0.282194 -0.462645  1.120897 -0.332280 -0.918113 -1.150760  0.321114   \n",
       "66   0.400726  0.187366 -0.892143 -0.332280  1.089190 -0.979483  1.049231   \n",
       "69  -0.330753 -0.717409  1.120897 -0.332280 -0.918113  0.407301  0.222720   \n",
       "73   1.391350  1.021343 -0.892143 -0.332280  1.089190 -0.484438  0.718762   \n",
       "74   0.064504  1.528641 -0.892143 -0.332280  1.089190  1.787594  1.670810   \n",
       "79   0.747359  1.209210 -0.892143 -0.332280  1.089190  0.376738  0.896550   \n",
       "81   0.932159  0.913193 -0.892143 -0.332280  1.089190 -0.786369  0.077504   \n",
       "83  -1.108159 -2.200838  1.120897 -0.332280 -0.918113  0.740699  0.216613   \n",
       "84   0.118627 -0.711834 -0.892143 -0.332280  1.089190  0.958481  0.469045   \n",
       "87  -0.254197  0.985665  1.120897 -0.332280 -0.918113  1.376823 -0.191214   \n",
       "91  -0.005210  0.356280  1.120897 -0.332280 -0.918113 -0.804090 -0.052784   \n",
       "99  -0.839466 -1.832908  1.120897 -0.332280 -0.918113  1.020924 -0.931545   \n",
       "100 -1.375575 -0.061266  1.120897 -0.332280 -0.918113  1.361450  0.561331   \n",
       "102  0.543801 -1.504557  1.120897 -0.332280 -0.918113  0.102871  0.348936   \n",
       "107  1.529476  1.927233 -0.892143  3.009509 -0.918113 -0.157388  0.635297   \n",
       "108 -0.665267 -0.115340  1.120897 -0.332280 -0.918113  1.101153  0.715369   \n",
       "125 -0.577954 -0.520064  1.120897 -0.332280 -0.918113  0.460191 -0.018855   \n",
       "128 -1.097475 -1.439891  1.120897 -0.332280 -0.918113  0.535934  0.697726   \n",
       "139 -0.433858  0.238654 -0.892143 -0.332280  1.089190  1.138172  0.382865   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "0   -1.016843  0.570789  0.377964 -0.570789 -0.377964    0.017966           1   \n",
       "1    1.624017  0.570789 -2.645751 -0.570789  2.645751    0.436070           1   \n",
       "2   -0.524713  0.570789  0.377964 -0.570789 -0.377964    0.215165           1   \n",
       "3   -0.037228  0.570789  0.377964 -0.570789 -0.377964    0.401310           1   \n",
       "4    0.070940  0.570789  0.377964 -0.570789 -0.377964    0.518599           1   \n",
       "14  -0.760867  0.570789  0.377964 -0.570789 -0.377964    0.316467           1   \n",
       "16   0.105899  0.570789  0.377964 -0.570789 -0.377964    0.372742           1   \n",
       "18  -0.208648  0.570789  0.377964 -0.570789 -0.377964    0.470710           1   \n",
       "25  -0.845175  0.570789  0.377964 -0.570789 -0.377964    0.654165           1   \n",
       "33  -1.415428  0.570789  0.377964 -0.570789 -0.377964    0.373990           1   \n",
       "40  -1.208781 -1.751960  0.377964  1.751960 -0.377964    0.275613           1   \n",
       "42  -0.962822  0.570789  0.377964 -0.570789 -0.377964    0.552538           1   \n",
       "44  -0.833751  0.570789  0.377964 -0.570789 -0.377964    0.161035           1   \n",
       "48  -0.034942  0.570789  0.377964 -0.570789 -0.377964    0.480271           1   \n",
       "52   0.093489 -1.751960  0.377964  1.751960 -0.377964    0.587611           1   \n",
       "53   0.898390  0.570789  0.377964 -0.570789 -0.377964    0.464666           1   \n",
       "65   0.457528  0.570789  0.377964 -0.570789 -0.377964    0.416547           1   \n",
       "66  -0.595118  0.570789  0.377964 -0.570789 -0.377964    0.313304           1   \n",
       "69   0.948774  0.570789  0.377964 -0.570789 -0.377964    0.597320           1   \n",
       "73  -0.150209  0.570789  0.377964 -0.570789 -0.377964    0.226870           1   \n",
       "74   1.312323  0.570789  0.377964 -0.570789 -0.377964    0.305227           1   \n",
       "79   0.910501 -1.751960 -2.645751  1.751960  2.645751    0.482929           1   \n",
       "81   0.151178  0.570789  0.377964 -0.570789 -0.377964    0.367950           1   \n",
       "83  -1.578397  0.570789  0.377964 -0.570789 -0.377964    0.459541           1   \n",
       "84  -0.211750  0.570789  0.377964 -0.570789 -0.377964    0.402025           1   \n",
       "87  -0.034734 -1.751960  0.377964  1.751960 -0.377964    0.278953           1   \n",
       "91  -0.179440 -1.751960  0.377964  1.751960 -0.377964    0.144541           1   \n",
       "99  -0.877639  0.570789  0.377964 -0.570789 -0.377964    0.552177           1   \n",
       "100 -0.744705 -1.751960  0.377964  1.751960 -0.377964    0.317383           1   \n",
       "102 -0.358731  0.570789  0.377964 -0.570789 -0.377964    0.466926           1   \n",
       "107  1.727076  0.570789  0.377964 -0.570789 -0.377964    0.071053           0   \n",
       "108 -0.430724 -1.751960 -2.645751  1.751960  2.645751    0.487033           1   \n",
       "125 -0.881556 -1.751960 -2.645751  1.751960  2.645751    0.485424           1   \n",
       "128 -1.442881  0.570789  0.377964 -0.570789 -0.377964    0.393076           1   \n",
       "139 -0.462409  0.570789  0.377964 -0.570789 -0.377964    0.346781           1   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         0  \n",
       "14            0         0  \n",
       "16            0         0  \n",
       "18            0         0  \n",
       "25            0         0  \n",
       "33            0         0  \n",
       "40            0         0  \n",
       "42            0         0  \n",
       "44            0         0  \n",
       "48            0         0  \n",
       "52            0         0  \n",
       "53            0         0  \n",
       "65            0         0  \n",
       "66            0         0  \n",
       "69            0         0  \n",
       "73            0         0  \n",
       "74            0         0  \n",
       "79            0         0  \n",
       "81            0         0  \n",
       "83            0         0  \n",
       "84            0         0  \n",
       "87            0         0  \n",
       "91            0         0  \n",
       "99            0         0  \n",
       "100           0         0  \n",
       "102           0         0  \n",
       "107           1         0  \n",
       "108           0         0  \n",
       "125           0         0  \n",
       "128           0         0  \n",
       "139           0         0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "\n",
    "high_accuracy_limit = 0.6\n",
    "\n",
    "keras_predict_df_test = pd.DataFrame(model.predict(X_test), columns=[\"Prediction\"])\n",
    "\n",
    "result_df_test = add_output_columns(X_test, y_test, keras_predict_df_test[\"Prediction\"])\n",
    "\n",
    "faults_test = result_df_test[result_df_test[\"Accuracy\"] == 0]\n",
    "\n",
    "high_acc = result_df_test[result_df_test[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults_test.shape[0], \"   Faults from high confidence predictions: \", faults_test[ faults_test[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8571428571428571\n",
      "Recall:  0.15\n",
      "True Positive:  6\n",
      "True Negative:  100\n",
      "False Positive:  1\n",
      "False Negative:  34\n"
     ]
    }
   ],
   "source": [
    "print_conf_matrix(result_df_test[\"Real Value\"], result_df_test[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Error rate: % 5.384615384615385       Number of high confidence predictions:  130\n",
      "Number of faults:  197    Faults from high confidence predictions:  7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.897630</td>\n",
       "      <td>2.112313</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.880965</td>\n",
       "      <td>1.959885</td>\n",
       "      <td>3.242402</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.122103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-2.005724</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.307061</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>-0.925751</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.441499</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.108159</td>\n",
       "      <td>-2.200838</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.740699</td>\n",
       "      <td>0.216613</td>\n",
       "      <td>-1.578397</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.734488</td>\n",
       "      <td>-0.413587</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.492693</td>\n",
       "      <td>-0.032426</td>\n",
       "      <td>-0.980969</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.458125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.355766</td>\n",
       "      <td>-1.871931</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.877772</td>\n",
       "      <td>0.424258</td>\n",
       "      <td>-1.659455</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.443167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>-0.534201</td>\n",
       "      <td>0.104303</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.056210</td>\n",
       "      <td>-1.297979</td>\n",
       "      <td>-1.242834</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.366868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.331730</td>\n",
       "      <td>1.740480</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.424415</td>\n",
       "      <td>-0.217000</td>\n",
       "      <td>-1.076460</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.040395</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.278953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.144541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.551471</td>\n",
       "      <td>1.420492</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.682386</td>\n",
       "      <td>-0.773436</td>\n",
       "      <td>-0.853500</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.380969</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    2.897630  2.112313 -0.892143  3.009509 -0.918113  0.880965  1.959885   \n",
       "1   -0.976951 -2.005724  1.120897 -0.332280 -0.918113 -0.307061  0.333328   \n",
       "2   -1.108159 -2.200838  1.120897 -0.332280 -0.918113  0.740699  0.216613   \n",
       "3   -0.734488 -0.413587  1.120897 -0.332280 -0.918113  0.492693 -0.032426   \n",
       "5   -1.355766 -1.871931  1.120897 -0.332280 -0.918113 -0.877772  0.424258   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "692 -0.534201  0.104303  1.120897 -0.332280 -0.918113 -1.056210 -1.297979   \n",
       "696  0.331730  1.740480 -0.892143  3.009509 -0.918113 -1.424415 -0.217000   \n",
       "699 -0.254197  0.985665  1.120897 -0.332280 -0.918113  1.376823 -0.191214   \n",
       "700 -0.005210  0.356280  1.120897 -0.332280 -0.918113 -0.804090 -0.052784   \n",
       "703  0.551471  1.420492 -0.892143 -0.332280  1.089190 -0.682386 -0.773436   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "0    3.242402  0.570789 -2.645751 -0.570789  2.645751    0.122103           0   \n",
       "1   -0.925751  0.570789  0.377964 -0.570789 -0.377964    0.441499           1   \n",
       "2   -1.578397  0.570789  0.377964 -0.570789 -0.377964    0.459541           1   \n",
       "3   -0.980969  0.570789 -2.645751 -0.570789  2.645751    0.458125           1   \n",
       "5   -1.659455  0.570789  0.377964 -0.570789 -0.377964    0.443167           1   \n",
       "..        ...       ...       ...       ...       ...         ...         ...   \n",
       "692 -1.242834  0.570789  0.377964 -0.570789 -0.377964    0.366868           1   \n",
       "696 -1.076460  0.570789  0.377964 -0.570789 -0.377964    0.040395           1   \n",
       "699 -0.034734 -1.751960  0.377964  1.751960 -0.377964    0.278953           1   \n",
       "700 -0.179440 -1.751960  0.377964  1.751960 -0.377964    0.144541           1   \n",
       "703 -0.853500  0.570789  0.377964 -0.570789 -0.377964    0.380969           1   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "0             1         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "5             0         0  \n",
       "..          ...       ...  \n",
       "692           0         0  \n",
       "696           0         0  \n",
       "699           0         0  \n",
       "700           0         0  \n",
       "703           0         0  \n",
       "\n",
       "[197 rows x 16 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTIRE DATASET\n",
    "# Predicts\n",
    "keras_predict_df = pd.DataFrame(model.predict(normalized_X), columns=[\"Prediction\"])\n",
    "\n",
    "#sort the dataframe by ID\n",
    "result_df = add_output_columns( \n",
    "    df = normalized_X,\n",
    "    y_test = y,\n",
    "    y_pred = keras_predict_df[\"Prediction\"]\n",
    ")\n",
    "faults = result_df[ result_df[\"Accuracy\"] == 0 ]\n",
    "\n",
    "# Accuracy counts of the model where the confidence is greater than high_accuracy_limit\n",
    "high_acc = result_df[result_df[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults.shape[0], \"   Faults from high confidence predictions: \", faults[ faults[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.124814e-16</td>\n",
       "      <td>1.009294e-16</td>\n",
       "      <td>7.065056e-17</td>\n",
       "      <td>2.573699e-16</td>\n",
       "      <td>-5.450186e-16</td>\n",
       "      <td>-3.027881e-17</td>\n",
       "      <td>0.703830</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>0.308343</td>\n",
       "      <td>0.459078</td>\n",
       "      <td>0.445678</td>\n",
       "      <td>0.366021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.375819e+00</td>\n",
       "      <td>-2.617827e+00</td>\n",
       "      <td>-1.976928e+00</td>\n",
       "      <td>-3.428721e+00</td>\n",
       "      <td>-2.103059e+00</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.171947e-01</td>\n",
       "      <td>-6.411746e-01</td>\n",
       "      <td>-8.397758e-01</td>\n",
       "      <td>-5.196470e-01</td>\n",
       "      <td>-7.021904e-01</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.487678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.630704e-02</td>\n",
       "      <td>-1.583170e-02</td>\n",
       "      <td>-1.698798e-02</td>\n",
       "      <td>2.159341e-01</td>\n",
       "      <td>-1.723928e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.822868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.399473e-01</td>\n",
       "      <td>7.538961e-01</td>\n",
       "      <td>8.097366e-01</td>\n",
       "      <td>6.919582e-01</td>\n",
       "      <td>6.037603e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.979437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.316165e+00</td>\n",
       "      <td>2.456273e+00</td>\n",
       "      <td>2.317426e+00</td>\n",
       "      <td>2.223853e+00</td>\n",
       "      <td>3.653464e+00</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02   \n",
       "mean   8.124814e-16  1.009294e-16  7.065056e-17  2.573699e-16 -5.450186e-16   \n",
       "std    1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00   \n",
       "min   -3.375819e+00 -2.617827e+00 -1.976928e+00 -3.428721e+00 -2.103059e+00   \n",
       "25%   -6.171947e-01 -6.411746e-01 -8.397758e-01 -5.196470e-01 -7.021904e-01   \n",
       "50%    8.630704e-02 -1.583170e-02 -1.698798e-02  2.159341e-01 -1.723928e-01   \n",
       "75%    6.399473e-01  7.538961e-01  8.097366e-01  6.919582e-01  6.037603e-01   \n",
       "max    3.316165e+00  2.456273e+00  2.317426e+00  2.223853e+00  3.653464e+00   \n",
       "\n",
       "                  5  Confidence  Real Value  Prediction    Accuracy  \n",
       "count  7.040000e+02  704.000000  704.000000  704.000000  704.000000  \n",
       "mean  -3.027881e-17    0.703830    0.301136    0.272727    0.840909  \n",
       "std    1.000711e+00    0.308343    0.459078    0.445678    0.366021  \n",
       "min   -1.498079e+00    0.001044    0.000000    0.000000    0.000000  \n",
       "25%   -1.498079e+00    0.487678    0.000000    0.000000    1.000000  \n",
       "50%    6.675217e-01    0.822868    0.000000    0.000000    1.000000  \n",
       "75%    6.675217e-01    0.979437    1.000000    1.000000    1.000000  \n",
       "max    6.675217e-01    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    353\n",
       "0     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence = result_df[ result_df[\"Confidence\"] > high_accuracy_limit ]\n",
    "\n",
    "high_confidence[\"Accuracy\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
