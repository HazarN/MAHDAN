{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Cosine similarity (%) : 91.66666666666669\n",
      "0.8477624970048978\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# !!! This section explains why we should not compare two sentences with these methods. !!! \n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilemez.\"\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime        Benzerlik Skoru\n",
      "----------  -----------------\n",
      "Mahkeme              0.860295\n",
      "mahkemenin           0.813442\n",
      "davanın              0.806494\n",
      "tutuklama            0.799902\n",
      "soruşturma           0.791518\n",
      "temyiz               0.771838\n",
      "mahkemede            0.771153\n",
      "dava                 0.770335\n",
      "yargılama            0.769724\n",
      "savcılık             0.730116\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "model = Word2Vec.load(\"utils/word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"mahkeme\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_similarity_comperators import SentenceComparator_Word2Vec,\\\n",
    "                                            SentenceComparator_Ollama,\\\n",
    "                                            SentenceComparator_semantic,\\\n",
    "                                            SentenceComparator_bert_cosine,\\\n",
    "                                            SentenceComparator_SBERT,\\\n",
    "                                            SentenceComparator_NLI,\\\n",
    "                                            SentenceComparator_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "    \"\"\"\n",
    "    This function tests the given model with the given name.\n",
    "\n",
    "    Args:\n",
    "        model(SentenceComparator): A SentenceComparator object.\n",
    "        model_name(str): A string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    test_sentences = [\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi kasada ödeme yapmadan marketten çıkmıştır.\",\n",
    "        \"C kasaya ödeme yapması gerekirken yapmamıştır.\",\n",
    "        \"C markete girdi ve sonra ödeme yapmadan çıktı.\",\n",
    "        \"Şahıs aldığı ürünleri parasını ödemeden çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi kesin unutkan birisidir ve ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Ben C kişisinin kötü birisi olduğunu biliyorum.\",\n",
    "        \"C kişisi iyi bir insan değil.\",\n",
    "        \"Kötü bir insan olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi marketten satın aldığı ürünleri kasada ödeme yapmadan çıkarmıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kameralarıyla doğrulanmıştır.\",\n",
    "        \"C kişisinin kasada ödeme yapmadan çıkması bilinçli bir eylem olarak değerlendirilebilir.\",\n",
    "        \"C kasada ödeme yapmadığı için sorumlu tutulmalıdır.\",\n",
    "        \"C kişisinin ödeme yapmadığına dair hiçbir kanıt bulunmamaktadır.\",\n",
    "        \"Market çalışanları, C'nin ödeme yapmadığını fark etmiştir.\",\n",
    "        \"C kişisi ödeme yapmayı unuttuğunu savunmaktadır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası asılsızdır.\",\n",
    "        \"C'nin kasada ödeme yapmaması kasıtlı bir davranış olarak değerlendirilemez.\",\n",
    "        \"C, dikkat eksikliği nedeniyle ödeme yapmayı unutmuş olabilir.\",\n",
    "        \"C kişisi ödeme yapmadan çıkmayı bir hata olarak tanımlamıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kayıtlarıyla teyit edilmiştir.\",\n",
    "        \"C'nin kasadan ödeme yapmadan ayrılması bilinçli bir davranış olarak nitelendirilebilir.\",\n",
    "        \"C, kasada ödeme yapmadığı için sorumluluk almalıdır.\",\n",
    "        \"C'nin ödeme yapmadığına dair herhangi bir kanıt yoktur.\",\n",
    "        \"Market çalışanları, C’nin kasada ödeme yapmadığını fark etti.\",\n",
    "        \"C kişisi, ödeme yapmayı unuttuğunu iddia ediyor.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası gerçeği yansıtmamaktadır.\",\n",
    "        \"C'nin ödeme yapmaması kasıtlı olarak değerlendirilemez.\",\n",
    "        \"C'nin dikkatsizliği yüzünden ödemeyi unutmuş olabileceği düşünülüyor.\",\n",
    "        \"C kişisi, ödeme yapmadan ayrılmayı bir hata olarak kabul etmiştir.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create log and excel files\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "    create_log(\"model_exec_times.txt\", f\"This file contains the execution times of the models. Number of test_sentences: {len(test_sentences)}\")\n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    elif model_name == \"sentiment_analysis\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"sentiment_1\": [], \"sentiment_2\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    compared_sentence_pairs = []\n",
    "    for index, sentence in enumerate(test_sentences):\n",
    "        for index2, sentence2 in enumerate(test_sentences):\n",
    "            if index != index2 and ( ( sentence, sentence2 ) not in compared_sentence_pairs\\\n",
    "                               and   ( sentence2, sentence ) not in compared_sentence_pairs):\n",
    "\n",
    "                # Calculate the similarity is a essential function of SentenceComparator classes\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                elif model_name == \"sentiment_analysis\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"sentiment_1\": result[0], \"sentiment_2\": result[1]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                \n",
    "                computation_count += 1\n",
    "                compared_sentence_pairs.append((sentence, sentence2))\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "#word2vec_sim = SentenceComparator_Word2Vec(\"utils/word2vec/w2v_.model\")\n",
    "#test_model(word2vec_sim, \"word2vec_sim\")\n",
    "#\n",
    "#sentiment_analysis = SentenceComparator_sentiment_analysis()\n",
    "#test_model(sentiment_analysis, \"sentiment_analysis\")\n",
    "#\n",
    "#sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "#                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "#                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "#                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "#\n",
    "#ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_Result</th>\n",
       "      <th>ollama_model_llama3.1_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi kasada ödeme yapmadan marketten çıkmı...</td>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kasaya ödeme yapması gerekirken yapmamıştır.</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C markete girdi ve sonra ödeme yapmadan çıktı.</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "699  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "700  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "701  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "702  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "703  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim_Result  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...             0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...             0.636648   \n",
       "2    C kişisi kasada ödeme yapmadan marketten çıkmı...             0.625543   \n",
       "3       C kasaya ödeme yapması gerekirken yapmamıştır.             0.657169   \n",
       "4       C markete girdi ve sonra ödeme yapmadan çıktı.             0.587133   \n",
       "..                                                 ...                  ...   \n",
       "699  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.697819   \n",
       "700  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.718892   \n",
       "701  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.703241   \n",
       "702  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.764747   \n",
       "703  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.766007   \n",
       "\n",
       "     sbert_cos_sim_Result  nli_model_CONTRADICTION  nli_model_ENTAILMENT  \\\n",
       "0                  0.9383                        0                     1   \n",
       "1                  0.1996                        1                     0   \n",
       "2                  0.1646                        1                     0   \n",
       "3                  0.4852                        1                     0   \n",
       "4                  0.2732                        1                     0   \n",
       "..                    ...                      ...                   ...   \n",
       "699                0.7362                        1                     0   \n",
       "700                0.6233                        1                     0   \n",
       "701                0.6767                        1                     0   \n",
       "702                0.6354                        1                     0   \n",
       "703                0.8142                        0                     0   \n",
       "\n",
       "     nli_model_NEUTRAL  nli_model_Result  semantic_similarity_Result  \\\n",
       "0                    0          0.769148                      0.9611   \n",
       "1                    0          0.589680                      0.7214   \n",
       "2                    0          0.747958                      0.7042   \n",
       "3                    0          0.710494                      0.6675   \n",
       "4                    0          0.737624                      0.6462   \n",
       "..                 ...               ...                         ...   \n",
       "699                  0          0.844054                      0.6441   \n",
       "700                  0          0.514596                      0.6645   \n",
       "701                  0          0.601376                      0.7230   \n",
       "702                  0          0.867559                      0.6490   \n",
       "703                  1          0.532981                      0.5583   \n",
       "\n",
       "     word2vec_sim_Result  sentiment_analysis_Result  \\\n",
       "0               0.946022                          0   \n",
       "1               0.398693                          1   \n",
       "2               0.312992                          1   \n",
       "3               0.391442                          0   \n",
       "4               0.396546                          1   \n",
       "..                   ...                        ...   \n",
       "699             0.515694                          0   \n",
       "700             0.496692                          0   \n",
       "701             0.457836                          1   \n",
       "702             0.658151                          1   \n",
       "703             0.408180                          1   \n",
       "\n",
       "     ollama_model_llama3.1_Result  \n",
       "0                               0  \n",
       "1                               1  \n",
       "2                               1  \n",
       "3                               1  \n",
       "4                               0  \n",
       "..                            ...  \n",
       "699                             1  \n",
       "700                             1  \n",
       "701                             0  \n",
       "702                             0  \n",
       "703                             1  \n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For sbert_similarity\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "\n",
    "# Dummy encoding\n",
    "dummy = pd.get_dummies(nli_df[\"label\"])\n",
    "nli_df.drop(\"label\", axis=1, inplace=True)\n",
    "nli_df = pd.concat([nli_df, dummy], axis=1)\n",
    "\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "########################################\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_df = excel_to_df(\"sentiment_analysis_log.xlsx\", \"Results\")\n",
    "\n",
    "\"\"\"\n",
    "There are two options for processing sentiment analysis results.\n",
    "\n",
    "1. We can calculate sentiment score as a difference between sentiment_1 and sentiment_2. \n",
    "If they are equal, the score will be 1. Otherwise, the score will be 0. With this approach, we have a binary classification problem.\n",
    "\n",
    "2. We can use sentiment_1 and sentiment_2 as two separate features. With this approach, \n",
    "we have a multi-class classification problem. The first sentence have two labels, and the second sentence have two labels.\n",
    "Threfore, we have 4 labels in total.\n",
    "\n",
    "\n",
    "# Dummy encoding Option 2\n",
    "dummy_1 = pd.get_dummies(sentiment_df[\"sentiment_1\"])\n",
    "dummy_2 = pd.get_dummies(sentiment_df[\"sentiment_2\"])\n",
    "\n",
    "rename_all_columns = lambda df, suffix: df.rename(columns={col: col + suffix for col in df.columns})\n",
    "\n",
    "dummy_1 = rename_all_columns(dummy_1, \"_setnence1\")\n",
    "dummy_2 = rename_all_columns(dummy_2, \"_setnence2\")\n",
    "\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "sentiment_df = pd.concat([sentiment_df, dummy_1, dummy_2], axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# Dummy encoding Option 1\n",
    "sentiment_df[\"Result\"] = (sentiment_df[\"sentiment_1\"] == sentiment_df[\"sentiment_2\"]) * 1\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:Cüaıbzkfakı .23456789]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "#all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], sentiment_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "#                        axis=1, \n",
    "#                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "\n",
    "def concat_columns_except_sentences(df_list, df_list_names):\n",
    "    initial_df = df_list[0].iloc[:,:2]\n",
    "    for index, df in enumerate(df_list):\n",
    "        df_except_sentences = df[ df.columns.difference([\"Sentence1\", \"Sentence2\"]) ] * 1\n",
    "        df_except_sentences.columns = [f\"{df_list_names[index]}_{col}\" for col in df_except_sentences.columns]\n",
    "        initial_df = pd.concat([initial_df, df_except_sentences], axis=1)\n",
    "    return initial_df\n",
    "\n",
    "all_results = concat_columns_except_sentences(\n",
    "    [bert_cos_sim, sbert_cos_df, nli_df, semantic_df, word2vec_df, sentiment_df, ollama_df], \n",
    "    [\"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "             \n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "#all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Normalize lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1_Result\"]\n",
    "X = all_results_parameters.drop(\"ollama_model_llama3.1_Result\", axis=1)\n",
    "\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg, Accuracy:  0.7304964539007093\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Reg, Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_cos_sim_Result  sbert_cos_sim_Result  nli_model_CONTRADICTION  \\\n",
       "0               0.964577                0.9383                        0   \n",
       "1               0.636648                0.1996                        1   \n",
       "2               0.625543                0.1646                        1   \n",
       "3               0.657169                0.4852                        1   \n",
       "4               0.587133                0.2732                        1   \n",
       "..                   ...                   ...                      ...   \n",
       "699             0.697819                0.7362                        1   \n",
       "700             0.718892                0.6233                        1   \n",
       "701             0.703241                0.6767                        1   \n",
       "702             0.764747                0.6354                        1   \n",
       "703             0.766007                0.8142                        0   \n",
       "\n",
       "     nli_model_ENTAILMENT  nli_model_NEUTRAL  nli_model_Result  \\\n",
       "0                       1                  0          0.769148   \n",
       "1                       0                  0          0.589680   \n",
       "2                       0                  0          0.747958   \n",
       "3                       0                  0          0.710494   \n",
       "4                       0                  0          0.737624   \n",
       "..                    ...                ...               ...   \n",
       "699                     0                  0          0.844054   \n",
       "700                     0                  0          0.514596   \n",
       "701                     0                  0          0.601376   \n",
       "702                     0                  0          0.867559   \n",
       "703                     0                  1          0.532981   \n",
       "\n",
       "     semantic_similarity_Result  word2vec_sim_Result  \\\n",
       "0                        0.9611             0.946022   \n",
       "1                        0.7214             0.398693   \n",
       "2                        0.7042             0.312992   \n",
       "3                        0.6675             0.391442   \n",
       "4                        0.6462             0.396546   \n",
       "..                          ...                  ...   \n",
       "699                      0.6441             0.515694   \n",
       "700                      0.6645             0.496692   \n",
       "701                      0.7230             0.457836   \n",
       "702                      0.6490             0.658151   \n",
       "703                      0.5583             0.408180   \n",
       "\n",
       "     sentiment_analysis_Result  \n",
       "0                            0  \n",
       "1                            1  \n",
       "2                            1  \n",
       "3                            0  \n",
       "4                            1  \n",
       "..                         ...  \n",
       "699                          0  \n",
       "700                          0  \n",
       "701                          1  \n",
       "702                          1  \n",
       "703                          1  \n",
       "\n",
       "[704 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44778095,  0.38182362,  0.05213473,  0.06798317, -0.09282145,\n",
       "        0.15166804,  0.3627731 , -0.5621213 , -0.13803115])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.5833333333333334\n",
      "Recall:  0.175\n",
      "True Positive:  7\n",
      "True Negative:  96\n",
      "False Positive:  5\n",
      "False Negative:  33\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_conf_matrix(y_test, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    print(\"True Positive: \", conf_matrix[1][1])\n",
    "    print(\"True Negative: \", conf_matrix[0][0])\n",
    "    print(\"False Positive: \", conf_matrix[0][1])\n",
    "    print(\"False Negative: \", conf_matrix[1][0])\n",
    "    \n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=100, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 24s]\n",
      "val_accuracy: 0.73758864402771\n",
      "\n",
      "Best val_accuracy So Far: 0.73758864402771\n",
      "Total elapsed time: 00h 03m 20s\n"
     ]
    }
   ],
   "source": [
    "def keras_model_tuner(hp):\n",
    "    hidden_layer_num = hp.Int('hidden_layer_num', min_value=1, max_value=5, step=1)\n",
    "    layer_unit = hp.Int('layer_unit', min_value=16, max_value=128, step=16)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer_unit, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(hidden_layer_num):\n",
    "        model.add(layers.Dense(layer_unit, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=5,\n",
    "    directory='log',\n",
    "    project_name='ollama_model4'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001 )\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model4\\tuner0.json\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Accuracy:  0.7588652482269503\n",
      "Precision:  1.0\n",
      "Recall:  0.15\n",
      "True Positive:  6\n",
      "True Negative:  101\n",
      "False Positive:  0\n",
      "False Negative:  34\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='ollama_model4'\n",
    ")\n",
    "\n",
    "tuner.reload()\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_columns(df, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    This function adds the Confidence, Real Value, and Prediction columns to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    if type(df) != pd.DataFrame:\n",
    "        df = pd.DataFrame(df)\n",
    "    df[\"Confidence\"] = 1.0\n",
    "    df[\"Real Value\"] = 5\n",
    "    df[\"Prediction\"] = 5\n",
    "    df[\"Accuracy\"] = 5\n",
    "    #print(df.head())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, val in enumerate(y_pred):\n",
    "        \n",
    "        real =  y_test.iloc[index]\n",
    "        pred = 0 if val < 0.5 else 1\n",
    "        confidence = (val-0.5)*2 if pred == 1 else (0.5-val)*2\n",
    "        \n",
    "        df[\"Confidence\"][index] = confidence\n",
    "        df[\"Real Value\"][index] = real\n",
    "        df[\"Prediction\"][index] = pred\n",
    "        df[\"Accuracy\"][index] = 1 if real == pred else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_accuracy(x,y,predictor):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predictor.\n",
    "\n",
    "    Args:\n",
    "        x: A pandas dataframe.\n",
    "        y: A pandas dataframe.\n",
    "        predictor: A predictor model.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: A float.\n",
    "    \"\"\"\n",
    "    y_pred = predictor.predict(x)\n",
    "    y_pred = [1 if val > 0.5 else 0 for val in y_pred]\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test Accuracy:  0.7588652482269503\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Train Accuracy:  0.7477797513321492\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step\n",
      "Entire Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Test Accuracy: \", calculate_accuracy(X_test, y_test, model))\n",
    "# Train\n",
    "print(\"Train Accuracy: \", calculate_accuracy(X_train, y_train, model))\n",
    "# Entire\n",
    "print(\"Entire Accuracy: \", calculate_accuracy(normalized_X, y, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Error rate: % 6.451612903225806       Number of high confidence predictions:  31\n",
      "Number of faults:  34    Faults from high confidence predictions:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271673</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.224133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>-0.582501</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.225806</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>-0.524713</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.382134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678161</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.522652</td>\n",
       "      <td>-0.037228</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.400346</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688566</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.574057</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>0.070940</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.803466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.880661</td>\n",
       "      <td>-0.038967</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.522799</td>\n",
       "      <td>0.362507</td>\n",
       "      <td>-0.760867</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.277258</td>\n",
       "      <td>-0.140984</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.540841</td>\n",
       "      <td>0.336721</td>\n",
       "      <td>0.105899</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.473359</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.039912</td>\n",
       "      <td>0.067510</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.510728</td>\n",
       "      <td>0.276328</td>\n",
       "      <td>-0.208648</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.488283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.168701</td>\n",
       "      <td>-0.763679</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-1.038817</td>\n",
       "      <td>-0.791758</td>\n",
       "      <td>-0.845175</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.593224</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.712509</td>\n",
       "      <td>-0.548495</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.409182</td>\n",
       "      <td>-0.968867</td>\n",
       "      <td>-1.415428</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.507932</td>\n",
       "      <td>0.823441</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.556588</td>\n",
       "      <td>-0.909831</td>\n",
       "      <td>-1.208781</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.636793</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.039668</td>\n",
       "      <td>-0.634346</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.744138</td>\n",
       "      <td>-0.896259</td>\n",
       "      <td>-0.962822</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.541981</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.446974</td>\n",
       "      <td>0.667906</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.854990</td>\n",
       "      <td>0.526045</td>\n",
       "      <td>-0.833751</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.070292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.326526</td>\n",
       "      <td>-0.013323</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.068289</td>\n",
       "      <td>-0.559683</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.625736</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.549689</td>\n",
       "      <td>-0.493305</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.384092</td>\n",
       "      <td>-0.126749</td>\n",
       "      <td>0.093489</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.586562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.796507</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.122937</td>\n",
       "      <td>0.109397</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.652803</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.282194</td>\n",
       "      <td>-0.462645</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.150760</td>\n",
       "      <td>0.321114</td>\n",
       "      <td>0.457528</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.728443</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.400726</td>\n",
       "      <td>0.187366</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.979483</td>\n",
       "      <td>1.049231</td>\n",
       "      <td>-0.595118</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.611965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.330753</td>\n",
       "      <td>-0.717409</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>0.948774</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.887161</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.809898</td>\n",
       "      <td>0.583171</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.229946</td>\n",
       "      <td>0.609511</td>\n",
       "      <td>0.115563</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.031429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.484438</td>\n",
       "      <td>0.718762</td>\n",
       "      <td>-0.150209</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.299181</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.064504</td>\n",
       "      <td>1.528641</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.787594</td>\n",
       "      <td>1.670810</td>\n",
       "      <td>1.312323</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.465996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.747359</td>\n",
       "      <td>1.209210</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>0.896550</td>\n",
       "      <td>0.910501</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.357321</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.932159</td>\n",
       "      <td>0.913193</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.786369</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.412673</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.774139</td>\n",
       "      <td>1.538118</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.492721</td>\n",
       "      <td>0.891121</td>\n",
       "      <td>1.478509</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.393919</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.118627</td>\n",
       "      <td>-0.711834</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.958481</td>\n",
       "      <td>0.469045</td>\n",
       "      <td>-0.211750</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.577520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.646443</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.415512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.839466</td>\n",
       "      <td>-1.832908</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.020924</td>\n",
       "      <td>-0.931545</td>\n",
       "      <td>-0.877639</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.425624</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.375575</td>\n",
       "      <td>-0.061266</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.361450</td>\n",
       "      <td>0.561331</td>\n",
       "      <td>-0.744705</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.486802</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.543801</td>\n",
       "      <td>-1.504557</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.348936</td>\n",
       "      <td>-0.358731</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.495843</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.665267</td>\n",
       "      <td>-0.115340</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.101153</td>\n",
       "      <td>0.715369</td>\n",
       "      <td>-0.430724</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.402789</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.479916</td>\n",
       "      <td>2.188129</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.282022</td>\n",
       "      <td>1.009873</td>\n",
       "      <td>0.301925</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.107350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.577954</td>\n",
       "      <td>-0.520064</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.460191</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>-0.881556</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.111281</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.433858</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.138172</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>-0.462409</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.196821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "1    1.271673  0.764906  1.120897 -0.332280 -0.918113 -0.325933  0.588475   \n",
       "2    0.787798 -0.582501 -0.892143  3.009509 -0.918113 -1.225806  0.475152   \n",
       "3    0.678161 -0.317145  1.120897 -0.332280 -0.918113  0.638371  0.522652   \n",
       "4    0.688566 -0.634903  1.120897 -0.332280 -0.918113 -0.574057 -1.054368   \n",
       "14  -0.880661 -0.038967  1.120897 -0.332280 -0.918113 -0.522799  0.362507   \n",
       "16   0.277258 -0.140984 -0.892143 -0.332280  1.089190  1.540841  0.336721   \n",
       "18  -1.039912  0.067510 -0.892143 -0.332280  1.089190  0.510728  0.276328   \n",
       "25  -1.168701 -0.763679 -0.892143 -0.332280  1.089190 -1.038817 -0.791758   \n",
       "33  -0.712509 -0.548495  1.120897 -0.332280 -0.918113 -0.409182 -0.968867   \n",
       "40  -0.507932  0.823441  1.120897 -0.332280 -0.918113  0.556588 -0.909831   \n",
       "42  -0.039668 -0.634346 -0.892143 -0.332280  1.089190 -0.744138 -0.896259   \n",
       "44   0.446974  0.667906 -0.892143  3.009509 -0.918113 -1.854990  0.526045   \n",
       "48  -0.326526 -0.013323  1.120897 -0.332280 -0.918113  0.068289 -0.559683   \n",
       "52  -0.549689 -0.493305 -0.892143 -0.332280  1.089190  0.384092 -0.126749   \n",
       "53   0.796507  0.013436 -0.892143 -0.332280  1.089190  1.122937  0.109397   \n",
       "65   0.282194 -0.462645  1.120897 -0.332280 -0.918113 -1.150760  0.321114   \n",
       "66   0.400726  0.187366 -0.892143 -0.332280  1.089190 -0.979483  1.049231   \n",
       "69  -0.330753 -0.717409  1.120897 -0.332280 -0.918113  0.407301  0.222720   \n",
       "70   0.809898  0.583171  1.120897 -0.332280 -0.918113 -0.229946  0.609511   \n",
       "73   1.391350  1.021343 -0.892143 -0.332280  1.089190 -0.484438  0.718762   \n",
       "74   0.064504  1.528641 -0.892143 -0.332280  1.089190  1.787594  1.670810   \n",
       "79   0.747359  1.209210 -0.892143 -0.332280  1.089190  0.376738  0.896550   \n",
       "81   0.932159  0.913193 -0.892143 -0.332280  1.089190 -0.786369  0.077504   \n",
       "82   0.774139  1.538118 -0.892143  3.009509 -0.918113 -0.492721  0.891121   \n",
       "84   0.118627 -0.711834 -0.892143 -0.332280  1.089190  0.958481  0.469045   \n",
       "87  -0.254197  0.985665  1.120897 -0.332280 -0.918113  1.376823 -0.191214   \n",
       "91  -0.005210  0.356280  1.120897 -0.332280 -0.918113 -0.804090 -0.052784   \n",
       "99  -0.839466 -1.832908  1.120897 -0.332280 -0.918113  1.020924 -0.931545   \n",
       "100 -1.375575 -0.061266  1.120897 -0.332280 -0.918113  1.361450  0.561331   \n",
       "102  0.543801 -1.504557  1.120897 -0.332280 -0.918113  0.102871  0.348936   \n",
       "108 -0.665267 -0.115340  1.120897 -0.332280 -0.918113  1.101153  0.715369   \n",
       "123  0.479916  2.188129 -0.892143  3.009509 -0.918113 -1.282022  1.009873   \n",
       "125 -0.577954 -0.520064  1.120897 -0.332280 -0.918113  0.460191 -0.018855   \n",
       "139 -0.433858  0.238654 -0.892143 -0.332280  1.089190  1.138172  0.382865   \n",
       "\n",
       "            7         8  Confidence  Real Value  Prediction  Accuracy  \n",
       "1    1.624017 -1.498079    0.224133           1           0         0  \n",
       "2   -0.524713  0.667522    0.382134           1           0         0  \n",
       "3   -0.037228  0.667522    0.400346           1           0         0  \n",
       "4    0.070940  0.667522    0.803466           1           0         0  \n",
       "14  -0.760867  0.667522    0.110423           1           0         0  \n",
       "16   0.105899  0.667522    0.473359           1           0         0  \n",
       "18  -0.208648  0.667522    0.488283           1           0         0  \n",
       "25  -0.845175  0.667522    0.593224           1           0         0  \n",
       "33  -1.415428  0.667522    0.000866           1           0         0  \n",
       "40  -1.208781 -1.498079    0.636793           1           0         0  \n",
       "42  -0.962822  0.667522    0.541981           1           0         0  \n",
       "44  -0.833751  0.667522    0.070292           1           0         0  \n",
       "48  -0.034942  0.667522    0.625736           1           0         0  \n",
       "52   0.093489 -1.498079    0.586562           1           0         0  \n",
       "53   0.898390  0.667522    0.652803           1           0         0  \n",
       "65   0.457528  0.667522    0.728443           1           0         0  \n",
       "66  -0.595118  0.667522    0.611965           1           0         0  \n",
       "69   0.948774  0.667522    0.887161           1           0         0  \n",
       "70   0.115563 -1.498079    0.031429           1           0         0  \n",
       "73  -0.150209  0.667522    0.299181           1           0         0  \n",
       "74   1.312323  0.667522    0.465996           1           0         0  \n",
       "79   0.910501  0.667522    0.357321           1           0         0  \n",
       "81   0.151178  0.667522    0.412673           1           0         0  \n",
       "82   1.478509  0.667522    0.393919           1           0         0  \n",
       "84  -0.211750  0.667522    0.577520           1           0         0  \n",
       "87  -0.034734 -1.498079    0.646443           1           0         0  \n",
       "91  -0.179440 -1.498079    0.415512           1           0         0  \n",
       "99  -0.877639  0.667522    0.425624           1           0         0  \n",
       "100 -0.744705 -1.498079    0.486802           1           0         0  \n",
       "102 -0.358731  0.667522    0.495843           1           0         0  \n",
       "108 -0.430724  0.667522    0.402789           1           0         0  \n",
       "123  0.301925  0.667522    0.107350           1           0         0  \n",
       "125 -0.881556  0.667522    0.111281           1           0         0  \n",
       "139 -0.462409  0.667522    0.196821           1           0         0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TEST DATASET\n",
    "\n",
    "high_accuracy_limit = 0.8\n",
    "\n",
    "keras_predict_df_test = pd.DataFrame(model.predict(X_test), columns=[\"Prediction\"])\n",
    "\n",
    "result_df_test = add_output_columns(X_test, y_test, keras_predict_df_test[\"Prediction\"])\n",
    "\n",
    "faults_test = result_df_test[result_df_test[\"Accuracy\"] == 0]\n",
    "\n",
    "high_acc = result_df_test[result_df_test[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "\n",
    "high_acc = high_acc = [high_acc[1], 0] if len(high_acc) == 1 else high_acc\n",
    "\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults_test.shape[0], \"   Faults from high confidence predictions: \", faults_test[ faults_test[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.052463</td>\n",
       "      <td>0.088038</td>\n",
       "      <td>0.114377</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>-0.150615</td>\n",
       "      <td>0.059059</td>\n",
       "      <td>0.195297</td>\n",
       "      <td>-0.096775</td>\n",
       "      <td>0.221663</td>\n",
       "      <td>0.430720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.728780</td>\n",
       "      <td>0.876483</td>\n",
       "      <td>1.021656</td>\n",
       "      <td>1.092882</td>\n",
       "      <td>0.990143</td>\n",
       "      <td>0.949740</td>\n",
       "      <td>0.664596</td>\n",
       "      <td>0.757020</td>\n",
       "      <td>0.888818</td>\n",
       "      <td>0.223803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.375575</td>\n",
       "      <td>-1.832908</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.854990</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>-1.415428</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.539249</td>\n",
       "      <td>-0.541387</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.701618</td>\n",
       "      <td>-0.108258</td>\n",
       "      <td>-0.707308</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.313716</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.091565</td>\n",
       "      <td>-0.026145</td>\n",
       "      <td>0.114377</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.085580</td>\n",
       "      <td>0.342829</td>\n",
       "      <td>-0.164825</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.445810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.685965</td>\n",
       "      <td>0.740656</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.581689</td>\n",
       "      <td>0.142274</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.591558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>2.188129</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.787594</td>\n",
       "      <td>1.670810</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.887161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "count  34.000000  34.000000  34.000000  34.000000  34.000000  34.000000   \n",
       "mean    0.052463   0.088038   0.114377   0.060871  -0.150615   0.059059   \n",
       "std     0.728780   0.876483   1.021656   1.092882   0.990143   0.949740   \n",
       "min    -1.375575  -1.832908  -0.892143  -0.332280  -0.918113  -1.854990   \n",
       "25%    -0.539249  -0.541387  -0.892143  -0.332280  -0.918113  -0.701618   \n",
       "50%     0.091565  -0.026145   0.114377  -0.332280  -0.918113   0.085580   \n",
       "75%     0.685965   0.740656   1.120897  -0.332280   1.089190   0.878453   \n",
       "max     1.391350   2.188129   1.120897   3.009509   1.089190   1.787594   \n",
       "\n",
       "               6          7          8  Confidence  Real Value  Prediction  \\\n",
       "count  34.000000  34.000000  34.000000   34.000000        34.0        34.0   \n",
       "mean    0.195297  -0.096775   0.221663    0.430720         1.0         0.0   \n",
       "std     0.664596   0.757020   0.888818    0.223803         0.0         0.0   \n",
       "min    -1.054368  -1.415428  -1.498079    0.000866         1.0         0.0   \n",
       "25%    -0.108258  -0.707308   0.667522    0.313716         1.0         0.0   \n",
       "50%     0.342829  -0.164825   0.667522    0.445810         1.0         0.0   \n",
       "75%     0.581689   0.142274   0.667522    0.591558         1.0         0.0   \n",
       "max     1.670810   1.624017   0.667522    0.887161         1.0         0.0   \n",
       "\n",
       "       Accuracy  \n",
       "count      34.0  \n",
       "mean        0.0  \n",
       "std         0.0  \n",
       "min         0.0  \n",
       "25%         0.0  \n",
       "50%         0.0  \n",
       "75%         0.0  \n",
       "max         0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faults_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.138364</td>\n",
       "      <td>-0.166639</td>\n",
       "      <td>0.150069</td>\n",
       "      <td>-0.024172</td>\n",
       "      <td>-0.135123</td>\n",
       "      <td>-0.052351</td>\n",
       "      <td>-0.163474</td>\n",
       "      <td>-0.058384</td>\n",
       "      <td>-0.100422</td>\n",
       "      <td>0.509153</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.758865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.031686</td>\n",
       "      <td>0.958244</td>\n",
       "      <td>1.009473</td>\n",
       "      <td>0.970247</td>\n",
       "      <td>0.982585</td>\n",
       "      <td>0.969650</td>\n",
       "      <td>1.085283</td>\n",
       "      <td>1.032692</td>\n",
       "      <td>1.039706</td>\n",
       "      <td>0.304770</td>\n",
       "      <td>0.452394</td>\n",
       "      <td>0.202567</td>\n",
       "      <td>0.429297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.375819</td>\n",
       "      <td>-2.263275</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.854990</td>\n",
       "      <td>-3.428721</td>\n",
       "      <td>-1.993943</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.780673</td>\n",
       "      <td>-0.763679</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.831397</td>\n",
       "      <td>-0.888116</td>\n",
       "      <td>-0.800966</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.232970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>-0.242444</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.229946</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>-0.200721</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.510452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.546704</td>\n",
       "      <td>0.583171</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.792241</td>\n",
       "      <td>0.635297</td>\n",
       "      <td>0.739200</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.774668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.556734</td>\n",
       "      <td>2.222692</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>2.096723</td>\n",
       "      <td>1.710846</td>\n",
       "      <td>3.653464</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.993084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  141.000000  141.000000  141.000000  141.000000  141.000000  141.000000   \n",
       "mean    -0.138364   -0.166639    0.150069   -0.024172   -0.135123   -0.052351   \n",
       "std      1.031686    0.958244    1.009473    0.970247    0.982585    0.969650   \n",
       "min     -3.375819   -2.263275   -0.892143   -0.332280   -0.918113   -1.854990   \n",
       "25%     -0.780673   -0.763679   -0.892143   -0.332280   -0.918113   -0.831397   \n",
       "50%     -0.005210   -0.242444    1.120897   -0.332280   -0.918113   -0.229946   \n",
       "75%      0.546704    0.583171    1.120897   -0.332280    1.089190    0.792241   \n",
       "max      2.556734    2.222692    1.120897    3.009509    1.089190    2.096723   \n",
       "\n",
       "                6           7           8  Confidence  Real Value  Prediction  \\\n",
       "count  141.000000  141.000000  141.000000  141.000000  141.000000  141.000000   \n",
       "mean    -0.163474   -0.058384   -0.100422    0.509153    0.283688    0.042553   \n",
       "std      1.085283    1.032692    1.039706    0.304770    0.452394    0.202567   \n",
       "min     -3.428721   -1.993943   -1.498079    0.000287    0.000000    0.000000   \n",
       "25%     -0.888116   -0.800966   -1.498079    0.232970    0.000000    0.000000   \n",
       "50%      0.077504   -0.200721    0.667522    0.510452    0.000000    0.000000   \n",
       "75%      0.635297    0.739200    0.667522    0.774668    1.000000    0.000000   \n",
       "max      1.710846    3.653464    0.667522    0.993084    1.000000    1.000000   \n",
       "\n",
       "         Accuracy  \n",
       "count  141.000000  \n",
       "mean     0.758865  \n",
       "std      0.429297  \n",
       "min      0.000000  \n",
       "25%      1.000000  \n",
       "50%      1.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:  0.15\n",
      "True Positive:  6\n",
      "True Negative:  101\n",
      "False Positive:  0\n",
      "False Negative:  34\n"
     ]
    }
   ],
   "source": [
    "print_conf_matrix(result_df_test[\"Real Value\"], result_df_test[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step\n",
      "Error rate: % 8.870967741935484       Number of high confidence predictions:  248\n",
      "Number of faults:  176    Faults from high confidence predictions:  22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.897630</td>\n",
       "      <td>2.112313</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.880965</td>\n",
       "      <td>1.959885</td>\n",
       "      <td>3.242402</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.554322</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-2.005724</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.307061</td>\n",
       "      <td>0.333328</td>\n",
       "      <td>-0.925751</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.067422</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.734488</td>\n",
       "      <td>-0.413587</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.492693</td>\n",
       "      <td>-0.032426</td>\n",
       "      <td>-0.980969</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.349132</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.300269</td>\n",
       "      <td>0.161165</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.760111</td>\n",
       "      <td>0.215256</td>\n",
       "      <td>-0.400379</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.137272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.703323</td>\n",
       "      <td>-1.193489</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.086699</td>\n",
       "      <td>0.391686</td>\n",
       "      <td>-1.330879</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>-0.534201</td>\n",
       "      <td>0.104303</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.056210</td>\n",
       "      <td>-1.297979</td>\n",
       "      <td>-1.242834</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.097838</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>-0.378626</td>\n",
       "      <td>1.761664</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.451226</td>\n",
       "      <td>1.197161</td>\n",
       "      <td>0.316859</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.193870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.646443</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.415512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.551471</td>\n",
       "      <td>1.420492</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.682386</td>\n",
       "      <td>-0.773436</td>\n",
       "      <td>-0.853500</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.110026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    2.897630  2.112313 -0.892143  3.009509 -0.918113  0.880965  1.959885   \n",
       "1   -0.976951 -2.005724  1.120897 -0.332280 -0.918113 -0.307061  0.333328   \n",
       "3   -0.734488 -0.413587  1.120897 -0.332280 -0.918113  0.492693 -0.032426   \n",
       "7   -0.300269  0.161165  1.120897 -0.332280 -0.918113 -0.760111  0.215256   \n",
       "9   -0.703323 -1.193489  1.120897 -0.332280 -0.918113 -0.086699  0.391686   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "692 -0.534201  0.104303  1.120897 -0.332280 -0.918113 -1.056210 -1.297979   \n",
       "698 -0.378626  1.761664 -0.892143  3.009509 -0.918113 -1.451226  1.197161   \n",
       "699 -0.254197  0.985665  1.120897 -0.332280 -0.918113  1.376823 -0.191214   \n",
       "700 -0.005210  0.356280  1.120897 -0.332280 -0.918113 -0.804090 -0.052784   \n",
       "703  0.551471  1.420492 -0.892143 -0.332280  1.089190 -0.682386 -0.773436   \n",
       "\n",
       "            7         8  Confidence  Real Value  Prediction  Accuracy  \n",
       "0    3.242402 -1.498079    0.554322           0           1         0  \n",
       "1   -0.925751  0.667522    0.067422           1           0         0  \n",
       "3   -0.980969 -1.498079    0.349132           1           0         0  \n",
       "7   -0.400379  0.667522    0.137272           1           0         0  \n",
       "9   -1.330879 -1.498079    0.013184           1           0         0  \n",
       "..        ...       ...         ...         ...         ...       ...  \n",
       "692 -1.242834  0.667522    0.097838           1           0         0  \n",
       "698  0.316859 -1.498079    0.193870           1           0         0  \n",
       "699 -0.034734 -1.498079    0.646443           1           0         0  \n",
       "700 -0.179440 -1.498079    0.415512           1           0         0  \n",
       "703 -0.853500  0.667522    0.110026           1           0         0  \n",
       "\n",
       "[176 rows x 13 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTIRE DATASET\n",
    "# Predicts\n",
    "keras_predict_df = pd.DataFrame(model.predict(normalized_X), columns=[\"Prediction\"])\n",
    "\n",
    "#sort the dataframe by ID\n",
    "result_df = add_output_columns( \n",
    "    df = normalized_X,\n",
    "    y_test = y,\n",
    "    y_pred = keras_predict_df[\"Prediction\"]\n",
    ")\n",
    "faults = result_df[ result_df[\"Accuracy\"] == 0 ]\n",
    "\n",
    "# Accuracy counts of the model where the confidence is greater than high_accuracy_limit\n",
    "high_acc = result_df[result_df[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults.shape[0], \"   Faults from high confidence predictions: \", faults[ faults[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.124814e-16</td>\n",
       "      <td>1.009294e-16</td>\n",
       "      <td>1.312082e-16</td>\n",
       "      <td>1.261617e-17</td>\n",
       "      <td>-8.074349e-17</td>\n",
       "      <td>7.065056e-17</td>\n",
       "      <td>2.573699e-16</td>\n",
       "      <td>-5.450186e-16</td>\n",
       "      <td>-3.027881e-17</td>\n",
       "      <td>0.476535</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>0.292023</td>\n",
       "      <td>0.459078</td>\n",
       "      <td>0.275148</td>\n",
       "      <td>0.433321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.375819e+00</td>\n",
       "      <td>-2.617827e+00</td>\n",
       "      <td>-8.921426e-01</td>\n",
       "      <td>-3.322801e-01</td>\n",
       "      <td>-9.181132e-01</td>\n",
       "      <td>-1.976928e+00</td>\n",
       "      <td>-3.428721e+00</td>\n",
       "      <td>-2.103059e+00</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.171947e-01</td>\n",
       "      <td>-6.411746e-01</td>\n",
       "      <td>-8.921426e-01</td>\n",
       "      <td>-3.322801e-01</td>\n",
       "      <td>-9.181132e-01</td>\n",
       "      <td>-8.397758e-01</td>\n",
       "      <td>-5.196470e-01</td>\n",
       "      <td>-7.021904e-01</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.222446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.630704e-02</td>\n",
       "      <td>-1.583170e-02</td>\n",
       "      <td>-8.921426e-01</td>\n",
       "      <td>-3.322801e-01</td>\n",
       "      <td>-9.181132e-01</td>\n",
       "      <td>-1.698798e-02</td>\n",
       "      <td>2.159341e-01</td>\n",
       "      <td>-1.723928e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.399473e-01</td>\n",
       "      <td>7.538961e-01</td>\n",
       "      <td>1.120897e+00</td>\n",
       "      <td>-3.322801e-01</td>\n",
       "      <td>1.089190e+00</td>\n",
       "      <td>8.097366e-01</td>\n",
       "      <td>6.919582e-01</td>\n",
       "      <td>6.037603e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.725297</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.316165e+00</td>\n",
       "      <td>2.456273e+00</td>\n",
       "      <td>1.120897e+00</td>\n",
       "      <td>3.009509e+00</td>\n",
       "      <td>1.089190e+00</td>\n",
       "      <td>2.317426e+00</td>\n",
       "      <td>2.223853e+00</td>\n",
       "      <td>3.653464e+00</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.996335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02   \n",
       "mean   8.124814e-16  1.009294e-16  1.312082e-16  1.261617e-17 -8.074349e-17   \n",
       "std    1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00   \n",
       "min   -3.375819e+00 -2.617827e+00 -8.921426e-01 -3.322801e-01 -9.181132e-01   \n",
       "25%   -6.171947e-01 -6.411746e-01 -8.921426e-01 -3.322801e-01 -9.181132e-01   \n",
       "50%    8.630704e-02 -1.583170e-02 -8.921426e-01 -3.322801e-01 -9.181132e-01   \n",
       "75%    6.399473e-01  7.538961e-01  1.120897e+00 -3.322801e-01  1.089190e+00   \n",
       "max    3.316165e+00  2.456273e+00  1.120897e+00  3.009509e+00  1.089190e+00   \n",
       "\n",
       "                  5             6             7             8  Confidence  \\\n",
       "count  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02  704.000000   \n",
       "mean   7.065056e-17  2.573699e-16 -5.450186e-16 -3.027881e-17    0.476535   \n",
       "std    1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00    0.292023   \n",
       "min   -1.976928e+00 -3.428721e+00 -2.103059e+00 -1.498079e+00    0.000287   \n",
       "25%   -8.397758e-01 -5.196470e-01 -7.021904e-01 -1.498079e+00    0.222446   \n",
       "50%   -1.698798e-02  2.159341e-01 -1.723928e-01  6.675217e-01    0.461482   \n",
       "75%    8.097366e-01  6.919582e-01  6.037603e-01  6.675217e-01    0.725297   \n",
       "max    2.317426e+00  2.223853e+00  3.653464e+00  6.675217e-01    0.996335   \n",
       "\n",
       "       Real Value  Prediction    Accuracy  \n",
       "count  704.000000  704.000000  704.000000  \n",
       "mean     0.301136    0.082386    0.750000  \n",
       "std      0.459078    0.275148    0.433321  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.750000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      1.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    226\n",
       "0     22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence = result_df[ result_df[\"Confidence\"] > high_accuracy_limit ]\n",
    "\n",
    "high_confidence[\"Accuracy\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
