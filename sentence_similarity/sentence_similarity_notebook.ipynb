{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Cosine similarity (%) : 91.66666666666669\n",
      "0.8477624970048978\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# !!! This section explains why we should not compare two sentences with these methods. !!! \n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilemez.\"\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime        Benzerlik Skoru\n",
      "----------  -----------------\n",
      "Mahkeme              0.860295\n",
      "mahkemenin           0.813442\n",
      "davanın              0.806494\n",
      "tutuklama            0.799902\n",
      "soruşturma           0.791518\n",
      "temyiz               0.771838\n",
      "mahkemede            0.771153\n",
      "dava                 0.770335\n",
      "yargılama            0.769724\n",
      "savcılık             0.730116\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "model = Word2Vec.load(\"utils/word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"mahkeme\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_similarity_comperators import SentenceComparator_Word2Vec,\\\n",
    "                                            SentenceComparator_Ollama,\\\n",
    "                                            SentenceComparator_semantic,\\\n",
    "                                            SentenceComparator_bert_cosine,\\\n",
    "                                            SentenceComparator_SBERT,\\\n",
    "                                            SentenceComparator_NLI,\\\n",
    "                                            SentenceComparator_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "    \"\"\"\n",
    "    This function tests the given model with the given name.\n",
    "\n",
    "    Args:\n",
    "        model(SentenceComparator): A SentenceComparator object.\n",
    "        model_name(str): A string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    test_sentences = [\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi kasada ödeme yapmadan marketten çıkmıştır.\",\n",
    "        \"C kasaya ödeme yapması gerekirken yapmamıştır.\",\n",
    "        \"C markete girdi ve sonra ödeme yapmadan çıktı.\",\n",
    "        \"Şahıs aldığı ürünleri parasını ödemeden çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi kesin unutkan birisidir ve ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Ben C kişisinin kötü birisi olduğunu biliyorum.\",\n",
    "        \"C kişisi iyi bir insan değil.\",\n",
    "        \"Kötü bir insan olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi marketten satın aldığı ürünleri kasada ödeme yapmadan çıkarmıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kameralarıyla doğrulanmıştır.\",\n",
    "        \"C kişisinin kasada ödeme yapmadan çıkması bilinçli bir eylem olarak değerlendirilebilir.\",\n",
    "        \"C kasada ödeme yapmadığı için sorumlu tutulmalıdır.\",\n",
    "        \"C kişisinin ödeme yapmadığına dair hiçbir kanıt bulunmamaktadır.\",\n",
    "        \"Market çalışanları, C'nin ödeme yapmadığını fark etmiştir.\",\n",
    "        \"C kişisi ödeme yapmayı unuttuğunu savunmaktadır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası asılsızdır.\",\n",
    "        \"C'nin kasada ödeme yapmaması kasıtlı bir davranış olarak değerlendirilemez.\",\n",
    "        \"C, dikkat eksikliği nedeniyle ödeme yapmayı unutmuş olabilir.\",\n",
    "        \"C kişisi ödeme yapmadan çıkmayı bir hata olarak tanımlamıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kayıtlarıyla teyit edilmiştir.\",\n",
    "        \"C'nin kasadan ödeme yapmadan ayrılması bilinçli bir davranış olarak nitelendirilebilir.\",\n",
    "        \"C, kasada ödeme yapmadığı için sorumluluk almalıdır.\",\n",
    "        \"C'nin ödeme yapmadığına dair herhangi bir kanıt yoktur.\",\n",
    "        \"Market çalışanları, C’nin kasada ödeme yapmadığını fark etti.\",\n",
    "        \"C kişisi, ödeme yapmayı unuttuğunu iddia ediyor.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası gerçeği yansıtmamaktadır.\",\n",
    "        \"C'nin ödeme yapmaması kasıtlı olarak değerlendirilemez.\",\n",
    "        \"C'nin dikkatsizliği yüzünden ödemeyi unutmuş olabileceği düşünülüyor.\",\n",
    "        \"C kişisi, ödeme yapmadan ayrılmayı bir hata olarak kabul etmiştir.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create log and excel files\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "    create_log(\"model_exec_times.txt\", f\"This file contains the execution times of the models. Number of test_sentences: {len(test_sentences)}\")\n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    elif model_name == \"sentiment_analysis\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"sentiment_1\": [], \"sentiment_2\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    compared_sentence_pairs = []\n",
    "    for index, sentence in enumerate(test_sentences):\n",
    "        for index2, sentence2 in enumerate(test_sentences):\n",
    "            if index != index2 and ( ( sentence, sentence2 ) not in compared_sentence_pairs\\\n",
    "                               and   ( sentence2, sentence ) not in compared_sentence_pairs):\n",
    "\n",
    "                # Calculate the similarity is a essential function of SentenceComparator classes\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                elif model_name == \"sentiment_analysis\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"sentiment_1\": result[0], \"sentiment_2\": result[1]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                \n",
    "                computation_count += 1\n",
    "                compared_sentence_pairs.append((sentence, sentence2))\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "#word2vec_sim = SentenceComparator_Word2Vec(\"utils/word2vec/w2v_.model\")\n",
    "#test_model(word2vec_sim, \"word2vec_sim\")\n",
    "#\n",
    "#sentiment_analysis = SentenceComparator_sentiment_analysis()\n",
    "#test_model(sentiment_analysis, \"sentiment_analysis\")\n",
    "#\n",
    "#sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "#                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "#                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "#                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "#\n",
    "#ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_Result</th>\n",
       "      <th>ollama_model_llama3.1_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi kasada ödeme yapmadan marketten çıkmı...</td>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kasaya ödeme yapması gerekirken yapmamıştır.</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C markete girdi ve sonra ödeme yapmadan çıktı.</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "699  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "700  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "701  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "702  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "703  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim_Result  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...             0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...             0.636648   \n",
       "2    C kişisi kasada ödeme yapmadan marketten çıkmı...             0.625543   \n",
       "3       C kasaya ödeme yapması gerekirken yapmamıştır.             0.657169   \n",
       "4       C markete girdi ve sonra ödeme yapmadan çıktı.             0.587133   \n",
       "..                                                 ...                  ...   \n",
       "699  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.697819   \n",
       "700  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.718892   \n",
       "701  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...             0.703241   \n",
       "702  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.764747   \n",
       "703  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...             0.766007   \n",
       "\n",
       "     sbert_cos_sim_Result  nli_model_CONTRADICTION  nli_model_ENTAILMENT  \\\n",
       "0                  0.9383                        0                     1   \n",
       "1                  0.1996                        1                     0   \n",
       "2                  0.1646                        1                     0   \n",
       "3                  0.4852                        1                     0   \n",
       "4                  0.2732                        1                     0   \n",
       "..                    ...                      ...                   ...   \n",
       "699                0.7362                        1                     0   \n",
       "700                0.6233                        1                     0   \n",
       "701                0.6767                        1                     0   \n",
       "702                0.6354                        1                     0   \n",
       "703                0.8142                        0                     0   \n",
       "\n",
       "     nli_model_NEUTRAL  nli_model_Result  semantic_similarity_Result  \\\n",
       "0                    0          0.769148                      0.9611   \n",
       "1                    0          0.589680                      0.7214   \n",
       "2                    0          0.747958                      0.7042   \n",
       "3                    0          0.710494                      0.6675   \n",
       "4                    0          0.737624                      0.6462   \n",
       "..                 ...               ...                         ...   \n",
       "699                  0          0.844054                      0.6441   \n",
       "700                  0          0.514596                      0.6645   \n",
       "701                  0          0.601376                      0.7230   \n",
       "702                  0          0.867559                      0.6490   \n",
       "703                  1          0.532981                      0.5583   \n",
       "\n",
       "     word2vec_sim_Result  sentiment_analysis_Result  \\\n",
       "0               0.946022                          0   \n",
       "1               0.398693                          1   \n",
       "2               0.312992                          1   \n",
       "3               0.391442                          0   \n",
       "4               0.396546                          1   \n",
       "..                   ...                        ...   \n",
       "699             0.515694                          0   \n",
       "700             0.496692                          0   \n",
       "701             0.457836                          1   \n",
       "702             0.658151                          1   \n",
       "703             0.408180                          1   \n",
       "\n",
       "     ollama_model_llama3.1_Result  \n",
       "0                               0  \n",
       "1                               1  \n",
       "2                               1  \n",
       "3                               1  \n",
       "4                               0  \n",
       "..                            ...  \n",
       "699                             1  \n",
       "700                             1  \n",
       "701                             0  \n",
       "702                             0  \n",
       "703                             1  \n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For sbert_similarity\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "\n",
    "# Dummy encoding\n",
    "dummy = pd.get_dummies(nli_df[\"label\"])\n",
    "nli_df.drop(\"label\", axis=1, inplace=True)\n",
    "nli_df = pd.concat([nli_df, dummy], axis=1)\n",
    "\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "########################################\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_df = excel_to_df(\"sentiment_analysis_log.xlsx\", \"Results\")\n",
    "\n",
    "\"\"\"\n",
    "There are two options for processing sentiment analysis results.\n",
    "\n",
    "1. We can calculate sentiment score as a difference between sentiment_1 and sentiment_2. \n",
    "If they are equal, the score will be 1. Otherwise, the score will be 0. With this approach, we have a binary classification problem.\n",
    "\n",
    "2. We can use sentiment_1 and sentiment_2 as two separate features. With this approach, \n",
    "we have a multi-class classification problem. The first sentence have two labels, and the second sentence have two labels.\n",
    "Threfore, we have 4 labels in total.\n",
    "\n",
    "\n",
    "# Dummy encoding Option 2\n",
    "dummy_1 = pd.get_dummies(sentiment_df[\"sentiment_1\"])\n",
    "dummy_2 = pd.get_dummies(sentiment_df[\"sentiment_2\"])\n",
    "\n",
    "rename_all_columns = lambda df, suffix: df.rename(columns={col: col + suffix for col in df.columns})\n",
    "\n",
    "dummy_1 = rename_all_columns(dummy_1, \"_setnence1\")\n",
    "dummy_2 = rename_all_columns(dummy_2, \"_setnence2\")\n",
    "\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "sentiment_df = pd.concat([sentiment_df, dummy_1, dummy_2], axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# Dummy encoding Option 1\n",
    "sentiment_df[\"Result\"] = (sentiment_df[\"sentiment_1\"] == sentiment_df[\"sentiment_2\"]) * 1\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:Cüaıbzkfakı .23456789]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "#all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], sentiment_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "#                        axis=1, \n",
    "#                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "\n",
    "def concat_columns_except_sentences(df_list, df_list_names):\n",
    "    initial_df = df_list[0].iloc[:,:2]\n",
    "    for index, df in enumerate(df_list):\n",
    "        df_except_sentences = df[ df.columns.difference([\"Sentence1\", \"Sentence2\"]) ] * 1\n",
    "        df_except_sentences.columns = [f\"{df_list_names[index]}_{col}\" for col in df_except_sentences.columns]\n",
    "        initial_df = pd.concat([initial_df, df_except_sentences], axis=1)\n",
    "    return initial_df\n",
    "\n",
    "all_results = concat_columns_except_sentences(\n",
    "    [bert_cos_sim, sbert_cos_df, nli_df, semantic_df, word2vec_df, sentiment_df, ollama_df], \n",
    "    [\"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "             \n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "#all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_Result</th>\n",
       "      <th>ollama_model_llama3.1_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.719333</td>\n",
       "      <td>0.559390</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.099432</td>\n",
       "      <td>0.457386</td>\n",
       "      <td>0.636065</td>\n",
       "      <td>0.672279</td>\n",
       "      <td>0.520255</td>\n",
       "      <td>0.691761</td>\n",
       "      <td>0.301136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.084696</td>\n",
       "      <td>0.179509</td>\n",
       "      <td>0.497114</td>\n",
       "      <td>0.299454</td>\n",
       "      <td>0.498535</td>\n",
       "      <td>0.151172</td>\n",
       "      <td>0.147471</td>\n",
       "      <td>0.131406</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.459078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.433617</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337423</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.244097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.667096</td>\n",
       "      <td>0.444375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509205</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.428049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.726638</td>\n",
       "      <td>0.556550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633499</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.497618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.773496</td>\n",
       "      <td>0.694625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758388</td>\n",
       "      <td>0.774250</td>\n",
       "      <td>0.599536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bert_cos_sim_Result  sbert_cos_sim_Result  nli_model_CONTRADICTION  \\\n",
       "count           704.000000            704.000000               704.000000   \n",
       "mean              0.719333              0.559390                 0.443182   \n",
       "std               0.084696              0.179509                 0.497114   \n",
       "min               0.433617              0.089800                 0.000000   \n",
       "25%               0.667096              0.444375                 0.000000   \n",
       "50%               0.726638              0.556550                 0.000000   \n",
       "75%               0.773496              0.694625                 1.000000   \n",
       "max               1.000000              1.000000                 1.000000   \n",
       "\n",
       "       nli_model_ENTAILMENT  nli_model_NEUTRAL  nli_model_Result  \\\n",
       "count            704.000000         704.000000        704.000000   \n",
       "mean               0.099432           0.457386          0.636065   \n",
       "std                0.299454           0.498535          0.151172   \n",
       "min                0.000000           0.000000          0.337423   \n",
       "25%                0.000000           0.000000          0.509205   \n",
       "50%                0.000000           0.000000          0.633499   \n",
       "75%                0.000000           1.000000          0.758388   \n",
       "max                1.000000           1.000000          0.986145   \n",
       "\n",
       "       semantic_similarity_Result  word2vec_sim_Result  \\\n",
       "count                  704.000000           704.000000   \n",
       "mean                     0.672279             0.520255   \n",
       "std                      0.147471             0.131406   \n",
       "min                      0.167000             0.244097   \n",
       "25%                      0.595700             0.428049   \n",
       "50%                      0.704100             0.497618   \n",
       "75%                      0.774250             0.599536   \n",
       "max                      1.000000             1.000000   \n",
       "\n",
       "       sentiment_analysis_Result  ollama_model_llama3.1_Result  \n",
       "count                 704.000000                    704.000000  \n",
       "mean                    0.691761                      0.301136  \n",
       "std                     0.462094                      0.459078  \n",
       "min                     0.000000                      0.000000  \n",
       "25%                     0.000000                      0.000000  \n",
       "50%                     1.000000                      0.000000  \n",
       "75%                     1.000000                      1.000000  \n",
       "max                     1.000000                      1.000000  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_parameters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sample_indexes = all_results_parameters[all_results_parameters[\"ollama_model_llama3.1_Result\"] == 0].index\n",
    "\n",
    "random_n_index = np.random.choice(positive_sample_indexes, 400)\n",
    "\n",
    "all_results_parameters.drop(random_n_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Normalize lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1_Result\"]\n",
    "X = all_results_parameters.drop(\"ollama_model_llama3.1_Result\", axis=1)\n",
    "\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg, Accuracy:  0.6588235294117647\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Reg, Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.687288</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416837</td>\n",
       "      <td>0.8487</td>\n",
       "      <td>0.561863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_cos_sim_Result  sbert_cos_sim_Result  nli_model_CONTRADICTION  \\\n",
       "0               0.964577                0.9383                        0   \n",
       "1               0.636648                0.1996                        1   \n",
       "2               0.625543                0.1646                        1   \n",
       "3               0.657169                0.4852                        1   \n",
       "4               0.587133                0.2732                        1   \n",
       "..                   ...                   ...                      ...   \n",
       "698             0.687288                0.8754                        0   \n",
       "699             0.697819                0.7362                        1   \n",
       "700             0.718892                0.6233                        1   \n",
       "702             0.764747                0.6354                        1   \n",
       "703             0.766007                0.8142                        0   \n",
       "\n",
       "     nli_model_ENTAILMENT  nli_model_NEUTRAL  nli_model_Result  \\\n",
       "0                       1                  0          0.769148   \n",
       "1                       0                  0          0.589680   \n",
       "2                       0                  0          0.747958   \n",
       "3                       0                  0          0.710494   \n",
       "4                       0                  0          0.737624   \n",
       "..                    ...                ...               ...   \n",
       "698                     1                  0          0.416837   \n",
       "699                     0                  0          0.844054   \n",
       "700                     0                  0          0.514596   \n",
       "702                     0                  0          0.867559   \n",
       "703                     0                  1          0.532981   \n",
       "\n",
       "     semantic_similarity_Result  word2vec_sim_Result  \\\n",
       "0                        0.9611             0.946022   \n",
       "1                        0.7214             0.398693   \n",
       "2                        0.7042             0.312992   \n",
       "3                        0.6675             0.391442   \n",
       "4                        0.6462             0.396546   \n",
       "..                          ...                  ...   \n",
       "698                      0.8487             0.561863   \n",
       "699                      0.6441             0.515694   \n",
       "700                      0.6645             0.496692   \n",
       "702                      0.6490             0.658151   \n",
       "703                      0.5583             0.408180   \n",
       "\n",
       "     sentiment_analysis_Result  \n",
       "0                            0  \n",
       "1                            1  \n",
       "2                            1  \n",
       "3                            0  \n",
       "4                            1  \n",
       "..                         ...  \n",
       "698                          0  \n",
       "699                          0  \n",
       "700                          0  \n",
       "702                          1  \n",
       "703                          1  \n",
       "\n",
       "[421 rows x 9 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70672523,  0.33109902,  0.0062056 ,  0.12326919, -0.08404978,\n",
       "        0.24898955,  0.199046  , -0.59667092,  0.07934688])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7083333333333334\n",
      "Recall:  0.6938775510204082\n",
      "True Positive:  34\n",
      "True Negative:  22\n",
      "False Positive:  14\n",
      "False Negative:  15\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_conf_matrix(y_test, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    print(\"True Positive: \", conf_matrix[1][1])\n",
    "    print(\"True Negative: \", conf_matrix[0][0])\n",
    "    print(\"False Positive: \", conf_matrix[0][1])\n",
    "    print(\"False Negative: \", conf_matrix[1][0])\n",
    "    \n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#model.fit(X_train, y_train, epochs=100, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 00m 27s]\n",
      "val_accuracy: 0.6745098233222961\n",
      "\n",
      "Best val_accuracy So Far: 0.6901960968971252\n",
      "Total elapsed time: 00h 04m 12s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |3                 |hidden_layer_num\n",
      "64                |128               |layer_unit\n",
      "\n",
      "Epoch 1/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5340 - loss: 0.6849 - val_accuracy: 0.5765 - val_loss: 0.6694\n",
      "Epoch 2/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6227 - loss: 0.6173 - val_accuracy: 0.6000 - val_loss: 0.6606\n",
      "Epoch 3/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6658 - loss: 0.5802 - val_accuracy: 0.6353 - val_loss: 0.6398\n",
      "Epoch 4/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6441 - loss: 0.5973 - val_accuracy: 0.6706 - val_loss: 0.6499\n",
      "Epoch 5/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7125 - loss: 0.5491 - val_accuracy: 0.6471 - val_loss: 0.6395\n",
      "Epoch 6/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6525 - loss: 0.5539 - val_accuracy: 0.5647 - val_loss: 0.6814\n",
      "Epoch 7/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6757 - loss: 0.5614 - val_accuracy: 0.6353 - val_loss: 0.6543\n",
      "Epoch 8/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7255 - loss: 0.5059 - val_accuracy: 0.6471 - val_loss: 0.6384\n",
      "Epoch 9/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7275 - loss: 0.5253 - val_accuracy: 0.6118 - val_loss: 0.6635\n",
      "Epoch 10/30\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7277 - loss: 0.5106 - val_accuracy: 0.6353 - val_loss: 0.6615\n",
      "Epoch 11/30\n",
      "\u001b[1m48/84\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7452 - loss: 0.4795 "
     ]
    }
   ],
   "source": [
    "def keras_model_tuner(hp):\n",
    "    hidden_layer_num = hp.Int('hidden_layer_num', min_value=0, max_value=3, step=1)\n",
    "    layer_unit = hp.Int('layer_unit', min_value=32, max_value=128, step=32)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer_unit, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(hidden_layer_num):\n",
    "        model.add(layers.Dense(layer_unit, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='models',\n",
    "    project_name='ollama_model_50pos_50neg_normalized'\n",
    ")\n",
    "\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=30, batch_size = 4, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model8\\tuner0.json\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Accuracy:  0.6046511627906976\n",
      "Precision:  0.625\n",
      "Recall:  0.6521739130434783\n",
      "True Positive:  30\n",
      "True Negative:  22\n",
      "False Positive:  18\n",
      "False Negative:  16\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=5,\n",
    "    directory='log',\n",
    "    project_name='ollama_model8'\n",
    ")\n",
    "\n",
    "tuner.reload()\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_columns(df, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    This function adds the Confidence, Real Value, and Prediction columns to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    if type(df) != pd.DataFrame:\n",
    "        df = pd.DataFrame(df) \n",
    "    df[\"Confidence\"] = 1.0\n",
    "    df[\"Real Value\"] = 5\n",
    "    df[\"Prediction\"] = 5\n",
    "    df[\"Accuracy\"] = 5\n",
    "    #print(df.head())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, val in enumerate(y_pred):\n",
    "        \n",
    "        real =  y_test.iloc[index]\n",
    "        pred = 0 if val < 0.5 else 1\n",
    "        confidence = (val-0.5)*2 if pred == 1 else (0.5-val)*2\n",
    "        \n",
    "        df[\"Confidence\"][index] = confidence\n",
    "        df[\"Real Value\"][index] = real\n",
    "        df[\"Prediction\"][index] = pred\n",
    "        df[\"Accuracy\"][index] = 1 if real == pred else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_accuracy(x,y,predictor):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predictor.\n",
    "\n",
    "    Args:\n",
    "        x: A pandas dataframe.\n",
    "        y: A pandas dataframe.\n",
    "        predictor: A predictor model.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: A float.\n",
    "    \"\"\"\n",
    "    y_pred = predictor.predict(x)\n",
    "    y_pred = [1 if val > 0.5 else 0 for val in y_pred]\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test Accuracy:  0.6046511627906976\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step\n",
      "Train Accuracy:  0.6501457725947521\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step\n",
      "Entire Accuracy:  0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Test Accuracy: \", calculate_accuracy(X_test, y_test, model))\n",
    "# Train\n",
    "print(\"Train Accuracy: \", calculate_accuracy(X_train, y_train, model))\n",
    "# Entire\n",
    "print(\"Entire Accuracy: \", calculate_accuracy(normalized_X, y, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Error rate: % 100.0       Number of high confidence predictions:  2\n",
      "Number of faults:  34    Faults from high confidence predictions:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.618687</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>-1.413313</td>\n",
       "      <td>-1.222307</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.178935</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.981597</td>\n",
       "      <td>0.941479</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>1.527866</td>\n",
       "      <td>0.423428</td>\n",
       "      <td>0.551337</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.325356</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.127971</td>\n",
       "      <td>0.892964</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.765766</td>\n",
       "      <td>1.021323</td>\n",
       "      <td>-0.584252</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.377188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.335093</td>\n",
       "      <td>0.625030</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.708254</td>\n",
       "      <td>1.279648</td>\n",
       "      <td>1.337684</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.179028</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.473230</td>\n",
       "      <td>-0.651793</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.719385</td>\n",
       "      <td>0.213713</td>\n",
       "      <td>-0.133534</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.069303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.259652</td>\n",
       "      <td>-0.856328</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-1.034310</td>\n",
       "      <td>-0.895276</td>\n",
       "      <td>-0.834946</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.456316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.293680</td>\n",
       "      <td>0.824051</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.255231</td>\n",
       "      <td>0.345653</td>\n",
       "      <td>0.551726</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.202754</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.205655</td>\n",
       "      <td>0.829564</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.029443</td>\n",
       "      <td>1.155347</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.426191</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.885727</td>\n",
       "      <td>0.085853</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.424523</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>-0.350788</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.121288</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.728865</td>\n",
       "      <td>-0.610445</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.754404</td>\n",
       "      <td>-0.103637</td>\n",
       "      <td>0.534341</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.361639</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.997051</td>\n",
       "      <td>-0.571854</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.052816</td>\n",
       "      <td>0.361624</td>\n",
       "      <td>-0.268276</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.121303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.477911</td>\n",
       "      <td>-1.934681</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.855626</td>\n",
       "      <td>-1.151517</td>\n",
       "      <td>-0.626609</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.454664</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.310620</td>\n",
       "      <td>0.698905</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>1.681971</td>\n",
       "      <td>1.304647</td>\n",
       "      <td>2.031769</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.527825</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.797458</td>\n",
       "      <td>-2.671776</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.363547</td>\n",
       "      <td>-3.050755</td>\n",
       "      <td>-1.186376</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.768824</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.370743</td>\n",
       "      <td>1.288250</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>2.147658</td>\n",
       "      <td>1.316452</td>\n",
       "      <td>1.438861</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.477996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.129551</td>\n",
       "      <td>-0.034331</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.467056</td>\n",
       "      <td>0.197741</td>\n",
       "      <td>-0.214903</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.163711</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.701104</td>\n",
       "      <td>-0.935164</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.361159</td>\n",
       "      <td>-0.955690</td>\n",
       "      <td>-1.076949</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.222889</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.915053</td>\n",
       "      <td>-0.767567</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.246923</td>\n",
       "      <td>-1.091797</td>\n",
       "      <td>-1.394792</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.261035</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.134005</td>\n",
       "      <td>0.322363</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.791703</td>\n",
       "      <td>-0.144608</td>\n",
       "      <td>-0.500374</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.635402</td>\n",
       "      <td>1.241389</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.022751</td>\n",
       "      <td>0.908133</td>\n",
       "      <td>0.650928</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.237480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.065948</td>\n",
       "      <td>-2.084636</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.325306</td>\n",
       "      <td>0.256073</td>\n",
       "      <td>-0.913435</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.104033</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.193567</td>\n",
       "      <td>0.593605</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-1.474644</td>\n",
       "      <td>0.544257</td>\n",
       "      <td>-0.599970</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-1.258401</td>\n",
       "      <td>0.493819</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.091610</td>\n",
       "      <td>-0.105026</td>\n",
       "      <td>-0.834413</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.261324</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.543042</td>\n",
       "      <td>0.624479</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.387925</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>0.170706</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.180776</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.616539</td>\n",
       "      <td>-0.728976</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.584001</td>\n",
       "      <td>-1.164016</td>\n",
       "      <td>0.057445</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.194956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.351121</td>\n",
       "      <td>0.186191</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>-0.758475</td>\n",
       "      <td>0.105965</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.092276</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.832463</td>\n",
       "      <td>-0.289586</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.807534</td>\n",
       "      <td>-1.048743</td>\n",
       "      <td>-0.811469</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.401483</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.594822</td>\n",
       "      <td>1.502157</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.103916</td>\n",
       "      <td>-0.282103</td>\n",
       "      <td>0.544151</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.439121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.148116</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.721466</td>\n",
       "      <td>-0.699449</td>\n",
       "      <td>1.052366</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.629781</td>\n",
       "      <td>-0.340857</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.971867</td>\n",
       "      <td>0.711612</td>\n",
       "      <td>-0.244729</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.193346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.238685</td>\n",
       "      <td>0.890759</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.128879</td>\n",
       "      <td>0.167881</td>\n",
       "      <td>0.677563</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.149236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.337415</td>\n",
       "      <td>0.378596</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-1.190245</td>\n",
       "      <td>0.481759</td>\n",
       "      <td>1.027209</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.178562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.899967</td>\n",
       "      <td>2.147735</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>1.407687</td>\n",
       "      <td>1.243538</td>\n",
       "      <td>3.055103</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-1.757471</td>\n",
       "      <td>-0.429617</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>1.243810</td>\n",
       "      <td>0.498425</td>\n",
       "      <td>-0.899935</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.072093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -0.618687  0.002055  1.105802 -0.346561 -0.891616 -1.051163 -1.413313   \n",
       "2   0.981597  0.941479 -0.904321 -0.346561  1.121559  1.527866  0.423428   \n",
       "10 -0.127971  0.892964 -0.904321  2.885496 -0.891616 -0.765766  1.021323   \n",
       "13  0.335093  0.625030  1.105802 -0.346561 -0.891616 -0.708254  1.279648   \n",
       "14  0.473230 -0.651793  1.105802 -0.346561 -0.891616 -0.719385  0.213713   \n",
       "15 -1.259652 -0.856328 -0.904321 -0.346561  1.121559 -1.034310 -0.895276   \n",
       "16  0.293680  0.824051  1.105802 -0.346561 -0.891616 -0.255231  0.345653   \n",
       "17 -0.205655  0.829564 -0.904321  2.885496 -0.891616 -0.029443  1.155347   \n",
       "18 -0.885727  0.085853  1.105802 -0.346561 -0.891616 -1.424523  0.011637   \n",
       "22 -0.728865 -0.610445 -0.904321 -0.346561  1.121559  0.754404 -0.103637   \n",
       "23  0.997051 -0.571854  1.105802 -0.346561 -0.891616 -1.052816  0.361624   \n",
       "25 -1.477911 -1.934681  1.105802 -0.346561 -0.891616  0.855626 -1.151517   \n",
       "27  2.310620  0.698905  1.105802 -0.346561 -0.891616  1.681971  1.304647   \n",
       "28 -1.797458 -2.671776  1.105802 -0.346561 -0.891616  0.363547 -3.050755   \n",
       "29  1.370743  1.288250 -0.904321 -0.346561  1.121559  2.147658  1.316452   \n",
       "30 -1.129551 -0.034331 -0.904321 -0.346561  1.121559  0.467056  0.197741   \n",
       "33 -0.701104 -0.935164  1.105802 -0.346561 -0.891616 -1.361159 -0.955690   \n",
       "47 -0.915053 -0.767567  1.105802 -0.346561 -0.891616 -0.246923 -1.091797   \n",
       "48 -0.134005  0.322363 -0.904321 -0.346561  1.121559  0.791703 -0.144608   \n",
       "49  0.635402  1.241389  1.105802 -0.346561 -0.891616 -1.022751  0.908133   \n",
       "53 -1.065948 -2.084636  1.105802 -0.346561 -0.891616 -0.325306  0.256073   \n",
       "54  0.193567  0.593605 -0.904321 -0.346561  1.121559 -1.474644  0.544257   \n",
       "55 -1.258401  0.493819 -0.904321 -0.346561  1.121559 -0.091610 -0.105026   \n",
       "58  0.543042  0.624479  1.105802 -0.346561 -0.891616 -0.387925 -0.040445   \n",
       "60  0.616539 -0.728976  1.105802 -0.346561 -0.891616 -0.584001 -1.164016   \n",
       "64 -0.351121  0.186191  1.105802 -0.346561 -0.891616  0.017823 -0.758475   \n",
       "67 -0.832463 -0.289586 -0.904321 -0.346561  1.121559 -0.807534 -1.048743   \n",
       "68  0.594822  1.502157 -0.904321  2.885496 -0.891616  0.103916 -0.282103   \n",
       "74  0.148116  0.371429  1.105802 -0.346561 -0.891616  0.721466 -0.699449   \n",
       "77  0.629781 -0.340857  1.105802 -0.346561 -0.891616  0.971867  0.711612   \n",
       "79 -0.238685  0.890759  1.105802 -0.346561 -0.891616  0.128879  0.167881   \n",
       "80 -0.337415  0.378596 -0.904321 -0.346561  1.121559 -1.190245  0.481759   \n",
       "81  1.899967  2.147735 -0.904321 -0.346561  1.121559  1.407687  1.243538   \n",
       "85 -1.757471 -0.429617  1.105802 -0.346561 -0.891616  1.243810  0.498425   \n",
       "\n",
       "           7         8  Confidence  Real Value  Prediction  Accuracy  \n",
       "0  -1.222307  0.684966    0.178935           1           0         0  \n",
       "2   0.551337  0.684966    0.325356           0           1         0  \n",
       "10 -0.584252  0.684966    0.377188           0           1         0  \n",
       "13  1.337684 -1.459927    0.179028           0           1         0  \n",
       "14 -0.133534 -1.459927    0.069303           0           1         0  \n",
       "15 -0.834946  0.684966    0.456316           1           0         0  \n",
       "16  0.551726  0.684966    0.202754           0           1         0  \n",
       "17  0.056034  0.684966    0.426191           0           1         0  \n",
       "18 -0.350788 -1.459927    0.121288           1           0         0  \n",
       "22  0.534341  0.684966    0.361639           1           0         0  \n",
       "23 -0.268276 -1.459927    0.121303           0           1         0  \n",
       "25 -0.626609  0.684966    0.454664           1           0         0  \n",
       "27  2.031769  0.684966    0.527825           0           1         0  \n",
       "28 -1.186376  0.684966    0.768824           1           0         0  \n",
       "29  1.438861  0.684966    0.477996           0           1         0  \n",
       "30 -0.214903  0.684966    0.163711           1           0         0  \n",
       "33 -1.076949  0.684966    0.222889           1           0         0  \n",
       "47 -1.394792  0.684966    0.261035           1           0         0  \n",
       "48 -0.500374  0.684966    0.007974           0           1         0  \n",
       "49  0.650928  0.684966    0.237480           0           1         0  \n",
       "53 -0.913435  0.684966    0.104033           1           0         0  \n",
       "54 -0.599970  0.684966    0.071566           1           0         0  \n",
       "55 -0.834413 -1.459927    0.261324           1           0         0  \n",
       "58  0.170706  0.684966    0.180776           0           1         0  \n",
       "60  0.057445  0.684966    0.194956           1           0         0  \n",
       "64  0.105965  0.684966    0.092276           1           0         0  \n",
       "67 -0.811469  0.684966    0.401483           1           0         0  \n",
       "68  0.544151  0.684966    0.439121           0           1         0  \n",
       "74  1.052366  0.684966    0.041267           0           1         0  \n",
       "77 -0.244729 -1.459927    0.193346           0           1         0  \n",
       "79  0.677563 -1.459927    0.149236           0           1         0  \n",
       "80  1.027209 -1.459927    0.178562           1           0         0  \n",
       "81  3.055103  0.684966    0.499953           0           1         0  \n",
       "85 -0.899935  0.684966    0.072093           0           1         0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TEST DATASET\n",
    "\n",
    "high_accuracy_limit = 0.8\n",
    "\n",
    "keras_predict_df_test = pd.DataFrame(model.predict(X_test), columns=[\"Prediction\"])\n",
    "\n",
    "result_df_test = add_output_columns(X_test, y_test, keras_predict_df_test[\"Prediction\"])\n",
    "\n",
    "faults_test = result_df_test[result_df_test[\"Accuracy\"] == 0]\n",
    "\n",
    "high_acc = result_df_test[result_df_test[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "\n",
    "high_acc = high_acc = [high_acc[1], 0] if len(high_acc) == 1 else high_acc\n",
    "\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults_test.shape[0], \"   Faults from high confidence predictions: \", faults_test[ faults_test[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.111762</td>\n",
       "      <td>0.059796</td>\n",
       "      <td>0.278104</td>\n",
       "      <td>-0.061379</td>\n",
       "      <td>-0.240295</td>\n",
       "      <td>-0.039639</td>\n",
       "      <td>-0.013587</td>\n",
       "      <td>0.033680</td>\n",
       "      <td>0.180285</td>\n",
       "      <td>0.259462</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.997712</td>\n",
       "      <td>1.037934</td>\n",
       "      <td>1.004165</td>\n",
       "      <td>0.930516</td>\n",
       "      <td>0.955972</td>\n",
       "      <td>0.981881</td>\n",
       "      <td>0.967461</td>\n",
       "      <td>0.998958</td>\n",
       "      <td>0.923508</td>\n",
       "      <td>0.173302</td>\n",
       "      <td>0.506640</td>\n",
       "      <td>0.506640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.797458</td>\n",
       "      <td>-2.671776</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.474644</td>\n",
       "      <td>-3.050755</td>\n",
       "      <td>-1.394792</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.872411</td>\n",
       "      <td>-0.600797</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.797092</td>\n",
       "      <td>-0.743719</td>\n",
       "      <td>-0.765254</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.128287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.169830</td>\n",
       "      <td>0.254277</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.169266</td>\n",
       "      <td>0.182811</td>\n",
       "      <td>-0.174218</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.198855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.581877</td>\n",
       "      <td>0.792765</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.746169</td>\n",
       "      <td>0.532799</td>\n",
       "      <td>0.551629</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.395410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.310620</td>\n",
       "      <td>2.147735</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>2.147658</td>\n",
       "      <td>1.316452</td>\n",
       "      <td>3.055103</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.768824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "count  34.000000  34.000000  34.000000  34.000000  34.000000  34.000000   \n",
       "mean   -0.111762   0.059796   0.278104  -0.061379  -0.240295  -0.039639   \n",
       "std     0.997712   1.037934   1.004165   0.930516   0.955972   0.981881   \n",
       "min    -1.797458  -2.671776  -0.904321  -0.346561  -0.891616  -1.474644   \n",
       "25%    -0.872411  -0.600797  -0.904321  -0.346561  -0.891616  -0.797092   \n",
       "50%    -0.169830   0.254277   1.105802  -0.346561  -0.891616  -0.169266   \n",
       "75%     0.581877   0.792765   1.105802  -0.346561   1.121559   0.746169   \n",
       "max     2.310620   2.147735   1.105802   2.885496   1.121559   2.147658   \n",
       "\n",
       "               6          7          8  Confidence  Real Value  Prediction  \\\n",
       "count  34.000000  34.000000  34.000000   34.000000   34.000000   34.000000   \n",
       "mean   -0.013587   0.033680   0.180285    0.259462    0.470588    0.529412   \n",
       "std     0.967461   0.998958   0.923508    0.173302    0.506640    0.506640   \n",
       "min    -3.050755  -1.394792  -1.459927    0.007974    0.000000    0.000000   \n",
       "25%    -0.743719  -0.765254   0.684966    0.128287    0.000000    0.000000   \n",
       "50%     0.182811  -0.174218   0.684966    0.198855    0.000000    1.000000   \n",
       "75%     0.532799   0.551629   0.684966    0.395410    1.000000    1.000000   \n",
       "max     1.316452   3.055103   0.684966    0.768824    1.000000    1.000000   \n",
       "\n",
       "       Accuracy  \n",
       "count      34.0  \n",
       "mean        0.0  \n",
       "std         0.0  \n",
       "min         0.0  \n",
       "25%         0.0  \n",
       "50%         0.0  \n",
       "75%         0.0  \n",
       "max         0.0  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faults_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.025358</td>\n",
       "      <td>0.128861</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.179588</td>\n",
       "      <td>-0.119119</td>\n",
       "      <td>-0.052630</td>\n",
       "      <td>0.138005</td>\n",
       "      <td>0.053645</td>\n",
       "      <td>0.161213</td>\n",
       "      <td>0.234797</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.047066</td>\n",
       "      <td>1.021581</td>\n",
       "      <td>1.006573</td>\n",
       "      <td>1.200192</td>\n",
       "      <td>0.984731</td>\n",
       "      <td>1.089204</td>\n",
       "      <td>0.923242</td>\n",
       "      <td>1.086535</td>\n",
       "      <td>0.926858</td>\n",
       "      <td>0.199757</td>\n",
       "      <td>0.501707</td>\n",
       "      <td>0.499521</td>\n",
       "      <td>0.491793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.797458</td>\n",
       "      <td>-2.671776</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.825107</td>\n",
       "      <td>-3.050755</td>\n",
       "      <td>-1.755123</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.723750</td>\n",
       "      <td>-0.607964</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.005830</td>\n",
       "      <td>-0.260402</td>\n",
       "      <td>-0.732861</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.071697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.136022</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.080594</td>\n",
       "      <td>0.257809</td>\n",
       "      <td>-0.229966</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.187721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.625252</td>\n",
       "      <td>0.828186</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.868875</td>\n",
       "      <td>0.752757</td>\n",
       "      <td>0.641750</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.378065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.270909</td>\n",
       "      <td>2.328012</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>2.217580</td>\n",
       "      <td>2.190727</td>\n",
       "      <td>3.547197</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.864057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "count  86.000000  86.000000  86.000000  86.000000  86.000000  86.000000   \n",
       "mean    0.025358   0.128861   0.007246   0.179588  -0.119119  -0.052630   \n",
       "std     1.047066   1.021581   1.006573   1.200192   0.984731   1.089204   \n",
       "min    -1.797458  -2.671776  -0.904321  -0.346561  -0.891616  -1.825107   \n",
       "25%    -0.723750  -0.607964  -0.904321  -0.346561  -0.891616  -1.005830   \n",
       "50%     0.001916   0.136022  -0.904321  -0.346561  -0.891616  -0.080594   \n",
       "75%     0.625252   0.828186   1.105802  -0.346561   1.121559   0.868875   \n",
       "max     3.270909   2.328012   1.105802   2.885496   1.121559   2.217580   \n",
       "\n",
       "               6          7          8  Confidence  Real Value  Prediction  \\\n",
       "count  86.000000  86.000000  86.000000   86.000000   86.000000   86.000000   \n",
       "mean    0.138005   0.053645   0.161213    0.234797    0.534884    0.558140   \n",
       "std     0.923242   1.086535   0.926858    0.199757    0.501707    0.499521   \n",
       "min    -3.050755  -1.755123  -1.459927    0.007680    0.000000    0.000000   \n",
       "25%    -0.260402  -0.732861   0.684966    0.071697    0.000000    0.000000   \n",
       "50%     0.257809  -0.229966   0.684966    0.187721    1.000000    1.000000   \n",
       "75%     0.752757   0.641750   0.684966    0.378065    1.000000    1.000000   \n",
       "max     2.190727   3.547197   0.684966    0.864057    1.000000    1.000000   \n",
       "\n",
       "        Accuracy  \n",
       "count  86.000000  \n",
       "mean    0.604651  \n",
       "std     0.491793  \n",
       "min     0.000000  \n",
       "25%     0.000000  \n",
       "50%     1.000000  \n",
       "75%     1.000000  \n",
       "max     1.000000  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.625\n",
      "Recall:  0.6521739130434783\n",
      "True Positive:  30\n",
      "True Negative:  22\n",
      "False Positive:  18\n",
      "False Negative:  16\n"
     ]
    }
   ],
   "source": [
    "print_conf_matrix(result_df_test[\"Real Value\"], result_df_test[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step\n",
      "Error rate: % 100.0       Number of high confidence predictions:  14\n",
      "Number of faults:  154    Faults from high confidence predictions:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.065948</td>\n",
       "      <td>-2.084636</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.325306</td>\n",
       "      <td>0.256073</td>\n",
       "      <td>-0.913435</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.104033</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.198494</td>\n",
       "      <td>-2.277593</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.689877</td>\n",
       "      <td>0.136632</td>\n",
       "      <td>-1.549180</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.821015</td>\n",
       "      <td>-0.510108</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.449582</td>\n",
       "      <td>-0.118220</td>\n",
       "      <td>-0.967224</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.121077</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.448623</td>\n",
       "      <td>-1.952322</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.878272</td>\n",
       "      <td>0.349125</td>\n",
       "      <td>-1.628139</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.789533</td>\n",
       "      <td>-1.281384</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.111795</td>\n",
       "      <td>0.315793</td>\n",
       "      <td>-1.308072</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.052908</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.813810</td>\n",
       "      <td>0.456882</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.753365</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>-0.347018</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.201854</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.300665</td>\n",
       "      <td>0.917222</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-1.011831</td>\n",
       "      <td>0.441483</td>\n",
       "      <td>0.783276</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.096689</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.148116</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.721466</td>\n",
       "      <td>-0.699449</td>\n",
       "      <td>1.052366</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>-0.618687</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>-1.413313</td>\n",
       "      <td>-1.222307</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.178935</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.478046</td>\n",
       "      <td>1.303687</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.688961</td>\n",
       "      <td>-0.876526</td>\n",
       "      <td>-0.843056</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.043959</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -1.065948 -2.084636  1.105802 -0.346561 -0.891616 -0.325306  0.256073   \n",
       "1   -1.198494 -2.277593  1.105802 -0.346561 -0.891616  0.689877  0.136632   \n",
       "2   -0.821015 -0.510108  1.105802 -0.346561 -0.891616  0.449582 -0.118220   \n",
       "3   -1.448623 -1.952322  1.105802 -0.346561 -0.891616 -0.878272  0.349125   \n",
       "7   -0.789533 -1.281384  1.105802 -0.346561 -0.891616 -0.111795  0.315793   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "414  0.813810  0.456882  1.105802 -0.346561 -0.891616 -0.753365  0.750500   \n",
       "416  0.300665  0.917222 -0.904321 -0.346561  1.121559 -1.011831  0.441483   \n",
       "420  0.148116  0.371429  1.105802 -0.346561 -0.891616  0.721466 -0.699449   \n",
       "421 -0.618687  0.002055  1.105802 -0.346561 -0.891616 -1.051163 -1.413313   \n",
       "428  0.478046  1.303687 -0.904321 -0.346561  1.121559 -0.688961 -0.876526   \n",
       "\n",
       "            7         8  Confidence  Real Value  Prediction  Accuracy  \n",
       "0   -0.913435  0.684966    0.104033           1           0         0  \n",
       "1   -1.549180  0.684966    0.055956           1           0         0  \n",
       "2   -0.967224 -1.459927    0.121077           1           0         0  \n",
       "3   -1.628139  0.684966    0.049463           1           0         0  \n",
       "7   -1.308072 -1.459927    0.052908           1           0         0  \n",
       "..        ...       ...         ...         ...         ...       ...  \n",
       "414 -0.347018  0.684966    0.201854           0           1         0  \n",
       "416  0.783276  0.684966    0.096689           1           0         0  \n",
       "420  1.052366  0.684966    0.041267           0           1         0  \n",
       "421 -1.222307  0.684966    0.178935           1           0         0  \n",
       "428 -0.843056  0.684966    0.043959           1           0         0  \n",
       "\n",
       "[154 rows x 13 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTIRE DATASET\n",
    "# Predicts\n",
    "keras_predict_df = pd.DataFrame(model.predict(normalized_X), columns=[\"Prediction\"])\n",
    "\n",
    "#sort the dataframe by ID\n",
    "result_df = add_output_columns( \n",
    "    df = normalized_X,\n",
    "    y_test = y,\n",
    "    y_pred = keras_predict_df[\"Prediction\"]\n",
    ")\n",
    "faults = result_df[ result_df[\"Accuracy\"] == 0 ]\n",
    "\n",
    "# Accuracy counts of the model where the confidence is greater than high_accuracy_limit\n",
    "high_acc = result_df[result_df[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "\n",
    "if len(high_acc) == 1:\n",
    "    high_acc = [high_acc[1], 0]\n",
    "\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults.shape[0], \"   Faults from high confidence predictions: \", faults[ faults[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.639228e-17</td>\n",
       "      <td>3.383537e-16</td>\n",
       "      <td>3.524518e-17</td>\n",
       "      <td>-2.114711e-17</td>\n",
       "      <td>-3.524518e-17</td>\n",
       "      <td>-2.114711e-17</td>\n",
       "      <td>8.458842e-17</td>\n",
       "      <td>1.127846e-16</td>\n",
       "      <td>1.409807e-16</td>\n",
       "      <td>0.572476</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.823413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>1.000994e+00</td>\n",
       "      <td>0.315560</td>\n",
       "      <td>0.494151</td>\n",
       "      <td>0.495646</td>\n",
       "      <td>0.381698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.473657e+00</td>\n",
       "      <td>-2.660679e+00</td>\n",
       "      <td>-8.520724e-01</td>\n",
       "      <td>-3.355336e-01</td>\n",
       "      <td>-9.572616e-01</td>\n",
       "      <td>-1.971620e+00</td>\n",
       "      <td>-3.035407e+00</td>\n",
       "      <td>-2.139442e+00</td>\n",
       "      <td>-1.581139e+00</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.744611e-01</td>\n",
       "      <td>-6.732226e-01</td>\n",
       "      <td>-8.520724e-01</td>\n",
       "      <td>-3.355336e-01</td>\n",
       "      <td>-9.572616e-01</td>\n",
       "      <td>-8.473640e-01</td>\n",
       "      <td>-5.471043e-01</td>\n",
       "      <td>-6.426903e-01</td>\n",
       "      <td>-1.581139e+00</td>\n",
       "      <td>0.298570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.195077e-01</td>\n",
       "      <td>1.184927e-02</td>\n",
       "      <td>-8.520724e-01</td>\n",
       "      <td>-3.355336e-01</td>\n",
       "      <td>-9.572616e-01</td>\n",
       "      <td>-1.163927e-02</td>\n",
       "      <td>1.794270e-01</td>\n",
       "      <td>-1.811255e-01</td>\n",
       "      <td>6.324555e-01</td>\n",
       "      <td>0.605217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.205919e-01</td>\n",
       "      <td>7.465837e-01</td>\n",
       "      <td>1.173609e+00</td>\n",
       "      <td>-3.355336e-01</td>\n",
       "      <td>1.044646e+00</td>\n",
       "      <td>7.909264e-01</td>\n",
       "      <td>6.812458e-01</td>\n",
       "      <td>5.593387e-01</td>\n",
       "      <td>6.324555e-01</td>\n",
       "      <td>0.863125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.224327e+00</td>\n",
       "      <td>2.293064e+00</td>\n",
       "      <td>1.173609e+00</td>\n",
       "      <td>2.980328e+00</td>\n",
       "      <td>1.044646e+00</td>\n",
       "      <td>2.249869e+00</td>\n",
       "      <td>2.182045e+00</td>\n",
       "      <td>3.643392e+00</td>\n",
       "      <td>6.324555e-01</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  5.040000e+02  5.040000e+02  5.040000e+02  5.040000e+02  5.040000e+02   \n",
       "mean  -5.639228e-17  3.383537e-16  3.524518e-17 -2.114711e-17 -3.524518e-17   \n",
       "std    1.000994e+00  1.000994e+00  1.000994e+00  1.000994e+00  1.000994e+00   \n",
       "min   -3.473657e+00 -2.660679e+00 -8.520724e-01 -3.355336e-01 -9.572616e-01   \n",
       "25%   -5.744611e-01 -6.732226e-01 -8.520724e-01 -3.355336e-01 -9.572616e-01   \n",
       "50%    1.195077e-01  1.184927e-02 -8.520724e-01 -3.355336e-01 -9.572616e-01   \n",
       "75%    6.205919e-01  7.465837e-01  1.173609e+00 -3.355336e-01  1.044646e+00   \n",
       "max    3.224327e+00  2.293064e+00  1.173609e+00  2.980328e+00  1.044646e+00   \n",
       "\n",
       "                  5             6             7             8  Confidence  \\\n",
       "count  5.040000e+02  5.040000e+02  5.040000e+02  5.040000e+02  504.000000   \n",
       "mean  -2.114711e-17  8.458842e-17  1.127846e-16  1.409807e-16    0.572476   \n",
       "std    1.000994e+00  1.000994e+00  1.000994e+00  1.000994e+00    0.315560   \n",
       "min   -1.971620e+00 -3.035407e+00 -2.139442e+00 -1.581139e+00    0.004024   \n",
       "25%   -8.473640e-01 -5.471043e-01 -6.426903e-01 -1.581139e+00    0.298570   \n",
       "50%   -1.163927e-02  1.794270e-01 -1.811255e-01  6.324555e-01    0.605217   \n",
       "75%    7.909264e-01  6.812458e-01  5.593387e-01  6.324555e-01    0.863125   \n",
       "max    2.249869e+00  2.182045e+00  3.643392e+00  6.324555e-01    0.999964   \n",
       "\n",
       "       Real Value  Prediction    Accuracy  \n",
       "count  504.000000  504.000000  504.000000  \n",
       "mean     0.420635    0.430556    0.823413  \n",
       "std      0.494151    0.495646    0.381698  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    1.000000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      1.000000    1.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence = result_df[ result_df[\"Confidence\"] > high_accuracy_limit ]\n",
    "\n",
    "high_confidence[\"Accuracy\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.909004</td>\n",
       "      <td>-0.567010</td>\n",
       "      <td>-0.617161</td>\n",
       "      <td>0.576884</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>-1.437072</td>\n",
       "      <td>-0.319951</td>\n",
       "      <td>-0.693894</td>\n",
       "      <td>0.833915</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.381493</td>\n",
       "      <td>1.772461</td>\n",
       "      <td>0.729949</td>\n",
       "      <td>1.515212</td>\n",
       "      <td>1.033871</td>\n",
       "      <td>1.135221</td>\n",
       "      <td>2.177646</td>\n",
       "      <td>1.994235</td>\n",
       "      <td>1.066538</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.468807</td>\n",
       "      <td>0.468807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.234867</td>\n",
       "      <td>-2.338787</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.980405</td>\n",
       "      <td>-3.254915</td>\n",
       "      <td>-2.060255</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.800247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.498286</td>\n",
       "      <td>-1.740208</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.209275</td>\n",
       "      <td>-2.875935</td>\n",
       "      <td>-1.939765</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.825330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.061127</td>\n",
       "      <td>-1.376208</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.310105</td>\n",
       "      <td>-2.659450</td>\n",
       "      <td>-1.303686</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.838223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.275645</td>\n",
       "      <td>1.161863</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.077482</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>1.769475</td>\n",
       "      <td>0.498425</td>\n",
       "      <td>1.509869</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.847431</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.270909</td>\n",
       "      <td>2.328012</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>2.217580</td>\n",
       "      <td>2.190727</td>\n",
       "      <td>3.547197</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.864057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "count  14.000000  14.000000  14.000000  14.000000  14.000000  14.000000   \n",
       "mean   -0.909004  -0.567010  -0.617161   0.576884   0.258770   0.619300   \n",
       "std     2.381493   1.772461   0.729949   1.515212   1.033871   1.135221   \n",
       "min    -3.234867  -2.338787  -0.904321  -0.346561  -0.891616  -0.980405   \n",
       "25%    -2.498286  -1.740208  -0.904321  -0.346561  -0.891616  -0.209275   \n",
       "50%    -2.061127  -1.376208  -0.904321  -0.346561   1.121559   0.310105   \n",
       "75%     1.275645   1.161863  -0.904321   2.077482   1.121559   1.769475   \n",
       "max     3.270909   2.328012   1.105802   2.885496   1.121559   2.217580   \n",
       "\n",
       "               6          7          8  Confidence  Real Value  Prediction  \\\n",
       "count  14.000000  14.000000  14.000000   14.000000   14.000000   14.000000   \n",
       "mean   -1.437072  -0.319951  -0.693894    0.833915    0.285714    0.285714   \n",
       "std     2.177646   1.994235   1.066538    0.019445    0.468807    0.468807   \n",
       "min    -3.254915  -2.060255  -1.459927    0.800247    0.000000    0.000000   \n",
       "25%    -2.875935  -1.939765  -1.459927    0.825330    0.000000    0.000000   \n",
       "50%    -2.659450  -1.303686  -1.459927    0.838223    0.000000    0.000000   \n",
       "75%     0.498425   1.509869   0.684966    0.847431    0.750000    0.750000   \n",
       "max     2.190727   3.547197   0.684966    0.864057    1.000000    1.000000   \n",
       "\n",
       "       Accuracy  \n",
       "count      14.0  \n",
       "mean        1.0  \n",
       "std         0.0  \n",
       "min         1.0  \n",
       "25%         1.0  \n",
       "50%         1.0  \n",
       "75%         1.0  \n",
       "max         1.0  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.270909</td>\n",
       "      <td>2.328012</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>2.217580</td>\n",
       "      <td>2.190727</td>\n",
       "      <td>3.547197</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.864057</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>-1.509495</td>\n",
       "      <td>-2.338787</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.842419</td>\n",
       "      <td>-2.795209</td>\n",
       "      <td>-1.755123</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.849083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>-2.387759</td>\n",
       "      <td>-1.784174</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>-0.243738</td>\n",
       "      <td>-3.254915</td>\n",
       "      <td>-2.060255</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.842740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-2.811472</td>\n",
       "      <td>-1.056451</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.240831</td>\n",
       "      <td>-2.828541</td>\n",
       "      <td>-1.897164</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.856455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-1.371192</td>\n",
       "      <td>-1.608308</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.014721</td>\n",
       "      <td>-2.687574</td>\n",
       "      <td>-0.412978</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.807007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-2.524212</td>\n",
       "      <td>-2.000837</td>\n",
       "      <td>1.105802</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>0.352234</td>\n",
       "      <td>-3.063255</td>\n",
       "      <td>-2.010680</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.848994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-3.234867</td>\n",
       "      <td>-1.378965</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>0.267976</td>\n",
       "      <td>-2.891733</td>\n",
       "      <td>-1.203644</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.832037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>-1.869958</td>\n",
       "      <td>-1.373452</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.114605</td>\n",
       "      <td>-2.895900</td>\n",
       "      <td>-1.403728</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.835012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-2.252297</td>\n",
       "      <td>-1.917039</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>1.340385</td>\n",
       "      <td>-1.828576</td>\n",
       "      <td>0.162950</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.800247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-2.420506</td>\n",
       "      <td>-1.292961</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.980405</td>\n",
       "      <td>-2.631326</td>\n",
       "      <td>-1.995278</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.825317</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-2.841015</td>\n",
       "      <td>-1.529471</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>-0.346561</td>\n",
       "      <td>1.121559</td>\n",
       "      <td>-0.897737</td>\n",
       "      <td>-2.574383</td>\n",
       "      <td>-1.953965</td>\n",
       "      <td>-1.459927</td>\n",
       "      <td>0.841435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2.541089</td>\n",
       "      <td>1.967458</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>2.162892</td>\n",
       "      <td>1.274092</td>\n",
       "      <td>2.323365</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.805067</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2.526796</td>\n",
       "      <td>2.145530</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>1.912505</td>\n",
       "      <td>1.947680</td>\n",
       "      <td>2.221155</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.841994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2.157925</td>\n",
       "      <td>1.901302</td>\n",
       "      <td>-0.904321</td>\n",
       "      <td>2.885496</td>\n",
       "      <td>-0.891616</td>\n",
       "      <td>2.066251</td>\n",
       "      <td>1.919903</td>\n",
       "      <td>1.958843</td>\n",
       "      <td>0.684966</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "33   3.270909  2.328012 -0.904321  2.885496 -0.891616  2.217580  2.190727   \n",
       "220 -1.509495 -2.338787 -0.904321 -0.346561  1.121559  0.842419 -2.795209   \n",
       "221 -2.387759 -1.784174  1.105802 -0.346561 -0.891616 -0.243738 -3.254915   \n",
       "223 -2.811472 -1.056451 -0.904321 -0.346561  1.121559 -0.240831 -2.828541   \n",
       "224 -1.371192 -1.608308 -0.904321 -0.346561  1.121559 -0.014721 -2.687574   \n",
       "227 -2.524212 -2.000837  1.105802 -0.346561 -0.891616  0.352234 -3.063255   \n",
       "229 -3.234867 -1.378965 -0.904321 -0.346561  1.121559  0.267976 -2.891733   \n",
       "230 -1.869958 -1.373452 -0.904321 -0.346561  1.121559 -0.114605 -2.895900   \n",
       "249 -2.252297 -1.917039 -0.904321 -0.346561  1.121559  1.340385 -1.828576   \n",
       "251 -2.420506 -1.292961 -0.904321 -0.346561  1.121559 -0.980405 -2.631326   \n",
       "254 -2.841015 -1.529471 -0.904321 -0.346561  1.121559 -0.897737 -2.574383   \n",
       "293  2.541089  1.967458 -0.904321  2.885496 -0.891616  2.162892  1.274092   \n",
       "348  2.526796  2.145530 -0.904321  2.885496 -0.891616  1.912505  1.947680   \n",
       "394  2.157925  1.901302 -0.904321  2.885496 -0.891616  2.066251  1.919903   \n",
       "\n",
       "            7         8  Confidence  Real Value  Prediction  Accuracy  \n",
       "33   3.547197  0.684966    0.864057           1           1         1  \n",
       "220 -1.755123 -1.459927    0.849083           0           0         1  \n",
       "221 -2.060255 -1.459927    0.842740           0           0         1  \n",
       "223 -1.897164 -1.459927    0.856455           0           0         1  \n",
       "224 -0.412978 -1.459927    0.807007           0           0         1  \n",
       "227 -2.010680 -1.459927    0.848994           0           0         1  \n",
       "229 -1.203644  0.684966    0.832037           0           0         1  \n",
       "230 -1.403728 -1.459927    0.835012           0           0         1  \n",
       "249  0.162950 -1.459927    0.800247           0           0         1  \n",
       "251 -1.995278 -1.459927    0.825317           0           0         1  \n",
       "254 -1.953965 -1.459927    0.841435           0           0         1  \n",
       "293  2.323365  0.684966    0.805067           1           1         1  \n",
       "348  2.221155  0.684966    0.841994           1           1         1  \n",
       "394  1.958843  0.684966    0.825367           1           1         1  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
