{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a correct answer for the question in the sample.txt file\n",
    "correct_answer = {\n",
    "  \"Suçun_Temel_Unsurları\": {\n",
    "    \"Fail\": \"C\",\n",
    "    \"Mağdur\": \"Market (Mülkiyet hakkı ihlal edilen taraf)\",\n",
    "    \"Konu\": \"Marketten alınan ürünler\",\n",
    "    \"Fiil\": \"Ürünlerin kasada ödeme yapılmaksızın dışarı çıkarılması\",\n",
    "    \"Netice\": {\n",
    "      \"Marketin_malvarlığında_azalma\": True,\n",
    "      \"Nedensellik_Bağı\": \"C’nin fiili ile netice arasında nedensellik bağı mevcuttur.\",\n",
    "      \"Objektif_İsnadiyet\": \"Fiil, neticeyi doğuracak şekilde gerçekleştirilmiştir.\"\n",
    "    }\n",
    "  },\n",
    "  \"Manevi_Unsur\": \"C, bu eylemi kasten mi gerçekleştirmiştir? C’nin kasıtlı olup olmadığı, marketten çıkarken ödeme yapmadığını fark etmesiyle bağlantılıdır. C, ödeme yapmayı unuttuğunu iddia etmektedir, bu nedenle kast unsuru tartışmalı olabilir.\",\n",
    "  \"Hukuka_Aykırılık_Unsuru\": \"Hukuka aykırılık unsuru açısından, C’nin eylemi hırsızlık suçu açısından değerlendirildiğinde, hukuka aykırı bir durum mevcuttur. Ancak, manevi unsurdaki belirsizlik bu hususu etkileyebilir.\",\n",
    "  \"Suçun_Nitelikli_Unsurları\": \"Bu olayda nitelikli bir unsur yoktur.\",\n",
    "  \"Kusurluluk\": \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"Yaptırımın_Uygulanması_İçin_Gereken_Veya_Yaptırım_Uygulanmasını_Engelleyen_Özel_Koşullar\": \"C’nin ödeme yapmayı unuttuğunu iddia etmesi, suçun manevi unsurunu etkileyebilir ve bu durum yaptırım uygulanmasını engelleyebilir.\",\n",
    "  \"Suçun_Özel_Görünüş_Biçimleri\": {\n",
    "    \"Teşebbüs\": \"Eylem tamamlanmıştır, teşebbüs söz konusu değildir.\",\n",
    "    \"İştirak\": \"Olayda iştirak eden başka bir kişi yoktur.\",\n",
    "    \"İçtima\": \"Tek bir suç tipi söz konusudur.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "test_sentences = [\"Havayı kirletmek, çevreyi kirletmek anlamına gelir.\", \"Hava kirliliği, çevre kirliliğinin bir parçasıdır.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "Cosine similarity (%) : 0.0\n",
      "0.04399732626783181\n",
      "67.0\n"
     ]
    }
   ],
   "source": [
    "# THRASH CODE\n",
    "\n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = correct_answer[\"Kusurluluk\"]\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime          Benzerlik Skoru\n",
      "------------  -----------------\n",
      "ateşkes                0.823591\n",
      "uzlaşma                0.794623\n",
      "ittifak                0.774118\n",
      "antlaşma               0.773479\n",
      "müzakereler            0.762028\n",
      "barışı                 0.759955\n",
      "antlaşmayı             0.749817\n",
      "müzakereleri           0.747546\n",
      "mütareke               0.727691\n",
      "müttefiklik            0.717039\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"utils/word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"barış\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_Word2Vec:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.generate_model(model_name)\n",
    "\n",
    "    def generate_model(self, model_name):\n",
    "        return Word2Vec.load(model_name)\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    def extract_key_features(self, words_1, words_2):\n",
    "        \n",
    "        # Nested iteration to compare each word in the sentences\n",
    "        #Dict: {word_1:{word_comp1:score, word_comp2:score, ...}, word_2:{word_comp1:score, word_comp2:score, ...}}\n",
    "        searched_pairs = [];similarity_dict = {}\n",
    "\n",
    "        for word_1 in words_1:\n",
    "            if word_1 not in similarity_dict:\n",
    "                similarity_dict.update({word_1:{}})\n",
    "            for word_2 in words_2:\n",
    "                if (word_1, word_2) in searched_pairs or (word_2, word_1) in searched_pairs:\n",
    "                    continue\n",
    "                try:\n",
    "                    searched_pairs.append((word_1, word_2))\n",
    "                    # Calculate the similarity between the words\n",
    "                    similarity_dict[word_1].update({word_2:self.model.wv.similarity(word_1, word_2)})\n",
    "                except:\n",
    "                    pass\n",
    "        return similarity_dict\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Clean the sentences and split them into words\n",
    "        sentence_1 = self.clean_sentence(sentence_1); words_1 = sentence_1.split()\n",
    "        sentence_2 = self.clean_sentence(sentence_2); words_2 = sentence_2.split()\n",
    "\n",
    "        # Extract key features\n",
    "        similarity_dict = self.extract_key_features(words_1, words_2)\n",
    "            \n",
    "        # Extract informations from the similarity_dict\n",
    "        key_features = []\n",
    "        for key, value in similarity_dict.items():\n",
    "            if len(value) > 0:\n",
    "                # Sort and get the best match\n",
    "                sorted_dict = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "                max_score = sorted_dict[0][1]\n",
    "                best_key = sorted_dict[0][0]\n",
    "\n",
    "                key_features.append({\"key\":key, \"score\":max_score,\"best_match\":best_key})\n",
    "\n",
    "        # Calculate the average score\n",
    "        avg_score = sum([x[\"score\"] for x in key_features]) / len(key_features)\n",
    "\n",
    "        return avg_score#,key_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_comparator = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "#avg_score,sim_dict = word2vec_comparator.compare_sentences(test_sentences[0], test_sentences[1])\n",
    "#\n",
    "#print(tabulate([x.values() for x in sim_dict], headers=[\"Kelime\", \"En Benzer Kelime, Benzerlik Skoru\"]))\n",
    "#print(\"\\nAverage Score: \", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding with turkish trained word2vec\n",
    "#def calculate_vector_of_senetence(sentence):\n",
    "#    sentence_vector = np.zeros(100)\n",
    "#    for word in sentence.split():\n",
    "#        try:\n",
    "#            sentence_vector += model.wv[word]\n",
    "#        except:\n",
    "#            pass\n",
    "#    return sentence_vector\n",
    "#\n",
    "#def cosine_similarity(v1, v2):\n",
    "#    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "#\n",
    "#def measure_similarity(sentence_1, sentence_2):\n",
    "#    v1 = calculate_vector_of_senetence(sentence_1)\n",
    "#    v2 = calculate_vector_of_senetence(sentence_2)\n",
    "#    cos_sim = cosine_similarity(v1, v2)\n",
    "#\n",
    "#    print(\"\\nCosine similarity: \", cos_sim)\n",
    "#\n",
    "#\n",
    "#measure_similarity(\n",
    "#    test_sentences[0],\n",
    "#    test_sentences[1]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelfile System Prompt: \n",
    "\n",
    "Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {\"cümleler\":<cümleler>,\"analiz/neden\":<kısaca>,\"değerlendirme\": <>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class SentenceComparator_Ollama:\n",
    "    def __init__(self,modelfile_system, llama_version=\"llama3.1\", temperature=0.4):\n",
    "        self.system_prompt = modelfile_system\n",
    "        self.llama_version = llama_version\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.role_messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: C kişisi kasada ödeme yapmadan marketten çıkmıştır, İkinci cümle: C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 0\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: Havalar güzelken denize gitmek çok iyi olur., İkinci cümle: Bir insan ev almadan önce araba parası biriktirmeli.'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 1\"\n",
    "                }\n",
    "            ]\n",
    "        self.generate_model()\n",
    "    \n",
    "    def generate_model(self):\n",
    "        modelfile = f'''\n",
    "        FROM {self.llama_version}\n",
    "        SYSTEM {self.system_prompt} \n",
    "        PARAMETER temperature {self.temperature}\n",
    "        '''\n",
    "        ollama.create(model=f'MAHDAN_{self.llama_version}', modelfile=modelfile)\n",
    "\n",
    "    def calculate_similarity(self,sentence_1,sentence_2):\n",
    "        sentence_in = 'İlk Cümle: ' + sentence_1 + ' ,\\n İkinci Cümle: ' + sentence_2        \n",
    "\n",
    "        # Concat messages and sentence\n",
    "        messages_temp = self.role_messages.copy()\n",
    "\n",
    "        messages_temp.append({\n",
    "            'role': 'user',\n",
    "            'content': sentence_in\n",
    "        })\n",
    "\n",
    "        response = ollama.chat(model=f'MAHDAN_{self.llama_version}', messages= messages_temp)\n",
    "\n",
    "        return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )\n",
    "#\n",
    "#\n",
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\umutc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "# Class of Semantic Similarity\n",
    "class SentenceComparator_semantic:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#model_name = 'bert-base-multilingual-cased'\n",
    "# Calculate cosine similarity between two sentences with BERT\n",
    "class SentenceComparator_bert_cosine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return BertModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentence):\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings1 = self.get_embeddings(sentence_1)\n",
    "        embeddings2 = self.get_embeddings(sentence_2)\n",
    "        \n",
    "        return cosine_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "#model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Class of SBERT Similarity\n",
    "class SentenceComparator_SBERT:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "        \n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çalışmak için *İnternet* gerektiriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NLI pipeline oluşturma (Türkçe destekleyen model kullanılabilir)\n",
    "#nli_model = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "class SentenceComparator_NLI:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return pipeline(\"text-classification\", model=self.model_name)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        input_sentence = sentence_1 + ' [SEP] ' + sentence_2\n",
    "        result = self.model(input_sentence)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_sentiment_analysis:\n",
    "    def __init__(self):\n",
    "        model_id = \"saribasmetehan/bert-base-turkish-sentiment-analysis\"\n",
    "        self.classifer = pipeline(\"text-classification\",model = model_id)\n",
    "    \n",
    "    def clearify_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        sentence_1 = self.clearify_sentence(sentence_1)\n",
    "        sentence_2 = self.clearify_sentence(sentence_2)\n",
    "\n",
    "        pred1 = self.classifer(sentence_1)\n",
    "        pred2 = self.classifer(sentence_2)\n",
    "\n",
    "        is_similar = pred1[0][\"label\"] == pred2[0][\"label\"]\n",
    "        \n",
    "        return is_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZEMBEREK https://github.com/ahmetaa/zemberek-nlp bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import jpype\n",
    "import os\n",
    "import atexit\n",
    "\n",
    "print(jpype.isJVMStarted())\n",
    "\n",
    "class SentenceComparator_jpype:\n",
    "    def __init__(self):\n",
    "        # JVM'i başlat\n",
    "        if not jpype.isJVMStarted():\n",
    "            jpype.startJVM(\"C:/Program Files/Java/jdk-22/bin/server/jvm.dll\", \n",
    "                           \"-Djava.class.path=utils/zemberek-full.jar\")\n",
    "        \n",
    "        # Zemberek sınıfını başlat\n",
    "        TurkishMorphology = jpype.JClass('zemberek.morphology.TurkishMorphology')\n",
    "        self.morphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "        # JVM'i kapatmayı atexit ile garanti altına al\n",
    "        atexit.register(self.shutdown_jvm)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Cümlelerin analizini yap\n",
    "        analysis1 = self.morphology.analyzeSentence(sentence_1)\n",
    "        analysis2 = self.morphology.analyzeSentence(sentence_2)\n",
    "        return analysis1, analysis2\n",
    "    \n",
    "    def shutdown_jvm(self):\n",
    "        # JVM'i kapat\n",
    "        if jpype.isJVMStarted():\n",
    "            jpype.shutdownJVM()\n",
    "\n",
    "# Örnek kullanım\n",
    "\n",
    "#jvm = SentenceComparator_jpype()\n",
    "#test_sentence = \"Bu güzel bir gün.\"\n",
    "#print(jvm.calculate_similarity(\"keşke hemen şurada ölsen ve gebersen.\", test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    sentences = [\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi kasada ödeme yapmadan marketten çıkmıştır.\",\n",
    "        \"C kasaya ödeme yapması gerekirken yapmamıştır.\",\n",
    "        \"C markete girdi ve sonra ödeme yapmadan çıktı.\",\n",
    "        \"Şahıs aldığı ürünleri parasını ödemeden çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi kesin unutkan birisidir ve ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Ben C kişisinin kötü birisi olduğunu biliyorum.\",\n",
    "        \"C kişisi iyi bir insan değil.\",\n",
    "        \"Kötü bir insan olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi marketten satın aldığı ürünleri kasada ödeme yapmadan çıkarmıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kameralarıyla doğrulanmıştır.\",\n",
    "        \"C kişisinin kasada ödeme yapmadan çıkması bilinçli bir eylem olarak değerlendirilebilir.\",\n",
    "        \"C kasada ödeme yapmadığı için sorumlu tutulmalıdır.\",\n",
    "        \"C kişisinin ödeme yapmadığına dair hiçbir kanıt bulunmamaktadır.\",\n",
    "        \"Market çalışanları, C'nin ödeme yapmadığını fark etmiştir.\",\n",
    "        \"C kişisi ödeme yapmayı unuttuğunu savunmaktadır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası asılsızdır.\",\n",
    "        \"C'nin kasada ödeme yapmaması kasıtlı bir davranış olarak değerlendirilemez.\",\n",
    "        \"C, dikkat eksikliği nedeniyle ödeme yapmayı unutmuş olabilir.\",\n",
    "        \"C kişisi ödeme yapmadan çıkmayı bir hata olarak tanımlamıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kayıtlarıyla teyit edilmiştir.\",\n",
    "        \"C'nin kasadan ödeme yapmadan ayrılması bilinçli bir davranış olarak nitelendirilebilir.\",\n",
    "        \"C, kasada ödeme yapmadığı için sorumluluk almalıdır.\",\n",
    "        \"C'nin ödeme yapmadığına dair herhangi bir kanıt yoktur.\",\n",
    "        \"Market çalışanları, C’nin kasada ödeme yapmadığını fark etti.\",\n",
    "        \"C kişisi, ödeme yapmayı unuttuğunu iddia ediyor.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası gerçeği yansıtmamaktadır.\",\n",
    "        \"C'nin ödeme yapmaması kasıtlı olarak değerlendirilemez.\",\n",
    "        \"C'nin dikkatsizliği yüzünden ödemeyi unutmuş olabileceği düşünülüyor.\",\n",
    "        \"C kişisi, ödeme yapmadan ayrılmayı bir hata olarak kabul etmiştir.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    compared_sentence_pairs = []\n",
    "\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for index2, sentence2 in enumerate(sentences):\n",
    "            if index != index2 and ( ( sentence, sentence2 ) not in compared_sentence_pairs and ( sentence2, sentence ) not in compared_sentence_pairs):\n",
    "\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                #print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\n",
    "                #print(\"Return: \\n\", result)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                #print(\"\\n\")\n",
    "                computation_count += 1\n",
    "                compared_sentence_pairs.append((sentence, sentence2))\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n",
    "\n",
    "# Test the NLI model    \n",
    "#test_model(nli_model, \"nli_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 46.18099522590637 seconds\n",
      "Total time: 108.06718468666077 seconds\n",
      "Total time: 3431.167523622513 seconds\n"
     ]
    }
   ],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "word2vec_sim = SentenceComparator_Word2Vec(\"utils/word2vec/w2v_.model\")\n",
    "test_model(word2vec_sim, \"word2vec_sim\")\n",
    "\n",
    "sentiment_comparator = SentenceComparator_sentiment_analysis()\n",
    "test_model(sentiment_comparator, \"sentiment_comparator\")\n",
    "\n",
    "sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "\n",
    "ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "    llama_version=\"llama3.1\",\n",
    "    modelfile_system= sys_prompt,\n",
    "    temperature=0.4\n",
    ")\n",
    "test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 33.11102080345154 seconds\n"
     ]
    }
   ],
   "source": [
    "sentiment_comparator = SentenceComparator_sentiment_analysis()\n",
    "test_model(sentiment_comparator, \"sentiment_comparator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "# For sbert_similarity_log.xlsx\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_df = excel_to_df(\"sentiment_comparator_log.xlsx\", \"Results\")\n",
    "sentiment_df[\"Result\"] = sentiment_df[\"Result\"].astype(int)\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:Cüaıbzkfakı .23456789]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], sentiment_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "                        axis=1, \n",
    "                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_comparator\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "             \n",
    "#all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>bert_cos_sim</th>\n",
       "      <th>sbert_cos_sim</th>\n",
       "      <th>nli_model</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>word2vec_sim</th>\n",
       "      <th>sentiment_comparator</th>\n",
       "      <th>ollama_model_llama3.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi marketten alışveriş yapmıştır ve kasa...</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kişisi kasada ödeme yapmadan marketten çıkmı...</td>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C kasaya ödeme yapması gerekirken yapmamıştır.</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C'nin dikkat ve özen yükümlülüğüne aykırı davr...</td>\n",
       "      <td>C markete girdi ve sonra ödeme yapmadan çıktı.</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>C'nin kasada ödeme yapmadığı iddiası gerçeği y...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>C'nin ödeme yapmaması kasıtlı olarak değerlend...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...</td>\n",
       "      <td>C kişisi, ödeme yapmadan ayrılmayı bir hata ol...</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence1  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "1    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "2    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "3    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "4    C'nin dikkat ve özen yükümlülüğüne aykırı davr...   \n",
       "..                                                 ...   \n",
       "699  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "700  C'nin kasada ödeme yapmadığı iddiası gerçeği y...   \n",
       "701  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "702  C'nin ödeme yapmaması kasıtlı olarak değerlend...   \n",
       "703  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...   \n",
       "\n",
       "                                             Sentence2  bert_cos_sim  \\\n",
       "0    C'nin dikkat ve özen yükümlülüğüne aykırı davr...      0.964577   \n",
       "1    C kişisi marketten alışveriş yapmıştır ve kasa...      0.636648   \n",
       "2    C kişisi kasada ödeme yapmadan marketten çıkmı...      0.625543   \n",
       "3       C kasaya ödeme yapması gerekirken yapmamıştır.      0.657169   \n",
       "4       C markete girdi ve sonra ödeme yapmadan çıktı.      0.587133   \n",
       "..                                                 ...           ...   \n",
       "699  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...      0.697819   \n",
       "700  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...      0.718892   \n",
       "701  C'nin dikkatsizliği yüzünden ödemeyi unutmuş o...      0.703241   \n",
       "702  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...      0.764747   \n",
       "703  C kişisi, ödeme yapmadan ayrılmayı bir hata ol...      0.766007   \n",
       "\n",
       "     sbert_cos_sim  nli_model  semantic_similarity  word2vec_sim  \\\n",
       "0           0.9383   0.769148               0.9611      0.946022   \n",
       "1           0.1996   0.589680               0.7214      0.398693   \n",
       "2           0.1646   0.747958               0.7042      0.312992   \n",
       "3           0.4852   0.710494               0.6675      0.391442   \n",
       "4           0.2732   0.737624               0.6462      0.396546   \n",
       "..             ...        ...                  ...           ...   \n",
       "699         0.7362   0.844054               0.6441      0.515694   \n",
       "700         0.6233   0.514596               0.6645      0.496692   \n",
       "701         0.6767   0.601376               0.7230      0.457836   \n",
       "702         0.6354   0.867559               0.6490      0.658151   \n",
       "703         0.8142   0.532981               0.5583      0.408180   \n",
       "\n",
       "     sentiment_comparator  ollama_model_llama3.1  \n",
       "0                       0                      0  \n",
       "1                       1                      1  \n",
       "2                       1                      1  \n",
       "3                       0                      1  \n",
       "4                       1                      0  \n",
       "..                    ...                    ...  \n",
       "699                     0                      1  \n",
       "700                     0                      1  \n",
       "701                     1                      0  \n",
       "702                     1                      0  \n",
       "703                     1                      1  \n",
       "\n",
       "[704 rows x 9 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Normalize lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1\"]\n",
    "X = all_results_parameters#.drop(\"ollama_model_llama3.1\", axis=1)\n",
    "\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg, Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Reg, Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "True Positive:  40\n",
      "True Negative:  101\n",
      "False Positive:  0\n",
      "False Negative:  0\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_conf_matrix(y_test, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    print(\"True Positive: \", conf_matrix[1][1])\n",
    "    print(\"True Negative: \", conf_matrix[0][0])\n",
    "    print(\"False Positive: \", conf_matrix[0][1])\n",
    "    print(\"False Negative: \", conf_matrix[1][0])\n",
    "    \n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8853 - loss: 0.4042\n",
      "Epoch 2/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0239\n",
      "Epoch 3/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 5/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0012 \n",
      "Epoch 6/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 7.7985e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.3936e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.9365e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.9711e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.3233e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.9054e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.6595e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.3191e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.0987e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.4551e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.3818e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 7.2266e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.4265e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.4532e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.1389e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.2741e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.7471e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.2230e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.9653e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.8814e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.4592e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.3815e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.9772e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.8848e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.6657e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.5916e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.3278e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.2690e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.1511e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.0884e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.5497e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.9704e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.2549e-06\n",
      "Epoch 39/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.0165e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 7.0439e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.6067e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.4739e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.8939e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.0365e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 5.1896e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.4569e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.0573e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.8164e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.7844e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.3764e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25e38cda0c0>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 27s]\n",
      "val_accuracy: 0.7281323671340942\n",
      "\n",
      "Best val_accuracy So Far: 0.7281323870023092\n",
      "Total elapsed time: 00h 02m 04s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def keras_model_tuner(hp):\n",
    "    \n",
    "    hidden_layer_num = hp.Int('hidden_layer_num', min_value=1, max_value=5, step=1)\n",
    "    layer_unit = hp.Int('layer_unit', min_value=16, max_value=128, step=16)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(layer_unit, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(hidden_layer_num):\n",
    "        model.add(layers.Dense(layer_unit, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='keras_tuner'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, batch_size=5, validation_data=(X_test, y_test))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_columns(df, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    This function adds the Confidence, Real Value, and Prediction columns to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    if type(df) != pd.DataFrame:\n",
    "        df = pd.DataFrame(df)\n",
    "    df[\"Confidence\"] = 1.0\n",
    "    df[\"Real Value\"] = 5\n",
    "    df[\"Prediction\"] = 5\n",
    "    df[\"Accuracy\"] = 5\n",
    "    #print(df.head())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, val in enumerate(y_pred):\n",
    "        \n",
    "        real =  y_test.iloc[index]\n",
    "        pred = 0 if val < 0.5 else 1\n",
    "        confidence = (val-0.5)*2 if pred == 1 else (0.5-val)*2\n",
    "        \n",
    "        df[\"Confidence\"][index] = confidence\n",
    "        df[\"Real Value\"][index] = real\n",
    "        df[\"Prediction\"][index] = pred\n",
    "        df[\"Accuracy\"][index] = 1 if real == pred else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_accuracy(x,y,predictor):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predictor.\n",
    "\n",
    "    Args:\n",
    "        x: A pandas dataframe.\n",
    "        y: A pandas dataframe.\n",
    "        predictor: A predictor model.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: A float.\n",
    "    \"\"\"\n",
    "    y_pred = predictor.predict(x)\n",
    "    y_pred = [1 if val > 0.5 else 0 for val in y_pred]\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test Accuracy:  0.6595744680851063\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step\n",
      "Train Accuracy:  0.8312611012433393\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step\n",
      "Entire Accuracy:  0.796875\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Test Accuracy: \", calculate_accuracy(X_test, y_test, model))\n",
    "# Train\n",
    "print(\"Train Accuracy: \", calculate_accuracy(X_train, y_train, model))\n",
    "# Entire\n",
    "print(\"Entire Accuracy: \", calculate_accuracy(normalized_X, y, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    43\n",
       "0     6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "keras_predict_df_test = pd.DataFrame(model.predict(X_test), columns=[\"Prediction\"])\n",
    "\n",
    "result_df_test = add_output_columns(X_test, y_test, keras_predict_df_test[\"Prediction\"])\n",
    "\n",
    "result_df_test_high_confidence = result_df_test[result_df_test[\"Confidence\"] > 0.8]\n",
    "result_df_test_high_confidence[\"Accuracy\"].value_counts()\n",
    "\n",
    "#result_df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.36666666666666664\n",
      "Recall:  0.275\n",
      "True Positive:  11\n",
      "True Negative:  82\n",
      "False Positive:  19\n",
      "False Negative:  29\n"
     ]
    }
   ],
   "source": [
    "print_conf_matrix(result_df_test[\"Real Value\"], result_df_te    st[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step\n",
      "Error rate: % 2.830188679245283       Number of high confidence predictions:  212\n",
      "Number of faults:  143    Faults from high confidence predictions:  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.897630</td>\n",
       "      <td>2.112313</td>\n",
       "      <td>0.880965</td>\n",
       "      <td>1.959885</td>\n",
       "      <td>3.242402</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.241112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.108159</td>\n",
       "      <td>-2.200838</td>\n",
       "      <td>0.740699</td>\n",
       "      <td>0.216613</td>\n",
       "      <td>-1.578397</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.314078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.300269</td>\n",
       "      <td>0.161165</td>\n",
       "      <td>-0.760111</td>\n",
       "      <td>0.215256</td>\n",
       "      <td>-0.400379</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.289999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.798547</td>\n",
       "      <td>0.189039</td>\n",
       "      <td>-1.441552</td>\n",
       "      <td>0.094468</td>\n",
       "      <td>-0.348146</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.381435</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.296473</td>\n",
       "      <td>-1.097046</td>\n",
       "      <td>-0.011958</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>-1.609117</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.166558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.224504</td>\n",
       "      <td>0.933820</td>\n",
       "      <td>-0.979147</td>\n",
       "      <td>-0.458575</td>\n",
       "      <td>-0.826684</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.376227</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.484438</td>\n",
       "      <td>0.718762</td>\n",
       "      <td>-0.150209</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.855119</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>-0.434589</td>\n",
       "      <td>0.136079</td>\n",
       "      <td>-1.600044</td>\n",
       "      <td>-0.052105</td>\n",
       "      <td>-0.459445</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>0.681903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.425972</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.498079</td>\n",
       "      <td>0.398032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5  Confidence  \\\n",
       "0    2.897630  2.112313  0.880965  1.959885  3.242402 -1.498079    0.241112   \n",
       "2   -1.108159 -2.200838  0.740699  0.216613 -1.578397  0.667522    0.314078   \n",
       "7   -0.300269  0.161165 -0.760111  0.215256 -0.400379  0.667522    0.289999   \n",
       "13  -0.798547  0.189039 -1.441552  0.094468 -0.348146 -1.498079    0.381435   \n",
       "15  -0.296473 -1.097046 -0.011958  0.225434 -1.609117  0.667522    0.166558   \n",
       "..        ...       ...       ...       ...       ...       ...         ...   \n",
       "681  0.224504  0.933820 -0.979147 -0.458575 -0.826684  0.667522    0.376227   \n",
       "686  1.391350  1.021343 -0.484438  0.718762 -0.150209  0.667522    0.855119   \n",
       "691 -0.434589  0.136079 -1.600044 -0.052105 -0.459445  0.667522    0.681903   \n",
       "699 -0.254197  0.985665  1.376823 -0.191214 -0.034734 -1.498079    0.425972   \n",
       "700 -0.005210  0.356280 -0.804090 -0.052784 -0.179440 -1.498079    0.398032   \n",
       "\n",
       "     Real Value  Prediction  Accuracy  \n",
       "0             0           1         0  \n",
       "2             1           0         0  \n",
       "7             1           0         0  \n",
       "13            1           0         0  \n",
       "15            1           0         0  \n",
       "..          ...         ...       ...  \n",
       "681           0           1         0  \n",
       "686           1           0         0  \n",
       "691           0           1         0  \n",
       "699           1           0         0  \n",
       "700           1           0         0  \n",
       "\n",
       "[143 rows x 10 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTIRE DATASET\n",
    "# Predicts\n",
    "keras_predict_df = pd.DataFrame(model.predict(normalized_X), columns=[\"Prediction\"])\n",
    "\n",
    "#sort the dataframe by ID\n",
    "result_df = add_output_columns( \n",
    "    df = normalized_X,\n",
    "    y_test = y,\n",
    "    y_pred = keras_predict_df[\"Prediction\"]\n",
    ")\n",
    "faults = result_df[ result_df[\"Accuracy\"] == 0 ]\n",
    "\n",
    "# Accuracy counts of the model where the confidence is greater than 0.8\n",
    "high_acc = result_df[result_df[\"Confidence\"] > 0.8][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults.shape[0], \"   Faults from high confidence predictions: \", faults[ faults[\"Confidence\"] > 0.8 ].shape[0])\n",
    "faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.124814e-16</td>\n",
       "      <td>1.009294e-16</td>\n",
       "      <td>7.065056e-17</td>\n",
       "      <td>2.573699e-16</td>\n",
       "      <td>-5.450186e-16</td>\n",
       "      <td>-3.027881e-17</td>\n",
       "      <td>0.553665</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.220170</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>0.315346</td>\n",
       "      <td>0.459078</td>\n",
       "      <td>0.414656</td>\n",
       "      <td>0.402611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.375819e+00</td>\n",
       "      <td>-2.617827e+00</td>\n",
       "      <td>-1.976928e+00</td>\n",
       "      <td>-3.428721e+00</td>\n",
       "      <td>-2.103059e+00</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.171947e-01</td>\n",
       "      <td>-6.411746e-01</td>\n",
       "      <td>-8.397758e-01</td>\n",
       "      <td>-5.196470e-01</td>\n",
       "      <td>-7.021904e-01</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.272107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.630704e-02</td>\n",
       "      <td>-1.583170e-02</td>\n",
       "      <td>-1.698798e-02</td>\n",
       "      <td>2.159341e-01</td>\n",
       "      <td>-1.723928e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.570579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.399473e-01</td>\n",
       "      <td>7.538961e-01</td>\n",
       "      <td>8.097366e-01</td>\n",
       "      <td>6.919582e-01</td>\n",
       "      <td>6.037603e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.841642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.316165e+00</td>\n",
       "      <td>2.456273e+00</td>\n",
       "      <td>2.317426e+00</td>\n",
       "      <td>2.223853e+00</td>\n",
       "      <td>3.653464e+00</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02   \n",
       "mean   8.124814e-16  1.009294e-16  7.065056e-17  2.573699e-16 -5.450186e-16   \n",
       "std    1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00   \n",
       "min   -3.375819e+00 -2.617827e+00 -1.976928e+00 -3.428721e+00 -2.103059e+00   \n",
       "25%   -6.171947e-01 -6.411746e-01 -8.397758e-01 -5.196470e-01 -7.021904e-01   \n",
       "50%    8.630704e-02 -1.583170e-02 -1.698798e-02  2.159341e-01 -1.723928e-01   \n",
       "75%    6.399473e-01  7.538961e-01  8.097366e-01  6.919582e-01  6.037603e-01   \n",
       "max    3.316165e+00  2.456273e+00  2.317426e+00  2.223853e+00  3.653464e+00   \n",
       "\n",
       "                  5  Confidence  Real Value  Prediction    Accuracy  \n",
       "count  7.040000e+02  704.000000  704.000000  704.000000  704.000000  \n",
       "mean  -3.027881e-17    0.553665    0.301136    0.220170    0.796875  \n",
       "std    1.000711e+00    0.315346    0.459078    0.414656    0.402611  \n",
       "min   -1.498079e+00    0.000682    0.000000    0.000000    0.000000  \n",
       "25%   -1.498079e+00    0.272107    0.000000    0.000000    1.000000  \n",
       "50%    6.675217e-01    0.570579    0.000000    0.000000    1.000000  \n",
       "75%    6.675217e-01    0.841642    1.000000    0.000000    1.000000  \n",
       "max    6.675217e-01    0.999890    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    211\n",
       "0      3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence = result_df[ result_df[\"Confidence\"] > 0.8 ]\n",
    "\n",
    "high_confidence[\"Accuracy\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_results.iloc[0][\"Sentence2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim</th>\n",
       "      <th>sbert_cos_sim</th>\n",
       "      <th>nli_model</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>word2vec_sim</th>\n",
       "      <th>sentiment_comparator</th>\n",
       "      <th>ollama_model_llama3.1</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997174</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995486</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997174</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.659807</td>\n",
       "      <td>0.3453</td>\n",
       "      <td>0.622968</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.345494</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.612294</td>\n",
       "      <td>0.5021</td>\n",
       "      <td>0.639196</td>\n",
       "      <td>0.3456</td>\n",
       "      <td>0.371388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.675849</td>\n",
       "      <td>0.4192</td>\n",
       "      <td>0.610121</td>\n",
       "      <td>0.6945</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.864439</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>0.741523</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.696818</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.518143</td>\n",
       "      <td>0.3757</td>\n",
       "      <td>0.427049</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>0.364210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.539610</td>\n",
       "      <td>0.3662</td>\n",
       "      <td>0.540047</td>\n",
       "      <td>0.4548</td>\n",
       "      <td>0.374539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.719682</td>\n",
       "      <td>0.7479</td>\n",
       "      <td>0.705915</td>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.609831</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.899933</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_cos_sim  sbert_cos_sim  nli_model  semantic_similarity  \\\n",
       "0        0.964577         0.9383   0.769148               0.9611   \n",
       "3        0.657169         0.4852   0.710494               0.6675   \n",
       "8        0.964577         0.9383   0.769148               0.9611   \n",
       "9        0.659807         0.3453   0.622968               0.7300   \n",
       "10       0.612294         0.5021   0.639196               0.3456   \n",
       "..            ...            ...        ...                  ...   \n",
       "234      0.675849         0.4192   0.610121               0.6945   \n",
       "235      0.864439         0.9119   0.741523               0.7778   \n",
       "236      0.518143         0.3757   0.427049               0.4238   \n",
       "237      0.539610         0.3662   0.540047               0.4548   \n",
       "238      0.719682         0.7479   0.705915               0.6742   \n",
       "\n",
       "     word2vec_sim  sentiment_comparator  ollama_model_llama3.1  Confidence  \\\n",
       "0        0.946022                     0                      1    0.997174   \n",
       "3        0.391442                     0                      1    0.995486   \n",
       "8        0.946022                     0                      1    0.997174   \n",
       "9        0.345494                     0                      1    0.998011   \n",
       "10       0.371388                     0                      0    1.000000   \n",
       "..            ...                   ...                    ...         ...   \n",
       "234      0.298700                     0                      1    0.999618   \n",
       "235      0.696818                     0                      1    0.999989   \n",
       "236      0.364210                     0                      0    0.999995   \n",
       "237      0.374539                     0                      0    0.999997   \n",
       "238      0.609831                     0                      1    0.899933   \n",
       "\n",
       "     Real Value  Prediction  Accuracy  \n",
       "0             1           1         1  \n",
       "3             1           1         1  \n",
       "8             1           1         1  \n",
       "9             1           1         1  \n",
       "10            0           0         1  \n",
       "..          ...         ...       ...  \n",
       "234           1           1         1  \n",
       "235           1           1         1  \n",
       "236           0           0         1  \n",
       "237           0           0         1  \n",
       "238           1           1         1  \n",
       "\n",
       "[126 rows x 11 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatted = pd.concat([all_results_parameters, result_df.iloc[:,-4:]], axis=1)\n",
    "concatted[ concatted[\"sentiment_comparator\"] == 0 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
