{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a correct answer for the question in the sample.txt file\n",
    "correct_answer = {\n",
    "  \"Suçun_Temel_Unsurları\": {\n",
    "    \"Fail\": \"C\",\n",
    "    \"Mağdur\": \"Market (Mülkiyet hakkı ihlal edilen taraf)\",\n",
    "    \"Konu\": \"Marketten alınan ürünler\",\n",
    "    \"Fiil\": \"Ürünlerin kasada ödeme yapılmaksızın dışarı çıkarılması\",\n",
    "    \"Netice\": {\n",
    "      \"Marketin_malvarlığında_azalma\": True,\n",
    "      \"Nedensellik_Bağı\": \"C’nin fiili ile netice arasında nedensellik bağı mevcuttur.\",\n",
    "      \"Objektif_İsnadiyet\": \"Fiil, neticeyi doğuracak şekilde gerçekleştirilmiştir.\"\n",
    "    }\n",
    "  },\n",
    "  \"Manevi_Unsur\": \"C, bu eylemi kasten mi gerçekleştirmiştir? C’nin kasıtlı olup olmadığı, marketten çıkarken ödeme yapmadığını fark etmesiyle bağlantılıdır. C, ödeme yapmayı unuttuğunu iddia etmektedir, bu nedenle kast unsuru tartışmalı olabilir.\",\n",
    "  \"Hukuka_Aykırılık_Unsuru\": \"Hukuka aykırılık unsuru açısından, C’nin eylemi hırsızlık suçu açısından değerlendirildiğinde, hukuka aykırı bir durum mevcuttur. Ancak, manevi unsurdaki belirsizlik bu hususu etkileyebilir.\",\n",
    "  \"Suçun_Nitelikli_Unsurları\": \"Bu olayda nitelikli bir unsur yoktur.\",\n",
    "  \"Kusurluluk\": \"C’nin kusur durumu, ödeme yapmayı unuttuğunu iddia etmesi sebebiyle, kusurluluğu tartışmaya açabilir.\",\n",
    "  \"Yaptırımın_Uygulanması_İçin_Gereken_Veya_Yaptırım_Uygulanmasını_Engelleyen_Özel_Koşullar\": \"C’nin ödeme yapmayı unuttuğunu iddia etmesi, suçun manevi unsurunu etkileyebilir ve bu durum yaptırım uygulanmasını engelleyebilir.\",\n",
    "  \"Suçun_Özel_Görünüş_Biçimleri\": {\n",
    "    \"Teşebbüs\": \"Eylem tamamlanmıştır, teşebbüs söz konusu değildir.\",\n",
    "    \"İştirak\": \"Olayda iştirak eden başka bir kişi yoktur.\",\n",
    "    \"İçtima\": \"Tek bir suç tipi söz konusudur.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "test_sentences = [\"Havayı kirletmek, çevreyi kirletmek anlamına gelir.\", \"Hava kirliliği, çevre kirliliğinin bir parçasıdır.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "Cosine similarity (%) : 0.0\n",
      "0.04399732626783181\n",
      "67.0\n"
     ]
    }
   ],
   "source": [
    "# THRASH CODE\n",
    "\n",
    "# It is not efficient to compare two sentences in Turkish.\n",
    "# Word count based cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "def sentence_to_word_dict(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and returns a dictionary with words as keys and their counts as values.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A string.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "sentence_1 = correct_answer[\"Kusurluluk\"]\n",
    "sentence_2 = \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\"\n",
    "\n",
    "dict_1 = sentence_to_word_dict(sentence_1)\n",
    "dict_2 = sentence_to_word_dict(sentence_2)\n",
    "\n",
    "word_space = np.unique(list(dict_1.keys()) + list(dict_2.keys()))\n",
    "\n",
    "# One-hot encoding\n",
    "binary_vector_1 = [1 if word in dict_1 else 0 for word in word_space]\n",
    "binary_vector_2 = [1 if word in dict_2 else 0 for word in word_space]\n",
    "\n",
    "print(binary_vector_1)\n",
    "print(binary_vector_2)\n",
    "\n",
    "cosine_similarity = np.dot(binary_vector_1, binary_vector_2) / (np.linalg.norm(binary_vector_1) * np.linalg.norm(binary_vector_2))\n",
    "print(\"Cosine similarity (%) :\", cosine_similarity * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF based cosine similarity\n",
    "# Not an efficient way\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [sentence_1,sentence_2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1) + 1, len(s2) + 1\n",
    "    dp = np.zeros((len_s1, len_s2))\n",
    "    for i in range(len_s1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_s2):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1):\n",
    "        for j in range(1, len_s2):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "print(levenshtein_distance(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime          Benzerlik Skoru\n",
      "------------  -----------------\n",
      "ateşkes                0.823591\n",
      "uzlaşma                0.794623\n",
      "ittifak                0.774118\n",
      "antlaşma               0.773479\n",
      "müzakereler            0.762028\n",
      "barışı                 0.759955\n",
      "antlaşmayı             0.749817\n",
      "müzakereleri           0.747546\n",
      "mütareke               0.727691\n",
      "müttefiklik            0.717039\n",
      "\n",
      "Word Vector: [ 0.1495104  -1.4914255  -0.50925356 -0.9685314   2.1551907   0.10626572\n",
      "  0.4027821   1.0281931   0.41044936 -1.1525857  -0.0205108   1.0924134\n",
      " -1.9218051   1.3797586  -0.63527036 -0.38006008 -0.6512365  -0.96633595\n",
      "  1.1853794   0.7896848  -0.03258616  0.8834496  -1.6903982   0.9449919\n",
      "  0.6057014   0.59224516 -1.0036951   2.0536163  -2.1637177  -0.65654767\n",
      "  1.0522053   0.11371119  1.1112392  -0.43076926  0.13155091 -1.1467836\n",
      " -0.8198967   1.1959015  -0.5887494  -1.0079744  -0.25314665  0.5018188\n",
      " -0.76072204 -0.30214065 -0.13227591  0.6748753   0.7053673   1.5428567\n",
      " -0.08245109  0.76109725 -0.6433578  -1.2249595  -0.8891968  -1.5681715\n",
      " -0.4665735   0.23464409  0.9810163   0.7976722   1.6759675   0.8835468\n",
      "  0.21888028  1.7479744   0.10691787  0.25808322  0.4888047   1.4980937\n",
      "  1.0098569  -0.28364947 -0.6473856   1.0754474  -0.7372982   0.2214374\n",
      " -0.68053114 -0.97661483  1.8531004   0.90171987  1.5329516   1.2696122\n",
      "  0.5022536  -0.7828172  -0.51381963  0.64165884  0.40742102 -1.4720764\n",
      " -0.76259804  1.090137   -0.79868716  0.5095711  -0.22643751  1.2640641\n",
      " -1.4126805  -0.7636473  -3.278555    0.06060509 -0.7998284  -0.8611428\n",
      " -1.5374006   1.47675     0.18929918  1.5398192 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the model Turkish word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"utils/word2vec/w2v_.model\")\n",
    "print(tabulate(model.wv.most_similar(\"barış\"), headers=[\"Kelime\", \"Benzerlik Skoru\"]))\n",
    "print(\"\\nWord Vector:\", model.wv.get_vector(\"umut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_Word2Vec:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.generate_model(model_name)\n",
    "\n",
    "    def generate_model(self, model_name):\n",
    "        return Word2Vec.load(model_name)\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    def extract_key_features(self, words_1, words_2):\n",
    "        \n",
    "        # Nested iteration to compare each word in the sentences\n",
    "        #Dict: {word_1:{word_comp1:score, word_comp2:score, ...}, word_2:{word_comp1:score, word_comp2:score, ...}}\n",
    "        searched_pairs = [];similarity_dict = {}\n",
    "\n",
    "        for word_1 in words_1:\n",
    "            if word_1 not in similarity_dict:\n",
    "                similarity_dict.update({word_1:{}})\n",
    "            for word_2 in words_2:\n",
    "                if (word_1, word_2) in searched_pairs or (word_2, word_1) in searched_pairs:\n",
    "                    continue\n",
    "                try:\n",
    "                    searched_pairs.append((word_1, word_2))\n",
    "                    # Calculate the similarity between the words\n",
    "                    similarity_dict[word_1].update({word_2:self.model.wv.similarity(word_1, word_2)})\n",
    "                except:\n",
    "                    pass\n",
    "        return similarity_dict\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Clean the sentences and split them into words\n",
    "        sentence_1 = self.clean_sentence(sentence_1); words_1 = sentence_1.split()\n",
    "        sentence_2 = self.clean_sentence(sentence_2); words_2 = sentence_2.split()\n",
    "\n",
    "        # Extract key features\n",
    "        similarity_dict = self.extract_key_features(words_1, words_2)\n",
    "            \n",
    "        # Extract informations from the similarity_dict\n",
    "        key_features = []\n",
    "        for key, value in similarity_dict.items():\n",
    "            if len(value) > 0:\n",
    "                # Sort and get the best match\n",
    "                sorted_dict = sorted(value.items(), key=lambda x:x[1], reverse=True)\n",
    "                max_score = sorted_dict[0][1]\n",
    "                best_key = sorted_dict[0][0]\n",
    "\n",
    "                key_features.append({\"key\":key, \"score\":max_score,\"best_match\":best_key})\n",
    "\n",
    "        # Calculate the average score\n",
    "        avg_score = sum([x[\"score\"] for x in key_features]) / len(key_features)\n",
    "\n",
    "        return avg_score#,key_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_comparator = SentenceComparator_Word2Vec(\"word2vec/w2v_.model\")\n",
    "#avg_score,sim_dict = word2vec_comparator.compare_sentences(test_sentences[0], test_sentences[1])\n",
    "#\n",
    "#print(tabulate([x.values() for x in sim_dict], headers=[\"Kelime\", \"En Benzer Kelime, Benzerlik Skoru\"]))\n",
    "#print(\"\\nAverage Score: \", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding with turkish trained word2vec\n",
    "#def calculate_vector_of_senetence(sentence):\n",
    "#    sentence_vector = np.zeros(100)\n",
    "#    for word in sentence.split():\n",
    "#        try:\n",
    "#            sentence_vector += model.wv[word]\n",
    "#        except:\n",
    "#            pass\n",
    "#    return sentence_vector\n",
    "#\n",
    "#def cosine_similarity(v1, v2):\n",
    "#    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "#\n",
    "#def measure_similarity(sentence_1, sentence_2):\n",
    "#    v1 = calculate_vector_of_senetence(sentence_1)\n",
    "#    v2 = calculate_vector_of_senetence(sentence_2)\n",
    "#    cos_sim = cosine_similarity(v1, v2)\n",
    "#\n",
    "#    print(\"\\nCosine similarity: \", cos_sim)\n",
    "#\n",
    "#\n",
    "#measure_similarity(\n",
    "#    test_sentences[0],\n",
    "#    test_sentences[1]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelfile System Prompt: \n",
    "\n",
    "Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {\"cümleler\":<cümleler>,\"analiz/neden\":<kısaca>,\"değerlendirme\": <>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class SentenceComparator_Ollama:\n",
    "    def __init__(self,modelfile_system, llama_version=\"llama3.1\", temperature=0.4):\n",
    "        self.system_prompt = modelfile_system\n",
    "        self.llama_version = llama_version\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.role_messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: C kişisi kasada ödeme yapmadan marketten çıkmıştır, İkinci cümle: C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 0\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'İlk cümle: Havalar güzelken denize gitmek çok iyi olur., İkinci cümle: Bir insan ev almadan önce araba parası biriktirmeli.'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': \"değerlendirme: 1\"\n",
    "                }\n",
    "            ]\n",
    "        self.generate_model()\n",
    "    \n",
    "    def generate_model(self):\n",
    "        modelfile = f'''\n",
    "        FROM {self.llama_version}\n",
    "        SYSTEM {self.system_prompt} \n",
    "        PARAMETER temperature {self.temperature}\n",
    "        '''\n",
    "        ollama.create(model=f'MAHDAN_{self.llama_version}', modelfile=modelfile)\n",
    "\n",
    "    def calculate_similarity(self,sentence_1,sentence_2):\n",
    "        sentence_in = 'İlk Cümle: ' + sentence_1 + ' ,\\n İkinci Cümle: ' + sentence_2        \n",
    "\n",
    "        # Concat messages and sentence\n",
    "        messages_temp = self.role_messages.copy()\n",
    "\n",
    "        messages_temp.append({\n",
    "            'role': 'user',\n",
    "            'content': sentence_in\n",
    "        })\n",
    "\n",
    "        response = ollama.chat(model=f'MAHDAN_{self.llama_version}', messages= messages_temp)\n",
    "\n",
    "        return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )\n",
    "#\n",
    "#\n",
    "#ollama_model = SentenceComparator_Ollama(\n",
    "#    llama_version=\"mistral\",\n",
    "#    modelfile_system=\"Sen bir metin kazıcı algoritmasın. Cümleleri sadece anlamsal olarak değerlendir. İSTENEN DÖNÜŞ FORMATI: {'cümleler':<cümleler>,'analiz/neden':<kısaca>,'değerlendirme': <>}\"\n",
    "#)\n",
    "#print(ollama_model.calculate_similarity( test_sentences[0],test_sentences[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "# Class of Semantic Similarity\n",
    "class SentenceComparator_semantic:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences)\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#model_name = 'bert-base-multilingual-cased'\n",
    "# Calculate cosine similarity between two sentences with BERT\n",
    "class SentenceComparator_bert_cosine:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return BertModel.from_pretrained(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentence):\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings1 = self.get_embeddings(sentence_1)\n",
    "        embeddings2 = self.get_embeddings(sentence_2)\n",
    "        \n",
    "        return cosine_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "#model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# Class of SBERT Similarity\n",
    "class SentenceComparator_SBERT:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "        \n",
    "    def generate_model(self):\n",
    "        return SentenceTransformer(self.model_name)\n",
    "    \n",
    "    def get_embeddings(self, sentences):\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        embeddings = self.get_embeddings([sentence_1, sentence_2])\n",
    "        return util.pytorch_cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çalışmak için *İnternet* gerektiriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# NLI pipeline oluşturma (Türkçe destekleyen model kullanılabilir)\n",
    "#nli_model = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "class SentenceComparator_NLI:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = self.generate_model()\n",
    "\n",
    "    def generate_model(self):\n",
    "        return pipeline(\"text-classification\", model=self.model_name)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        input_sentence = sentence_1 + ' [SEP] ' + sentence_2\n",
    "        result = self.model(input_sentence)\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceComparator_sentiment_analysis:\n",
    "    def __init__(self):\n",
    "        model_id = \"saribasmetehan/bert-base-turkish-sentiment-analysis\"\n",
    "        self.classifer = pipeline(\"text-classification\",model = model_id)\n",
    "    \n",
    "    def clearify_sentence(self, sentence):\n",
    "        return sentence.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        sentence_1 = self.clearify_sentence(sentence_1)\n",
    "        sentence_2 = self.clearify_sentence(sentence_2)\n",
    "\n",
    "        pred1 = self.classifer(sentence_1)\n",
    "        pred2 = self.classifer(sentence_2)\n",
    "\n",
    "        #is_similar = pred1[0][\"label\"] == pred2[0][\"label\"]\n",
    "        \n",
    "        return (pred1[0][\"label\"], pred2[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_log(log_name, additional_info=\"\"):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'w') as f:\n",
    "        f.write(f\"Log file created. ({log_name})\\nAdditional Info: {additional_info}\\n\")\n",
    "\n",
    "def append_to_log(log_name, message):\n",
    "    log_name = \"log/\" + log_name\n",
    "    with open(log_name, 'a') as f:\n",
    "        f.write(\"\\n\" + message)\n",
    "\n",
    "def create_excel_file(file_name, sheet_name, data):    \n",
    "    file_name = \"log/\" + file_name\n",
    "    # if exist, remove the file\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def append_to_excel(file_name, sheet_name, data):\n",
    "    file_name = \"log/\" + file_name\n",
    "    # Data is one row\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    excel_df = pd.concat([excel_df, pd.DataFrame([data])], ignore_index=True)\n",
    "    excel_df.to_excel(file_name, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZEMBEREK https://github.com/ahmetaa/zemberek-nlp bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import jpype\n",
    "import os\n",
    "import atexit\n",
    "\n",
    "print(jpype.isJVMStarted())\n",
    "\n",
    "class SentenceComparator_jpype:\n",
    "    def __init__(self):\n",
    "        # JVM'i başlat\n",
    "        if not jpype.isJVMStarted():\n",
    "            jpype.startJVM(\"C:/Program Files/Java/jdk-22/bin/server/jvm.dll\", \n",
    "                           \"-Djava.class.path=utils/zemberek-full.jar\")\n",
    "        \n",
    "        # Zemberek sınıfını başlat\n",
    "        TurkishMorphology = jpype.JClass('zemberek.morphology.TurkishMorphology')\n",
    "        self.morphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "        # JVM'i kapatmayı atexit ile garanti altına al\n",
    "        atexit.register(self.shutdown_jvm)\n",
    "    \n",
    "    def calculate_similarity(self, sentence_1, sentence_2):\n",
    "        # Cümlelerin analizini yap\n",
    "        analysis1 = self.morphology.analyzeSentence(sentence_1)\n",
    "        analysis2 = self.morphology.analyzeSentence(sentence_2)\n",
    "        return analysis1, analysis2\n",
    "    \n",
    "    def shutdown_jvm(self):\n",
    "        # JVM'i kapat\n",
    "        if jpype.isJVMStarted():\n",
    "            jpype.shutdownJVM()\n",
    "\n",
    "# Örnek kullanım\n",
    "\n",
    "#jvm = SentenceComparator_jpype()\n",
    "#test_sentence = \"Bu güzel bir gün.\"\n",
    "#print(jvm.calculate_similarity(\"keşke hemen şurada ölsen ve gebersen.\", test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_model(model, model_name):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    computation_count = 0\n",
    "\n",
    "    # Define the sentences to compare\n",
    "    sentences = [\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendirilebilir.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi marketten alışveriş yapmıştır ve kasada ödeme yapmadan çıkmıştır.\",\n",
    "        \"C kişisi kasada ödeme yapmadan marketten çıkmıştır.\",\n",
    "        \"C kasaya ödeme yapması gerekirken yapmamıştır.\",\n",
    "        \"C markete girdi ve sonra ödeme yapmadan çıktı.\",\n",
    "        \"Şahıs aldığı ürünleri parasını ödemeden çıkmıştır.\",\n",
    "        \"C kişisi ödeme yapmayı unutarak marketten çıkmıştır.\",\n",
    "        \"C kişisi kesin unutkan birisidir ve ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi hırsızdır ve hırsızlık suçu işlediği için bu durudman şüphe bile edilemez.\",\n",
    "        \"C'nin dikkat ve özen yükümlülüğüne aykırı davranmış olması nedeniyle kusurlu olduğu değerlendrilemez.\",\n",
    "        \"C kişisi ödeme yapmadı sonra da marketten çıkarken ödemeyi unuttu.\",\n",
    "        \"C kişisi kötü bir insan.\",\n",
    "        \"Ben C kişisinin kötü birisi olduğunu biliyorum.\",\n",
    "        \"C kişisi iyi bir insan değil.\",\n",
    "        \"Kötü bir insan olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"Kusurlu olan C kişisi, ödeme yapmayı unuttuğunu iddia etmektedir.\",\n",
    "        \"C kişisi marketten çıkarken ödeme yapmayı unutmuştur.\",\n",
    "        \"C kişisi marketten satın aldığı ürünleri kasada ödeme yapmadan çıkarmıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kameralarıyla doğrulanmıştır.\",\n",
    "        \"C kişisinin kasada ödeme yapmadan çıkması bilinçli bir eylem olarak değerlendirilebilir.\",\n",
    "        \"C kasada ödeme yapmadığı için sorumlu tutulmalıdır.\",\n",
    "        \"C kişisinin ödeme yapmadığına dair hiçbir kanıt bulunmamaktadır.\",\n",
    "        \"Market çalışanları, C'nin ödeme yapmadığını fark etmiştir.\",\n",
    "        \"C kişisi ödeme yapmayı unuttuğunu savunmaktadır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası asılsızdır.\",\n",
    "        \"C'nin kasada ödeme yapmaması kasıtlı bir davranış olarak değerlendirilemez.\",\n",
    "        \"C, dikkat eksikliği nedeniyle ödeme yapmayı unutmuş olabilir.\",\n",
    "        \"C kişisi ödeme yapmadan çıkmayı bir hata olarak tanımlamıştır.\",\n",
    "        \"C'nin kasada ödeme yapmadığı, güvenlik kayıtlarıyla teyit edilmiştir.\",\n",
    "        \"C'nin kasadan ödeme yapmadan ayrılması bilinçli bir davranış olarak nitelendirilebilir.\",\n",
    "        \"C, kasada ödeme yapmadığı için sorumluluk almalıdır.\",\n",
    "        \"C'nin ödeme yapmadığına dair herhangi bir kanıt yoktur.\",\n",
    "        \"Market çalışanları, C’nin kasada ödeme yapmadığını fark etti.\",\n",
    "        \"C kişisi, ödeme yapmayı unuttuğunu iddia ediyor.\",\n",
    "        \"C'nin kasada ödeme yapmadığı iddiası gerçeği yansıtmamaktadır.\",\n",
    "        \"C'nin ödeme yapmaması kasıtlı olarak değerlendirilemez.\",\n",
    "        \"C'nin dikkatsizliği yüzünden ödemeyi unutmuş olabileceği düşünülüyor.\",\n",
    "        \"C kişisi, ödeme yapmadan ayrılmayı bir hata olarak kabul etmiştir.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    create_log(f\"{model_name}_log.txt\", \"Score is calculated in the range of 0-1. Higher score indicates higher similarity.\")  \n",
    "\n",
    "    if model_name == \"nli_model\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"label\": [], \"score\": []})\n",
    "    elif model_name == \"sentiment_analysis\":\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"sentiment_1\": [], \"sentiment_2\": []})\n",
    "    else:\n",
    "        create_excel_file(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": [], \"Sentence2\": [], \"Result\": []})\n",
    "\n",
    "    # Compare the sentences\n",
    "    compared_sentence_pairs = []\n",
    "\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        for index2, sentence2 in enumerate(sentences):\n",
    "            if index != index2 and ( ( sentence, sentence2 ) not in compared_sentence_pairs and ( sentence2, sentence ) not in compared_sentence_pairs):\n",
    "\n",
    "                result = model.calculate_similarity(sentence, sentence2)\n",
    "                \n",
    "                #print(\"\\nSentence1: \", sentence,\"\\nSentence2: \", sentence2)\n",
    "                #print(\"Return: \\n\", result)\n",
    "                \n",
    "                append_to_log(f\"{model_name}_log.txt\", f\"\\nSentence1: {sentence}\\nSentence2: {sentence2}\\nResult: {result}\")\n",
    "                if model_name == \"nli_model\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"label\": result[\"label\"], \"score\": result[\"score\"]})\n",
    "                elif model_name == \"sentiment_analysis\":\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"sentiment_1\": result[0], \"sentiment_2\": result[1]})\n",
    "                else:\n",
    "                    append_to_excel(f\"{model_name}_log.xlsx\", \"Results\", {\"Sentence1\": sentence, \"Sentence2\": sentence2, \"Result\": result})\n",
    "                #print(\"\\n\")\n",
    "                computation_count += 1\n",
    "                compared_sentence_pairs.append((sentence, sentence2))\n",
    "        #break # Delete this line for nested for :)\n",
    "\n",
    "    end = time.time()\n",
    "    append_to_log(\"model_exec_times.txt\", f\"{model_name} Avg comparison time: {(end - start) / computation_count} seconds, Total time: {end - start} seconds\")\n",
    "    append_to_log(f\"{model_name}_log.txt\", f\"\\nTotal time: {end - start} seconds\")\n",
    "    print(f\"Total time: {end - start} seconds\") \n",
    "\n",
    "# Test the NLI model    \n",
    "#test_model(nli_model, \"nli_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nli_model = SentenceComparator_NLI(\"microsoft/deberta-large-mnli\")\n",
    "#test_model(nli_model, \"nli_model\")\n",
    "#\n",
    "#semantic_similarity = SentenceComparator_semantic(\"paraphrase-MiniLM-L6-v2\")\n",
    "#test_model( semantic_similarity, \"semantic_similarity\")\n",
    "#\n",
    "#bert_cosine_similarity = SentenceComparator_bert_cosine(\"bert-base-multilingual-cased\")\n",
    "#test_model(bert_cosine_similarity, \"bert_cosine_similarity\")\n",
    "#\n",
    "#sbert_similarity = SentenceComparator_SBERT(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "#test_model(sbert_similarity, \"sbert_similarity\")\n",
    "#\n",
    "#word2vec_sim = SentenceComparator_Word2Vec(\"utils/word2vec/w2v_.model\")\n",
    "#test_model(word2vec_sim, \"word2vec_sim\")\n",
    "#\n",
    "#sentiment_analysis = SentenceComparator_sentiment_analysis()\n",
    "#test_model(sentiment_analysis, \"sentiment_analysis\")\n",
    "#\n",
    "#sys_prompt= \"Sen bir text-miner algoritmasın.\\\n",
    "#                Cümleleri sadece anlamsal olarak değerlendir.\\\n",
    "#                İstenen dönüş: değerlendirme:<benzer anlam->1, farklı anlam->0>.\\\n",
    "#                Bu formate göre bir dönüş sağla ve sadece anlama odaklan.\"\n",
    "#\n",
    "#ollama_model_llama3 = SentenceComparator_Ollama(\n",
    "#    llama_version=\"llama3.1\",\n",
    "#    modelfile_system= sys_prompt,\n",
    "#    temperature=0.4\n",
    "#)\n",
    "#test_model(ollama_model_llama3, \"ollama_model_llama3.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_df(file_name, sheet_name):\n",
    "    file_name = \"log/\" + file_name\n",
    "    excel_df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    return excel_df\n",
    "\n",
    "########################################\n",
    "\n",
    "# For bert_cos_sim\n",
    "bert_cos_sim = excel_to_df(\"bert_cosine_similarity_log.xlsx\", \"Results\")\n",
    "bert_cos_sim[\"Result\"] = bert_cos_sim[\"Result\"].str.replace(r'[\\[\\]]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For sbert_similarity_log.xlsx\n",
    "sbert_cos_df = excel_to_df(\"sbert_similarity_log.xlsx\", \"Results\")\n",
    "sbert_cos_df[\"Result\"] = sbert_cos_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For nli_model_log.xlsx\n",
    "nli_df = excel_to_df(\"nli_model_log.xlsx\", \"Results\")\n",
    "\n",
    "# Dummy encoding\n",
    "dummy = pd.get_dummies(nli_df[\"label\"])\n",
    "nli_df.drop(\"label\", axis=1, inplace=True)\n",
    "nli_df = pd.concat([nli_df, dummy], axis=1)\n",
    "\n",
    "nli_df.rename(columns={\"score\":\"Result\"}, inplace=True)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For semantic_similarity_log.xlsx\n",
    "semantic_df = excel_to_df(\"semantic_similarity_log.xlsx\", \"Results\")\n",
    "semantic_df[\"Result\"] = semantic_df[\"Result\"].str.replace(r'[\\[\\]()tensor]','',regex=True).astype(float)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For word2vec_sim_log.xlsx\n",
    "word2vec_df = excel_to_df(\"word2vec_sim_log.xlsx\", \"Results\")\n",
    "\n",
    "########################################\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment_df = excel_to_df(\"sentiment_analysis_log.xlsx\", \"Results\")\n",
    "\n",
    "# Dummy encoding\n",
    "dummy_1 = pd.get_dummies(sentiment_df[\"sentiment_1\"])\n",
    "dummy_2 = pd.get_dummies(sentiment_df[\"sentiment_2\"])\n",
    "sentiment_df.drop([\"sentiment_1\", \"sentiment_2\"], axis=1, inplace=True)\n",
    "sentiment_df = pd.concat([sentiment_df, dummy_1, dummy_2], axis=1)\n",
    "\n",
    "########################################\n",
    "\n",
    "# For ollama_model_llama3.1_log.xlsx\n",
    "ollama_df = excel_to_df(\"ollama_model_llama3.1_log.xlsx\", \"Results\")\n",
    "ollama_df[\"Result\"] = ollama_df[\"Result\"].str.replace(r'[\\[\\]()tensorDdeğerlendirme:Cüaıbzkfakı .23456789]','',regex=True).astype(int)\n",
    "ollama_df\n",
    "\n",
    "# Concatenate all the results\n",
    "#all_results = pd.concat([bert_cos_sim[\"Sentence1\"],bert_cos_sim[\"Sentence2\"],bert_cos_sim[\"Result\"], sbert_cos_df[\"Result\"], nli_df[\"Result\"], semantic_df[\"Result\"], word2vec_df[\"Result\"], sentiment_df[\"Result\"], ollama_df[\"Result\"]],\n",
    "#                        axis=1, \n",
    "#                        keys=[\"Sentence1\", \"Sentence2\", \"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "\n",
    "\n",
    "def concat_columns_except_sentences(df_list, df_list_names):\n",
    "    initial_df = df_list[0].iloc[:,:2]\n",
    "    for index, df in enumerate(df_list):\n",
    "        df_except_sentences = df[ df.columns.difference([\"Sentence1\", \"Sentence2\"]) ] * 1\n",
    "        df_except_sentences.columns = [f\"{df_list_names[index]}_{col}\" for col in df_except_sentences.columns]\n",
    "        initial_df = pd.concat([initial_df, df_except_sentences], axis=1)\n",
    "    return initial_df\n",
    "\n",
    "all_results = concat_columns_except_sentences(\n",
    "    [bert_cos_sim, sbert_cos_df, nli_df, semantic_df, word2vec_df, sentiment_df, ollama_df], \n",
    "    [\"bert_cos_sim\", \"sbert_cos_sim\", \"nli_model\", \"semantic_similarity\", \"word2vec_sim\", \"sentiment_analysis\", \"ollama_model_llama3.1\"])\n",
    "             \n",
    "#all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an excel file\n",
    "#all_results.to_excel(\"log/all_results.xlsx\", index=False)\n",
    "\n",
    "# Read the results from the excel file\n",
    "#all_results = pd.read_excel(\"log/all_results.xlsx\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "all_results_parameters = all_results.drop([\"Sentence1\", \"Sentence2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Normalize lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "y = all_results_parameters[\"ollama_model_llama3.1_Result\"]\n",
    "X = all_results_parameters.drop(\"ollama_model_llama3.1_Result\", axis=1)\n",
    "\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg, Accuracy:  0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# Create the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Reg, Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_cos_sim_Result</th>\n",
       "      <th>sbert_cos_sim_Result</th>\n",
       "      <th>nli_model_CONTRADICTION</th>\n",
       "      <th>nli_model_ENTAILMENT</th>\n",
       "      <th>nli_model_NEUTRAL</th>\n",
       "      <th>nli_model_Result</th>\n",
       "      <th>semantic_similarity_Result</th>\n",
       "      <th>word2vec_sim_Result</th>\n",
       "      <th>sentiment_analysis_LABEL_1</th>\n",
       "      <th>sentiment_analysis_LABEL_1</th>\n",
       "      <th>sentiment_analysis_LABEL_2</th>\n",
       "      <th>sentiment_analysis_LABEL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964577</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769148</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>0.946022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589680</td>\n",
       "      <td>0.7214</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.625543</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657169</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710494</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.6462</td>\n",
       "      <td>0.396546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844054</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.718892</td>\n",
       "      <td>0.6233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514596</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.703241</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601376</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.457836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>0.6490</td>\n",
       "      <td>0.658151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.8142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532981</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bert_cos_sim_Result  sbert_cos_sim_Result  nli_model_CONTRADICTION  \\\n",
       "0               0.964577                0.9383                        0   \n",
       "1               0.636648                0.1996                        1   \n",
       "2               0.625543                0.1646                        1   \n",
       "3               0.657169                0.4852                        1   \n",
       "4               0.587133                0.2732                        1   \n",
       "..                   ...                   ...                      ...   \n",
       "699             0.697819                0.7362                        1   \n",
       "700             0.718892                0.6233                        1   \n",
       "701             0.703241                0.6767                        1   \n",
       "702             0.764747                0.6354                        1   \n",
       "703             0.766007                0.8142                        0   \n",
       "\n",
       "     nli_model_ENTAILMENT  nli_model_NEUTRAL  nli_model_Result  \\\n",
       "0                       1                  0          0.769148   \n",
       "1                       0                  0          0.589680   \n",
       "2                       0                  0          0.747958   \n",
       "3                       0                  0          0.710494   \n",
       "4                       0                  0          0.737624   \n",
       "..                    ...                ...               ...   \n",
       "699                     0                  0          0.844054   \n",
       "700                     0                  0          0.514596   \n",
       "701                     0                  0          0.601376   \n",
       "702                     0                  0          0.867559   \n",
       "703                     0                  1          0.532981   \n",
       "\n",
       "     semantic_similarity_Result  word2vec_sim_Result  \\\n",
       "0                        0.9611             0.946022   \n",
       "1                        0.7214             0.398693   \n",
       "2                        0.7042             0.312992   \n",
       "3                        0.6675             0.391442   \n",
       "4                        0.6462             0.396546   \n",
       "..                          ...                  ...   \n",
       "699                      0.6441             0.515694   \n",
       "700                      0.6645             0.496692   \n",
       "701                      0.7230             0.457836   \n",
       "702                      0.6490             0.658151   \n",
       "703                      0.5583             0.408180   \n",
       "\n",
       "     sentiment_analysis_LABEL_1  sentiment_analysis_LABEL_1  \\\n",
       "0                             1                           0   \n",
       "1                             1                           1   \n",
       "2                             1                           1   \n",
       "3                             1                           0   \n",
       "4                             1                           1   \n",
       "..                          ...                         ...   \n",
       "699                           0                           1   \n",
       "700                           0                           1   \n",
       "701                           1                           1   \n",
       "702                           1                           1   \n",
       "703                           1                           1   \n",
       "\n",
       "     sentiment_analysis_LABEL_2  sentiment_analysis_LABEL_2  \n",
       "0                             0                           1  \n",
       "1                             0                           0  \n",
       "2                             0                           0  \n",
       "3                             0                           1  \n",
       "4                             0                           0  \n",
       "..                          ...                         ...  \n",
       "699                           1                           0  \n",
       "700                           1                           0  \n",
       "701                           0                           0  \n",
       "702                           0                           0  \n",
       "703                           0                           0  \n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44644703,  0.37668813,  0.05561013,  0.06242784, -0.09295003,\n",
       "        0.13803581,  0.3736199 , -0.55249954, -0.07450976, -0.01439088,\n",
       "        0.07450976,  0.01439088])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.5384615384615384\n",
      "Recall:  0.175\n",
      "True Positive:  7\n",
      "True Negative:  95\n",
      "False Positive:  6\n",
      "False Negative:  33\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_conf_matrix(y_test, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    precision = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[0][1])\n",
    "    recall = conf_matrix[1][1] / (conf_matrix[1][1] + conf_matrix[1][0])\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    print(\"True Positive: \", conf_matrix[1][1])\n",
    "    print(\"True Negative: \", conf_matrix[0][0])\n",
    "    print(\"False Positive: \", conf_matrix[0][1])\n",
    "    print(\"False Negative: \", conf_matrix[1][0])\n",
    "    \n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6046 - loss: 0.6481\n",
      "Epoch 2/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.6838 - loss: 0.5990\n",
      "Epoch 3/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.7126 - loss: 0.5616\n",
      "Epoch 4/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.6923 - loss: 0.5661\n",
      "Epoch 5/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.7053 - loss: 0.5484\n",
      "Epoch 6/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.7318 - loss: 0.5158\n",
      "Epoch 7/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.7314 - loss: 0.5323\n",
      "Epoch 8/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.7134 - loss: 0.5318\n",
      "Epoch 9/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - accuracy: 0.7596 - loss: 0.5016\n",
      "Epoch 10/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.7301 - loss: 0.5311\n",
      "Epoch 11/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.7559 - loss: 0.5061\n",
      "Epoch 12/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.7204 - loss: 0.5255\n",
      "Epoch 13/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.7213 - loss: 0.5214\n",
      "Epoch 14/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.7631 - loss: 0.4908\n",
      "Epoch 15/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7536 - loss: 0.4947\n",
      "Epoch 16/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.7317 - loss: 0.5080\n",
      "Epoch 17/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.7271 - loss: 0.5126\n",
      "Epoch 18/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.7694 - loss: 0.4905\n",
      "Epoch 19/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7628 - loss: 0.4896\n",
      "Epoch 20/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.7850 - loss: 0.4766\n",
      "Epoch 21/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.7311 - loss: 0.5054\n",
      "Epoch 22/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - accuracy: 0.7426 - loss: 0.4794\n",
      "Epoch 23/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.7631 - loss: 0.4884\n",
      "Epoch 24/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.7577 - loss: 0.5009\n",
      "Epoch 25/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.7643 - loss: 0.4842\n",
      "Epoch 26/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.7689 - loss: 0.4658\n",
      "Epoch 27/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.7895 - loss: 0.4632\n",
      "Epoch 28/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.7945 - loss: 0.4702\n",
      "Epoch 29/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.7639 - loss: 0.4949\n",
      "Epoch 30/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.7371 - loss: 0.5176\n",
      "Epoch 31/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.7644 - loss: 0.4752\n",
      "Epoch 32/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7464 - loss: 0.5099\n",
      "Epoch 33/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.7960 - loss: 0.4620\n",
      "Epoch 34/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - accuracy: 0.7792 - loss: 0.4804\n",
      "Epoch 35/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7172 - loss: 0.5063\n",
      "Epoch 36/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - accuracy: 0.7691 - loss: 0.4793\n",
      "Epoch 37/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.7568 - loss: 0.4601\n",
      "Epoch 38/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.7831 - loss: 0.4574\n",
      "Epoch 39/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.7849 - loss: 0.4672\n",
      "Epoch 40/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.7838 - loss: 0.4609\n",
      "Epoch 41/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.7725 - loss: 0.4695\n",
      "Epoch 42/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7750 - loss: 0.4726\n",
      "Epoch 43/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.7775 - loss: 0.4702\n",
      "Epoch 44/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.7644 - loss: 0.4871\n",
      "Epoch 45/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.7714 - loss: 0.4766\n",
      "Epoch 46/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - accuracy: 0.7568 - loss: 0.4750\n",
      "Epoch 47/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7780 - loss: 0.4590\n",
      "Epoch 48/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.7676 - loss: 0.4640\n",
      "Epoch 49/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.8134 - loss: 0.4463\n",
      "Epoch 50/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.7774 - loss: 0.4707\n",
      "Epoch 51/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - accuracy: 0.7739 - loss: 0.4745\n",
      "Epoch 52/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.7809 - loss: 0.4689\n",
      "Epoch 53/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.7631 - loss: 0.4761\n",
      "Epoch 54/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.7759 - loss: 0.4589\n",
      "Epoch 55/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.7852 - loss: 0.4594\n",
      "Epoch 56/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.7760 - loss: 0.4699\n",
      "Epoch 57/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.7827 - loss: 0.4675\n",
      "Epoch 58/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.7594 - loss: 0.4566\n",
      "Epoch 59/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.7921 - loss: 0.4519\n",
      "Epoch 60/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.7809 - loss: 0.4473\n",
      "Epoch 61/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 0.7754 - loss: 0.4619\n",
      "Epoch 62/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.7746 - loss: 0.4601\n",
      "Epoch 63/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7933 - loss: 0.4409  \n",
      "Epoch 64/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.7724 - loss: 0.4428\n",
      "Epoch 65/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.7885 - loss: 0.4556\n",
      "Epoch 66/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7673 - loss: 0.4521  \n",
      "Epoch 67/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.8097 - loss: 0.4292\n",
      "Epoch 68/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.7940 - loss: 0.4201\n",
      "Epoch 69/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.7637 - loss: 0.4582\n",
      "Epoch 70/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.7737 - loss: 0.4615\n",
      "Epoch 71/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.7979 - loss: 0.4320\n",
      "Epoch 72/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.7918 - loss: 0.4459\n",
      "Epoch 73/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - accuracy: 0.7989 - loss: 0.4077\n",
      "Epoch 74/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.8074 - loss: 0.4233\n",
      "Epoch 75/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.8106 - loss: 0.4299\n",
      "Epoch 76/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.8018 - loss: 0.4139\n",
      "Epoch 77/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.7780 - loss: 0.4375\n",
      "Epoch 78/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.7939 - loss: 0.4234\n",
      "Epoch 79/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7562 - loss: 0.4543\n",
      "Epoch 80/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.7685 - loss: 0.4563\n",
      "Epoch 81/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.7632 - loss: 0.4683\n",
      "Epoch 82/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.8307 - loss: 0.4172\n",
      "Epoch 83/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.7900 - loss: 0.4423\n",
      "Epoch 84/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.7820 - loss: 0.4329\n",
      "Epoch 85/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.8079 - loss: 0.4177\n",
      "Epoch 86/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.8236 - loss: 0.4141\n",
      "Epoch 87/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - accuracy: 0.7951 - loss: 0.4222\n",
      "Epoch 88/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.8020 - loss: 0.4247\n",
      "Epoch 89/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.8009 - loss: 0.4100\n",
      "Epoch 90/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7914 - loss: 0.4250\n",
      "Epoch 91/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - accuracy: 0.7819 - loss: 0.4380\n",
      "Epoch 92/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.7863 - loss: 0.4270\n",
      "Epoch 93/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7799 - loss: 0.4300\n",
      "Epoch 94/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.7936 - loss: 0.4347\n",
      "Epoch 95/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.8165 - loss: 0.4113\n",
      "Epoch 96/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.7810 - loss: 0.4353\n",
      "Epoch 97/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - accuracy: 0.8113 - loss: 0.4267\n",
      "Epoch 98/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - accuracy: 0.8384 - loss: 0.3797\n",
      "Epoch 99/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.7933 - loss: 0.4360\n",
      "Epoch 100/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.8148 - loss: 0.3888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x196be3f3410>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model2\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "def keras_model_tuner(hp):\n",
    "    hidden_layer_num = hp.Int('hidden_layer_num', min_value=1, max_value=5, step=1)\n",
    "    layer_unit = hp.Int('layer_unit', min_value=16, max_value=128, step=16)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer_unit, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    for i in range(hidden_layer_num):\n",
    "        model.add(layers.Dense(layer_unit, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='ollama_model2'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001 )\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from log\\ollama_model2\\tuner0.json\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000196A12B1580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 79ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000196A12B1580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Accuracy:  0.75177304964539\n",
      "Precision:  0.8571428571428571\n",
      "Recall:  0.15\n",
      "True Positive:  6\n",
      "True Negative:  100\n",
      "False Positive:  1\n",
      "False Negative:  34\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    keras_model_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='log',\n",
    "    project_name='ollama_model2'\n",
    ")\n",
    "\n",
    "tuner.reload()\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print_conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_columns(df, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    This function adds the Confidence, Real Value, and Prediction columns to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        df: A pandas dataframe.\n",
    "    \"\"\"\n",
    "    if type(df) != pd.DataFrame:\n",
    "        df = pd.DataFrame(df)\n",
    "    df[\"Confidence\"] = 1.0\n",
    "    df[\"Real Value\"] = 5\n",
    "    df[\"Prediction\"] = 5\n",
    "    df[\"Accuracy\"] = 5\n",
    "    #print(df.head())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, val in enumerate(y_pred):\n",
    "        \n",
    "        real =  y_test.iloc[index]\n",
    "        pred = 0 if val < 0.5 else 1\n",
    "        confidence = (val-0.5)*2 if pred == 1 else (0.5-val)*2\n",
    "        \n",
    "        df[\"Confidence\"][index] = confidence\n",
    "        df[\"Real Value\"][index] = real\n",
    "        df[\"Prediction\"][index] = pred\n",
    "        df[\"Accuracy\"][index] = 1 if real == pred else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_accuracy(x,y,predictor):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predictor.\n",
    "\n",
    "    Args:\n",
    "        x: A pandas dataframe.\n",
    "        y: A pandas dataframe.\n",
    "        predictor: A predictor model.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: A float.\n",
    "    \"\"\"\n",
    "    y_pred = predictor.predict(x)\n",
    "    y_pred = [1 if val > 0.5 else 0 for val in y_pred]\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Test Accuracy:  0.75177304964539\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step\n",
      "Train Accuracy:  0.7122557726465364\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step\n",
      "Entire Accuracy:  0.7201704545454546\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Test Accuracy: \", calculate_accuracy(X_test, y_test, model))\n",
    "# Train\n",
    "print(\"Train Accuracy: \", calculate_accuracy(X_train, y_train, model))\n",
    "# Entire\n",
    "print(\"Entire Accuracy: \", calculate_accuracy(normalized_X, y, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815188</td>\n",
       "      <td>1.590520</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.817017</td>\n",
       "      <td>-0.407003</td>\n",
       "      <td>-1.016843</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271673</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.436070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>-0.582501</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.225806</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>-0.524713</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.215165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678161</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.522652</td>\n",
       "      <td>-0.037228</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.401310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688566</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.574057</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>0.070940</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.518599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.286441</td>\n",
       "      <td>-0.745840</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.842474</td>\n",
       "      <td>0.195577</td>\n",
       "      <td>1.072469</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.561216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.351238</td>\n",
       "      <td>-0.550725</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.255695</td>\n",
       "      <td>0.718084</td>\n",
       "      <td>-0.655045</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.351091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.256504</td>\n",
       "      <td>0.109878</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.441526</td>\n",
       "      <td>0.229506</td>\n",
       "      <td>-0.615179</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.221825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.433858</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.138172</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>-0.462409</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.203513</td>\n",
       "      <td>0.724211</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.640465</td>\n",
       "      <td>1.445522</td>\n",
       "      <td>1.360279</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.442997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.815188  1.590520 -0.892143  3.009509 -0.918113 -0.817017 -0.407003   \n",
       "1    1.271673  0.764906  1.120897 -0.332280 -0.918113 -0.325933  0.588475   \n",
       "2    0.787798 -0.582501 -0.892143  3.009509 -0.918113 -1.225806  0.475152   \n",
       "3    0.678161 -0.317145  1.120897 -0.332280 -0.918113  0.638371  0.522652   \n",
       "4    0.688566 -0.634903  1.120897 -0.332280 -0.918113 -0.574057 -1.054368   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "136  0.286441 -0.745840  1.120897 -0.332280 -0.918113 -0.842474  0.195577   \n",
       "137  0.351238 -0.550725 -0.892143 -0.332280  1.089190 -0.255695  0.718084   \n",
       "138  0.256504  0.109878  1.120897 -0.332280 -0.918113  1.441526  0.229506   \n",
       "139 -0.433858  0.238654 -0.892143 -0.332280  1.089190  1.138172  0.382865   \n",
       "140  0.203513  0.724211 -0.892143 -0.332280  1.089190 -0.640465  1.445522   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "0   -1.016843  0.570789  0.377964 -0.570789 -0.377964    0.017966           1   \n",
       "1    1.624017  0.570789 -2.645751 -0.570789  2.645751    0.436070           1   \n",
       "2   -0.524713  0.570789  0.377964 -0.570789 -0.377964    0.215165           1   \n",
       "3   -0.037228  0.570789  0.377964 -0.570789 -0.377964    0.401310           1   \n",
       "4    0.070940  0.570789  0.377964 -0.570789 -0.377964    0.518599           1   \n",
       "..        ...       ...       ...       ...       ...         ...         ...   \n",
       "136  1.072469  0.570789  0.377964 -0.570789 -0.377964    0.561216           0   \n",
       "137 -0.655045  0.570789  0.377964 -0.570789 -0.377964    0.351091           0   \n",
       "138 -0.615179 -1.751960  0.377964  1.751960 -0.377964    0.221825           0   \n",
       "139 -0.462409  0.570789  0.377964 -0.570789 -0.377964    0.346781           1   \n",
       "140  1.360279  0.570789  0.377964 -0.570789 -0.377964    0.442997           0   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         0  \n",
       "..          ...       ...  \n",
       "136           0         1  \n",
       "137           0         1  \n",
       "138           0         1  \n",
       "139           0         0  \n",
       "140           0         1  \n",
       "\n",
       "[141 rows x 16 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Error rate: % 3.225806451612903       Number of high confidence predictions:  31\n",
      "Number of faults:  35    Faults from high confidence predictions:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815188</td>\n",
       "      <td>1.590520</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.817017</td>\n",
       "      <td>-0.407003</td>\n",
       "      <td>-1.016843</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271673</td>\n",
       "      <td>0.764906</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.325933</td>\n",
       "      <td>0.588475</td>\n",
       "      <td>1.624017</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.436070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.787798</td>\n",
       "      <td>-0.582501</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.225806</td>\n",
       "      <td>0.475152</td>\n",
       "      <td>-0.524713</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.215165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678161</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.522652</td>\n",
       "      <td>-0.037228</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.401310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.688566</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.574057</td>\n",
       "      <td>-1.054368</td>\n",
       "      <td>0.070940</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.518599</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.880661</td>\n",
       "      <td>-0.038967</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.522799</td>\n",
       "      <td>0.362507</td>\n",
       "      <td>-0.760867</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.316467</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.277258</td>\n",
       "      <td>-0.140984</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.540841</td>\n",
       "      <td>0.336721</td>\n",
       "      <td>0.105899</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.372742</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.039912</td>\n",
       "      <td>0.067510</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.510728</td>\n",
       "      <td>0.276328</td>\n",
       "      <td>-0.208648</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.470710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.168701</td>\n",
       "      <td>-0.763679</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-1.038817</td>\n",
       "      <td>-0.791758</td>\n",
       "      <td>-0.845175</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.654165</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.712509</td>\n",
       "      <td>-0.548495</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.409182</td>\n",
       "      <td>-0.968867</td>\n",
       "      <td>-1.415428</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.373990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.507932</td>\n",
       "      <td>0.823441</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.556588</td>\n",
       "      <td>-0.909831</td>\n",
       "      <td>-1.208781</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.275613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.039668</td>\n",
       "      <td>-0.634346</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.744138</td>\n",
       "      <td>-0.896259</td>\n",
       "      <td>-0.962822</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.552538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.446974</td>\n",
       "      <td>0.667906</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.854990</td>\n",
       "      <td>0.526045</td>\n",
       "      <td>-0.833751</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.161035</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.326526</td>\n",
       "      <td>-0.013323</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.068289</td>\n",
       "      <td>-0.559683</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.480271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.549689</td>\n",
       "      <td>-0.493305</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.384092</td>\n",
       "      <td>-0.126749</td>\n",
       "      <td>0.093489</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.587611</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.796507</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.122937</td>\n",
       "      <td>0.109397</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.464666</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.282194</td>\n",
       "      <td>-0.462645</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.150760</td>\n",
       "      <td>0.321114</td>\n",
       "      <td>0.457528</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.416547</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.400726</td>\n",
       "      <td>0.187366</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.979483</td>\n",
       "      <td>1.049231</td>\n",
       "      <td>-0.595118</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.313304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.330753</td>\n",
       "      <td>-0.717409</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.407301</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>0.948774</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.597320</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.484438</td>\n",
       "      <td>0.718762</td>\n",
       "      <td>-0.150209</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.226870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.064504</td>\n",
       "      <td>1.528641</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.787594</td>\n",
       "      <td>1.670810</td>\n",
       "      <td>1.312323</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.305227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.747359</td>\n",
       "      <td>1.209210</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.376738</td>\n",
       "      <td>0.896550</td>\n",
       "      <td>0.910501</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.482929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.932159</td>\n",
       "      <td>0.913193</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.786369</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.367950</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-1.108159</td>\n",
       "      <td>-2.200838</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.740699</td>\n",
       "      <td>0.216613</td>\n",
       "      <td>-1.578397</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.118627</td>\n",
       "      <td>-0.711834</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>0.958481</td>\n",
       "      <td>0.469045</td>\n",
       "      <td>-0.211750</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.402025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.278953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.144541</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.839466</td>\n",
       "      <td>-1.832908</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.020924</td>\n",
       "      <td>-0.931545</td>\n",
       "      <td>-0.877639</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.552177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.375575</td>\n",
       "      <td>-0.061266</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.361450</td>\n",
       "      <td>0.561331</td>\n",
       "      <td>-0.744705</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.543801</td>\n",
       "      <td>-1.504557</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.348936</td>\n",
       "      <td>-0.358731</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.466926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.529476</td>\n",
       "      <td>1.927233</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>3.009509</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.157388</td>\n",
       "      <td>0.635297</td>\n",
       "      <td>1.727076</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.665267</td>\n",
       "      <td>-0.115340</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.101153</td>\n",
       "      <td>0.715369</td>\n",
       "      <td>-0.430724</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.487033</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.577954</td>\n",
       "      <td>-0.520064</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.460191</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>-0.881556</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>-2.645751</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>0.485424</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-1.097475</td>\n",
       "      <td>-1.439891</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.535934</td>\n",
       "      <td>0.697726</td>\n",
       "      <td>-1.442881</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.393076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.433858</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.332280</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>1.138172</td>\n",
       "      <td>0.382865</td>\n",
       "      <td>-0.462409</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.346781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.815188  1.590520 -0.892143  3.009509 -0.918113 -0.817017 -0.407003   \n",
       "1    1.271673  0.764906  1.120897 -0.332280 -0.918113 -0.325933  0.588475   \n",
       "2    0.787798 -0.582501 -0.892143  3.009509 -0.918113 -1.225806  0.475152   \n",
       "3    0.678161 -0.317145  1.120897 -0.332280 -0.918113  0.638371  0.522652   \n",
       "4    0.688566 -0.634903  1.120897 -0.332280 -0.918113 -0.574057 -1.054368   \n",
       "14  -0.880661 -0.038967  1.120897 -0.332280 -0.918113 -0.522799  0.362507   \n",
       "16   0.277258 -0.140984 -0.892143 -0.332280  1.089190  1.540841  0.336721   \n",
       "18  -1.039912  0.067510 -0.892143 -0.332280  1.089190  0.510728  0.276328   \n",
       "25  -1.168701 -0.763679 -0.892143 -0.332280  1.089190 -1.038817 -0.791758   \n",
       "33  -0.712509 -0.548495  1.120897 -0.332280 -0.918113 -0.409182 -0.968867   \n",
       "40  -0.507932  0.823441  1.120897 -0.332280 -0.918113  0.556588 -0.909831   \n",
       "42  -0.039668 -0.634346 -0.892143 -0.332280  1.089190 -0.744138 -0.896259   \n",
       "44   0.446974  0.667906 -0.892143  3.009509 -0.918113 -1.854990  0.526045   \n",
       "48  -0.326526 -0.013323  1.120897 -0.332280 -0.918113  0.068289 -0.559683   \n",
       "52  -0.549689 -0.493305 -0.892143 -0.332280  1.089190  0.384092 -0.126749   \n",
       "53   0.796507  0.013436 -0.892143 -0.332280  1.089190  1.122937  0.109397   \n",
       "65   0.282194 -0.462645  1.120897 -0.332280 -0.918113 -1.150760  0.321114   \n",
       "66   0.400726  0.187366 -0.892143 -0.332280  1.089190 -0.979483  1.049231   \n",
       "69  -0.330753 -0.717409  1.120897 -0.332280 -0.918113  0.407301  0.222720   \n",
       "73   1.391350  1.021343 -0.892143 -0.332280  1.089190 -0.484438  0.718762   \n",
       "74   0.064504  1.528641 -0.892143 -0.332280  1.089190  1.787594  1.670810   \n",
       "79   0.747359  1.209210 -0.892143 -0.332280  1.089190  0.376738  0.896550   \n",
       "81   0.932159  0.913193 -0.892143 -0.332280  1.089190 -0.786369  0.077504   \n",
       "83  -1.108159 -2.200838  1.120897 -0.332280 -0.918113  0.740699  0.216613   \n",
       "84   0.118627 -0.711834 -0.892143 -0.332280  1.089190  0.958481  0.469045   \n",
       "87  -0.254197  0.985665  1.120897 -0.332280 -0.918113  1.376823 -0.191214   \n",
       "91  -0.005210  0.356280  1.120897 -0.332280 -0.918113 -0.804090 -0.052784   \n",
       "99  -0.839466 -1.832908  1.120897 -0.332280 -0.918113  1.020924 -0.931545   \n",
       "100 -1.375575 -0.061266  1.120897 -0.332280 -0.918113  1.361450  0.561331   \n",
       "102  0.543801 -1.504557  1.120897 -0.332280 -0.918113  0.102871  0.348936   \n",
       "107  1.529476  1.927233 -0.892143  3.009509 -0.918113 -0.157388  0.635297   \n",
       "108 -0.665267 -0.115340  1.120897 -0.332280 -0.918113  1.101153  0.715369   \n",
       "125 -0.577954 -0.520064  1.120897 -0.332280 -0.918113  0.460191 -0.018855   \n",
       "128 -1.097475 -1.439891  1.120897 -0.332280 -0.918113  0.535934  0.697726   \n",
       "139 -0.433858  0.238654 -0.892143 -0.332280  1.089190  1.138172  0.382865   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "0   -1.016843  0.570789  0.377964 -0.570789 -0.377964    0.017966           1   \n",
       "1    1.624017  0.570789 -2.645751 -0.570789  2.645751    0.436070           1   \n",
       "2   -0.524713  0.570789  0.377964 -0.570789 -0.377964    0.215165           1   \n",
       "3   -0.037228  0.570789  0.377964 -0.570789 -0.377964    0.401310           1   \n",
       "4    0.070940  0.570789  0.377964 -0.570789 -0.377964    0.518599           1   \n",
       "14  -0.760867  0.570789  0.377964 -0.570789 -0.377964    0.316467           1   \n",
       "16   0.105899  0.570789  0.377964 -0.570789 -0.377964    0.372742           1   \n",
       "18  -0.208648  0.570789  0.377964 -0.570789 -0.377964    0.470710           1   \n",
       "25  -0.845175  0.570789  0.377964 -0.570789 -0.377964    0.654165           1   \n",
       "33  -1.415428  0.570789  0.377964 -0.570789 -0.377964    0.373990           1   \n",
       "40  -1.208781 -1.751960  0.377964  1.751960 -0.377964    0.275613           1   \n",
       "42  -0.962822  0.570789  0.377964 -0.570789 -0.377964    0.552538           1   \n",
       "44  -0.833751  0.570789  0.377964 -0.570789 -0.377964    0.161035           1   \n",
       "48  -0.034942  0.570789  0.377964 -0.570789 -0.377964    0.480271           1   \n",
       "52   0.093489 -1.751960  0.377964  1.751960 -0.377964    0.587611           1   \n",
       "53   0.898390  0.570789  0.377964 -0.570789 -0.377964    0.464666           1   \n",
       "65   0.457528  0.570789  0.377964 -0.570789 -0.377964    0.416547           1   \n",
       "66  -0.595118  0.570789  0.377964 -0.570789 -0.377964    0.313304           1   \n",
       "69   0.948774  0.570789  0.377964 -0.570789 -0.377964    0.597320           1   \n",
       "73  -0.150209  0.570789  0.377964 -0.570789 -0.377964    0.226870           1   \n",
       "74   1.312323  0.570789  0.377964 -0.570789 -0.377964    0.305227           1   \n",
       "79   0.910501 -1.751960 -2.645751  1.751960  2.645751    0.482929           1   \n",
       "81   0.151178  0.570789  0.377964 -0.570789 -0.377964    0.367950           1   \n",
       "83  -1.578397  0.570789  0.377964 -0.570789 -0.377964    0.459541           1   \n",
       "84  -0.211750  0.570789  0.377964 -0.570789 -0.377964    0.402025           1   \n",
       "87  -0.034734 -1.751960  0.377964  1.751960 -0.377964    0.278953           1   \n",
       "91  -0.179440 -1.751960  0.377964  1.751960 -0.377964    0.144541           1   \n",
       "99  -0.877639  0.570789  0.377964 -0.570789 -0.377964    0.552177           1   \n",
       "100 -0.744705 -1.751960  0.377964  1.751960 -0.377964    0.317383           1   \n",
       "102 -0.358731  0.570789  0.377964 -0.570789 -0.377964    0.466926           1   \n",
       "107  1.727076  0.570789  0.377964 -0.570789 -0.377964    0.071053           0   \n",
       "108 -0.430724 -1.751960 -2.645751  1.751960  2.645751    0.487033           1   \n",
       "125 -0.881556 -1.751960 -2.645751  1.751960  2.645751    0.485424           1   \n",
       "128 -1.442881  0.570789  0.377964 -0.570789 -0.377964    0.393076           1   \n",
       "139 -0.462409  0.570789  0.377964 -0.570789 -0.377964    0.346781           1   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         0  \n",
       "14            0         0  \n",
       "16            0         0  \n",
       "18            0         0  \n",
       "25            0         0  \n",
       "33            0         0  \n",
       "40            0         0  \n",
       "42            0         0  \n",
       "44            0         0  \n",
       "48            0         0  \n",
       "52            0         0  \n",
       "53            0         0  \n",
       "65            0         0  \n",
       "66            0         0  \n",
       "69            0         0  \n",
       "73            0         0  \n",
       "74            0         0  \n",
       "79            0         0  \n",
       "81            0         0  \n",
       "83            0         0  \n",
       "84            0         0  \n",
       "87            0         0  \n",
       "91            0         0  \n",
       "99            0         0  \n",
       "100           0         0  \n",
       "102           0         0  \n",
       "107           1         0  \n",
       "108           0         0  \n",
       "125           0         0  \n",
       "128           0         0  \n",
       "139           0         0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST DATASET\n",
    "\n",
    "high_accuracy_limit = 0.6\n",
    "\n",
    "keras_predict_df_test = pd.DataFrame(model.predict(X_test), columns=[\"Prediction\"])\n",
    "\n",
    "result_df_test = add_output_columns(X_test, y_test, keras_predict_df_test[\"Prediction\"])\n",
    "\n",
    "faults_test = result_df_test[result_df_test[\"Accuracy\"] == 0]\n",
    "\n",
    "high_acc = result_df_test[result_df_test[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults_test.shape[0], \"   Faults from high confidence predictions: \", faults_test[ faults_test[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.37142857142857144\n",
      "Recall:  0.325\n",
      "True Positive:  13\n",
      "True Negative:  79\n",
      "False Positive:  22\n",
      "False Negative:  27\n"
     ]
    }
   ],
   "source": [
    "print_conf_matrix(result_df_test[\"Real Value\"], result_df_test[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n",
      "Error rate: % 5.603448275862069       Number of high confidence predictions:  464\n",
      "Number of faults:  83    Faults from high confidence predictions:  26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.702469</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-1.493281</td>\n",
       "      <td>0.614939</td>\n",
       "      <td>-0.603952</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.228788</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.778340</td>\n",
       "      <td>0.203533</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.392635</td>\n",
       "      <td>0.755405</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.320309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.543801</td>\n",
       "      <td>-1.504557</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.348936</td>\n",
       "      <td>-0.358731</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.788819</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.841309</td>\n",
       "      <td>0.105418</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>0.190801</td>\n",
       "      <td>0.630547</td>\n",
       "      <td>-0.264714</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.212725</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.589170</td>\n",
       "      <td>0.819538</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.578596</td>\n",
       "      <td>0.688905</td>\n",
       "      <td>0.042473</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.136343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.224504</td>\n",
       "      <td>0.933820</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.979147</td>\n",
       "      <td>-0.458575</td>\n",
       "      <td>-0.826684</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1.391350</td>\n",
       "      <td>1.021343</td>\n",
       "      <td>-0.892143</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>1.089190</td>\n",
       "      <td>-0.484438</td>\n",
       "      <td>0.718762</td>\n",
       "      <td>-0.150209</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.923103</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>-0.434589</td>\n",
       "      <td>0.136079</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-1.600044</td>\n",
       "      <td>-0.052105</td>\n",
       "      <td>-0.459445</td>\n",
       "      <td>0.570789</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>-0.570789</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.487993</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.254197</td>\n",
       "      <td>0.985665</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>1.376823</td>\n",
       "      <td>-0.191214</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.928853</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>1.120897</td>\n",
       "      <td>-0.33228</td>\n",
       "      <td>-0.918113</td>\n",
       "      <td>-0.804090</td>\n",
       "      <td>-0.052784</td>\n",
       "      <td>-0.179440</td>\n",
       "      <td>-1.751960</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>1.751960</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0.894517</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2        3         4         5         6  \\\n",
       "26   0.269861  0.702469 -0.892143 -0.33228  1.089190 -1.493281  0.614939   \n",
       "28   0.778340  0.203533  1.120897 -0.33228 -0.918113  1.392635  0.755405   \n",
       "30   0.543801 -1.504557  1.120897 -0.33228 -0.918113  0.102871  0.348936   \n",
       "34   0.841309  0.105418  1.120897 -0.33228 -0.918113  0.190801  0.630547   \n",
       "35   0.589170  0.819538  1.120897 -0.33228 -0.918113 -0.578596  0.688905   \n",
       "..        ...       ...       ...      ...       ...       ...       ...   \n",
       "681  0.224504  0.933820 -0.892143 -0.33228  1.089190 -0.979147 -0.458575   \n",
       "686  1.391350  1.021343 -0.892143 -0.33228  1.089190 -0.484438  0.718762   \n",
       "691 -0.434589  0.136079  1.120897 -0.33228 -0.918113 -1.600044 -0.052105   \n",
       "699 -0.254197  0.985665  1.120897 -0.33228 -0.918113  1.376823 -0.191214   \n",
       "700 -0.005210  0.356280  1.120897 -0.33228 -0.918113 -0.804090 -0.052784   \n",
       "\n",
       "            7         8         9        10        11  Confidence  Real Value  \\\n",
       "26  -0.603952  0.570789  0.377964 -0.570789 -0.377964    0.228788           1   \n",
       "28  -0.001080  0.570789  0.377964 -0.570789 -0.377964    0.320309           1   \n",
       "30  -0.358731  0.570789  0.377964 -0.570789 -0.377964    0.788819           1   \n",
       "34  -0.264714  0.570789  0.377964 -0.570789 -0.377964    0.212725           1   \n",
       "35   0.042473  0.570789  0.377964 -0.570789 -0.377964    0.136343           1   \n",
       "..        ...       ...       ...       ...       ...         ...         ...   \n",
       "681 -0.826684  0.570789  0.377964 -0.570789 -0.377964    0.143404           0   \n",
       "686 -0.150209  0.570789  0.377964 -0.570789 -0.377964    0.923103           1   \n",
       "691 -0.459445  0.570789  0.377964 -0.570789 -0.377964    0.487993           0   \n",
       "699 -0.034734 -1.751960  0.377964  1.751960 -0.377964    0.928853           1   \n",
       "700 -0.179440 -1.751960  0.377964  1.751960 -0.377964    0.894517           1   \n",
       "\n",
       "     Prediction  Accuracy  \n",
       "26            0         0  \n",
       "28            0         0  \n",
       "30            0         0  \n",
       "34            0         0  \n",
       "35            0         0  \n",
       "..          ...       ...  \n",
       "681           1         0  \n",
       "686           0         0  \n",
       "691           1         0  \n",
       "699           0         0  \n",
       "700           0         0  \n",
       "\n",
       "[83 rows x 16 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTIRE DATASET\n",
    "# Predicts\n",
    "keras_predict_df = pd.DataFrame(model.predict(normalized_X), columns=[\"Prediction\"])\n",
    "\n",
    "#sort the dataframe by ID\n",
    "result_df = add_output_columns( \n",
    "    df = normalized_X,\n",
    "    y_test = y,\n",
    "    y_pred = keras_predict_df[\"Prediction\"]\n",
    ")\n",
    "faults = result_df[ result_df[\"Accuracy\"] == 0 ]\n",
    "\n",
    "# Accuracy counts of the model where the confidence is greater than high_accuracy_limit\n",
    "high_acc = result_df[result_df[\"Confidence\"] > high_accuracy_limit][\"Accuracy\"].value_counts()\n",
    "print(\"Error rate: %\", 100* high_acc[0] / (high_acc[0] + high_acc[1]), \"      Number of high confidence predictions: \", high_acc[0] + high_acc[1])\n",
    "\n",
    "print(\"Number of faults: \", faults.shape[0], \"   Faults from high confidence predictions: \", faults[ faults[\"Confidence\"] > high_accuracy_limit ].shape[0])\n",
    "faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Real Value</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "      <td>704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.124814e-16</td>\n",
       "      <td>1.009294e-16</td>\n",
       "      <td>7.065056e-17</td>\n",
       "      <td>2.573699e-16</td>\n",
       "      <td>-5.450186e-16</td>\n",
       "      <td>-3.027881e-17</td>\n",
       "      <td>0.703830</td>\n",
       "      <td>0.301136</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>1.000711e+00</td>\n",
       "      <td>0.308343</td>\n",
       "      <td>0.459078</td>\n",
       "      <td>0.445678</td>\n",
       "      <td>0.366021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.375819e+00</td>\n",
       "      <td>-2.617827e+00</td>\n",
       "      <td>-1.976928e+00</td>\n",
       "      <td>-3.428721e+00</td>\n",
       "      <td>-2.103059e+00</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.171947e-01</td>\n",
       "      <td>-6.411746e-01</td>\n",
       "      <td>-8.397758e-01</td>\n",
       "      <td>-5.196470e-01</td>\n",
       "      <td>-7.021904e-01</td>\n",
       "      <td>-1.498079e+00</td>\n",
       "      <td>0.487678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.630704e-02</td>\n",
       "      <td>-1.583170e-02</td>\n",
       "      <td>-1.698798e-02</td>\n",
       "      <td>2.159341e-01</td>\n",
       "      <td>-1.723928e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.822868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.399473e-01</td>\n",
       "      <td>7.538961e-01</td>\n",
       "      <td>8.097366e-01</td>\n",
       "      <td>6.919582e-01</td>\n",
       "      <td>6.037603e-01</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>0.979437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.316165e+00</td>\n",
       "      <td>2.456273e+00</td>\n",
       "      <td>2.317426e+00</td>\n",
       "      <td>2.223853e+00</td>\n",
       "      <td>3.653464e+00</td>\n",
       "      <td>6.675217e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02  7.040000e+02   \n",
       "mean   8.124814e-16  1.009294e-16  7.065056e-17  2.573699e-16 -5.450186e-16   \n",
       "std    1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00  1.000711e+00   \n",
       "min   -3.375819e+00 -2.617827e+00 -1.976928e+00 -3.428721e+00 -2.103059e+00   \n",
       "25%   -6.171947e-01 -6.411746e-01 -8.397758e-01 -5.196470e-01 -7.021904e-01   \n",
       "50%    8.630704e-02 -1.583170e-02 -1.698798e-02  2.159341e-01 -1.723928e-01   \n",
       "75%    6.399473e-01  7.538961e-01  8.097366e-01  6.919582e-01  6.037603e-01   \n",
       "max    3.316165e+00  2.456273e+00  2.317426e+00  2.223853e+00  3.653464e+00   \n",
       "\n",
       "                  5  Confidence  Real Value  Prediction    Accuracy  \n",
       "count  7.040000e+02  704.000000  704.000000  704.000000  704.000000  \n",
       "mean  -3.027881e-17    0.703830    0.301136    0.272727    0.840909  \n",
       "std    1.000711e+00    0.308343    0.459078    0.445678    0.366021  \n",
       "min   -1.498079e+00    0.001044    0.000000    0.000000    0.000000  \n",
       "25%   -1.498079e+00    0.487678    0.000000    0.000000    1.000000  \n",
       "50%    6.675217e-01    0.822868    0.000000    0.000000    1.000000  \n",
       "75%    6.675217e-01    0.979437    1.000000    1.000000    1.000000  \n",
       "max    6.675217e-01    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy\n",
       "1    353\n",
       "0     17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_confidence = result_df[ result_df[\"Confidence\"] > high_accuracy_limit ]\n",
    "\n",
    "high_confidence[\"Accuracy\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
